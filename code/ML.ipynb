{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "#libraries that need to be installed extra , if already do not exists\n",
    "#mglearn, pandas, numpy, sklearn\n",
    "#----------------------------------------\n",
    "#---------------------------------------------------------------------------\n",
    "#FLAG used in each code block mean \n",
    "#                1: that can be executed at any machine with final csv file\n",
    "#                0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#import libaries\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import mglearn, csv, random , matplotlib\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import time\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "#classification models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "#clustering\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "#data scaling \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from collections import defaultdict\n",
    "\n",
    "#accuracy score metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "#cosine similatiry metric\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#covariance with ellipticEnvelope (anomaly detection)\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "#word2vec\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#oversampling \n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "#jupiter display dataset without limits \n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for duplicates :11836 and 11836. Should be equeal\n",
      "Dataset filtering. Users before:86927 users after:11836\n",
      "Final dataset shape:(11836, 328)\n"
     ]
    }
   ],
   "source": [
    "def filter_users_by_label(df):\n",
    "    u_before = df.shape[0]\n",
    "    user_ids = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    target = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    \n",
    "    filtered_bots = set([int(x.split(\" \")[0]) for x in open(\"../data/bot_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "    filtered_clear = set([int(x.split(\" \")[0]) for x in open(\"../data/clear_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "\n",
    "    keep_ids = []\n",
    "    for x in np.where(target == 2)[0]:\n",
    "        if user_ids[x]  in filtered_bots:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 1\n",
    "\n",
    "    for x in np.where(target == 0)[0]:\n",
    "        if user_ids[x] in filtered_clear:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 0\n",
    "    \n",
    "    df[\"target\"] = target \n",
    "    df[\"user_id\"] = user_ids\n",
    "    print(\"Check for duplicates :{} and {}. Should be equeal\".format(len(keep_ids), len(set(keep_ids))))\n",
    "    df = df.iloc[keep_ids]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    u_after = df.shape[0]\n",
    "    print(\"Dataset filtering. Users before:{} users after:{}\".format(u_before, u_after))\n",
    "    return df\n",
    "    \n",
    "\n",
    "def graph_features(df):\n",
    "    graph = [(np.int64(x.split(\"\\\"\")[1]), np.int64(x.split(\"\\\"\")[3])) for x in open(\"../data/ret_graph.dot\", \"r\").read().split(\"\\n\") if \"->\" in x ]\n",
    "    #identify self loops in graphs \n",
    "    self_loop = set([a for a,b in graph if a == b]) \n",
    "    \n",
    "    #Get node degree data with weighted and un-weighted\n",
    "    degree_data = { np.int64(x.split(\",\")[0]) : x.split(\",\")[3:] for x in open(\"../data/ret_data_degree.csv\",\"r\").read().split(\"\\n\")[1:] if x != \"\"}\n",
    "    \n",
    "    #Assemble all features in single DataFrame\n",
    "    df[\"rt_self\"]  = np.array([1 if x in self_loop else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"in_degree\"] = np.array([degree_data[x][0] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"out_degree\"] = np.array([degree_data[x][1] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_in_degree\"] = np.array([degree_data[x][3] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_out_degree\"] = np.array([degree_data[x][4] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_degree\"] = np.array([degree_data[x][5] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    return df\n",
    "                               \n",
    "def word_2_vec(dataFrame):\n",
    "    model = Word2Vec.load(\"../data/word2vec_1_new.model\")\n",
    "    \n",
    "    dataframes = [dataFrame]\n",
    "    \n",
    "    labels_to_remove = []\n",
    "    labels = [\"mst_fr_ment_tw_word_\", \"mst_fr_ment_rt_word_\", \n",
    "              \"mst_fr_hs_tw_word_\", \"mst_fr_hs_rt_word_\", \n",
    "              \"words_frq_tw_\", \"words_frq_rt_\"]\n",
    "    for label in labels:\n",
    "        for i in range(1,4):\n",
    "            labels_to_remove.append(label+str(i))\n",
    "            temp = []\n",
    "            for word in dataFrame[label+str(i)]:\n",
    "                if word == \"_\":\n",
    "                    score = [0]*10\n",
    "                elif label == \"words_frq_rt_\" and i == 2 and word != word:\n",
    "                    score = model.wv['nan']\n",
    "                else:\n",
    "                    score = model.wv[word]\n",
    "                temp.append(score)\n",
    "            col = [\"{}{}_{}\".format(label,i,k) for k in range(10)]\n",
    "            dataframes.append(pd.DataFrame(temp, columns=col))\n",
    "    \n",
    "    \n",
    "    #Merge all dataframes and drop the text based columns that was used in word2vec model\n",
    "    merged = pd.concat(dataframes, axis=\"columns\")\n",
    "    merged = merged.drop(labels_to_remove, axis=\"columns\")\n",
    "    return merged\n",
    "\n",
    "def combine_data():\n",
    "    df = pd.read_csv(\"../data/new_vectors_new.csv\",sep = '\\t', header=0)\n",
    "    \n",
    "    #read labels from botometer and keep only users that agree to been bot or removed according to botometer\n",
    "    df = filter_users_by_label(df)\n",
    "    \n",
    "    #Read graph features \n",
    "    df = graph_features(df)\n",
    "                               \n",
    "    #transform word features with word2vec to numerical vectors  \n",
    "    df = word_2_vec(df)\n",
    "    df = df.drop([\"followers.1\"], axis=\"columns\")\n",
    "    print(\"Final dataset shape:{}\".format(df.shape))\n",
    "    \n",
    "    #save csv dataframe in order to reduce computation since the dataset remain same within multiple executions\n",
    "    df.to_csv (r'../data/features_large_with_words.csv', index = False, header=True)\n",
    "    \n",
    "\n",
    "combine_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data from csv file with features and merge it with graph features and labels from label file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of mix of bot labels from Botsentiel and Botometer\n",
    "### !!!! \n",
    "### Important combine_data are used only on Alex machine since i have all files that is combined to final csv file with all features no need to run \n",
    "### !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "def filter_users_by_label(df):\n",
    "    u_before = df.shape[0]\n",
    "    user_ids = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    target = df.iloc[:,-1]\n",
    "    df = df.iloc[:,:-1]\n",
    "    \n",
    "    filtered_bots = set([int(x.split(\" \")[0]) for x in open(\"../data/bot_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "    filtered_clear = set([int(x.split(\" \")[0]) for x in open(\"../data/clear_labels_jan.txt\",\"r\").read().split(\"\\n\") if x != ''])\n",
    "\n",
    "    keep_ids = []\n",
    "    for x in np.where(target == 2)[0]:\n",
    "        if user_ids[x]  in filtered_bots:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 1\n",
    "\n",
    "    for x in np.where(target == 0)[0]:\n",
    "        if user_ids[x] in filtered_clear:\n",
    "            keep_ids.append(x)\n",
    "            target[x] = 0\n",
    "    \n",
    "    df[\"target\"] = target \n",
    "    df[\"user_id\"] = user_ids\n",
    "    print(\"Check for duplicates :{} and {}. Should be equeal\".format(len(keep_ids), len(set(keep_ids))))\n",
    "    df = df.iloc[keep_ids]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    u_after = df.shape[0]\n",
    "    print(\"Dataset filtering. Users before:{} users after:{}\".format(u_before, u_after))\n",
    "    return df\n",
    "    \n",
    "\n",
    "def graph_features(df):\n",
    "    graph = [(np.int64(x.split(\"\\\"\")[1]), np.int64(x.split(\"\\\"\")[3])) for x in open(\"../data/ret_graph.dot\", \"r\").read().split(\"\\n\") if \"->\" in x ]\n",
    "    #identify self loops in graphs \n",
    "    self_loop = set([a for a,b in graph if a == b]) \n",
    "    \n",
    "    #Get node degree data with weighted and un-weighted\n",
    "    degree_data = { np.int64(x.split(\",\")[0]) : x.split(\",\")[3:] for x in open(\"../data/ret_data_degree.csv\",\"r\").read().split(\"\\n\")[1:] if x != \"\"}\n",
    "    \n",
    "    #Assemble all features in single DataFrame\n",
    "    df[\"rt_self\"]  = np.array([1 if x in self_loop else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"in_degree\"] = np.array([degree_data[x][0] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"out_degree\"] = np.array([degree_data[x][1] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_in_degree\"] = np.array([degree_data[x][3] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_out_degree\"] = np.array([degree_data[x][4] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    df[\"w_degree\"] = np.array([degree_data[x][5] if x in degree_data else 0 for x in df[\"user_id\"] ])\n",
    "    return df\n",
    "                               \n",
    "def word_2_vec(dataFrame):\n",
    "    model = Word2Vec.load(\"../data/word2vec_1_new.model\")\n",
    "    \n",
    "    dataframes = [dataFrame]\n",
    "    \n",
    "    labels_to_remove = []\n",
    "    labels = [\"mst_fr_ment_tw_word_\", \"mst_fr_ment_rt_word_\", \n",
    "              \"mst_fr_hs_tw_word_\", \"mst_fr_hs_rt_word_\", \n",
    "              \"words_frq_tw_\", \"words_frq_rt_\"]\n",
    "    for label in labels:\n",
    "        for i in range(1,4):\n",
    "            labels_to_remove.append(label+str(i))\n",
    "            temp = []\n",
    "            for word in dataFrame[label+str(i)]:\n",
    "                if word == \"_\":\n",
    "                    score = [0]*10\n",
    "                elif label == \"words_frq_rt_\" and i == 2 and word != word:\n",
    "                    score = model.wv['nan']\n",
    "                else:\n",
    "                    score = model.wv[word]\n",
    "                temp.append(score)\n",
    "            col = [\"{}{}_{}\".format(label,i,k) for k in range(10)]\n",
    "            dataframes.append(pd.DataFrame(temp, columns=col))\n",
    "    \n",
    "    \n",
    "    #Merge all dataframes and drop the text based columns that was used in word2vec model\n",
    "    merged = pd.concat(dataframes, axis=\"columns\")\n",
    "    merged = merged.drop(labels_to_remove, axis=\"columns\")\n",
    "    return merged\n",
    "\n",
    "def combine_data():\n",
    "    df = pd.read_csv(\"../data/new_vectors_new.csv\",sep = '\\t', header=0)\n",
    "    \n",
    "    #read labels from botometer and keep only users that agree to been bot or removed according to botometer\n",
    "    df = filter_users_by_label(df)\n",
    "    \n",
    "    #Read graph features \n",
    "    df = graph_features(df)\n",
    "                               \n",
    "    #transform word features with word2vec to numerical vectors  \n",
    "    df = word_2_vec(df)\n",
    "    df = df.drop([\"followers.1\"], axis=\"columns\")\n",
    "    print(\"Final dataset shape:{}\".format(df.shape))\n",
    "    \n",
    "    #save csv dataframe in order to reduce computation since the dataset remain same within multiple executions\n",
    "    df.to_csv (r'../data/features_large_with_words.csv', index = False, header=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#scale  data of each portion of dataset (train, evaluation and test)\n",
    "def scale_data(train, evaluation, test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(train), columns = train.columns)\n",
    "    # scale train and test\n",
    "    X_eval = pd.DataFrame(scaler.transform(evaluation), columns = evaluation.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(test), columns = test.columns)\n",
    "    return X_train, X_eval, X_test\n",
    "\n",
    "#oversample dataset portion\n",
    "#used in separated form for train and evalution datasets\n",
    "#do not use in test data at all\n",
    "def oversample(data, target):\n",
    "    smk = SMOTETomek()\n",
    "    data, target = smk.fit_sample(data, target)\n",
    "    return data, target\n",
    "\n",
    "#load twitter features\n",
    "def read_data(filename, verbose=False):\n",
    "    #read the csv file that was combined with word2vec and graph features\n",
    "    df = pd.read_csv(filename, header=0)\n",
    "    \n",
    "    #random shuffle the dataframe\n",
    "    df = shuffle(df)\n",
    "    \n",
    "    #extract user id from dataframe\n",
    "    user_ids = df[\"user_id\"]\n",
    "    \n",
    "    #extract target from datafrmae\n",
    "    target = df[\"target\"]\n",
    "    \n",
    "    df = df.drop([\"user_id\",\"target\"], axis=\"columns\")\n",
    "    \n",
    "    #make stratified train and test split 80/20 \n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, \n",
    "                                                        target,\n",
    "                                                        test_size=0.2,\n",
    "                                                        stratify=target)\n",
    "    \n",
    "    #scale features values with MinMaxScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    #scale train and test\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    #after scaling cast from np.array to dataframe again\n",
    "    X_test = pd.DataFrame(X_test, columns = df.columns)\n",
    "    X_train = pd.DataFrame(X_train, columns = df.columns)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "def kbest_features(data, labels,n_features):\n",
    "    #scaler = MinMaxScaler()\n",
    "    #scaled_data = scaler.fit(data).transform(data)\n",
    "\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=n_features)\n",
    "    fit = bestfeatures.fit(data, labels)\n",
    "    dfscores = pd.DataFrame(fit.scores_)\n",
    "    dfcolumns = pd.DataFrame(data.columns)\n",
    "    #concat two dataframes for better visualization \n",
    "    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n",
    "    featureScores.columns = ['Specs','Score']  #naming the dataframe columns\n",
    "    #print(featureScores.nlargest(n_features,'Score'))  #print 10 best features\n",
    "    res = featureScores.nlargest(n_features,'Score')\n",
    "    #print(dir(res))\n",
    "    res = [x[0] for x in res.values]\n",
    "    \n",
    "    embeded_rf_support = [True if x in res else False for x in df.columns.tolist()]\n",
    "    #res = [x[0] for x in res.values]\n",
    "    #res = res.get_support()\n",
    "    #embeded_rf_feature = data.loc[:,res].columns.tolist()\n",
    "    return data[res]\n",
    "\n",
    "def rforest_features(data, labels, n_features):\n",
    "    #scaler = MinMaxScaler()\n",
    "    #scaled_data = scaler.fit(data).transform(data)\n",
    "\n",
    "    embeded_rf_selector = SelectFromModel(RandomForestClassifier(n_estimators=100), max_features=n_features)\n",
    "    embeded_rf_selector.fit(data, labels)\n",
    "\n",
    "    embeded_rf_support = embeded_rf_selector.get_support()\n",
    "    embeded_rf_feature = data.loc[:,embeded_rf_support].columns.tolist()\n",
    "    #print(\"R-Forest selected features: {} number of features: {}\".format(embeded_rf_feature, len(embeded_rf_feature)))\n",
    "    return data[embeded_rf_feature], embeded_rf_feature\n",
    "\n",
    "def lasso_features(data, labels,n_features):\n",
    "    #scaler = StandardScaler()\n",
    "    #scaled_data= scaler.fit(data).transform(data)\n",
    "    #embeded_lr_selector = SelectFromModel(LogisticRegression(solver = \"sag\", penalty=\"l2\", max_iter=10000), \n",
    "    #                                      max_features=n_features)\n",
    "    embeded_lr_selector = SelectFromModel(LogisticRegression(solver = \"saga\", penalty=\"l2\", max_iter=10000), \n",
    "                                          max_features=n_features)\n",
    "    \n",
    "    embeded_lr_selector.fit(data, labels)\n",
    "    embeded_lr_support = embeded_lr_selector.get_support()\n",
    "    \n",
    "    #print(embeded_lr_support)\n",
    "    embeded_lr_feature = data.loc[:,embeded_lr_support].columns.tolist()\n",
    "    #print(\"Lasso selected features: {} number of features:{}\".format(embeded_lr_feature,len(embeded_lr_feature)))\n",
    "    #return data[:,embeded_lr_support], embeded_lr_feature\n",
    "    return data[embeded_lr_feature], embeded_lr_feature\n",
    "\n",
    "def lasso(data, labels, alpha = 0.02):\n",
    "    res = Lasso(alpha=alpha,selection='cyclic',max_iter=100000)\n",
    "    res.fit(data,labels)\n",
    "    mst = np.where(res.coef_ != 0.0)[0]\n",
    "    #print(mst)\n",
    "    #print(\"Features:{}\".format(list(data.columns[mst])))\n",
    "    #print(\"Number:{}\".format(len(list(data.columns[mst]))))\n",
    "    return data[list(data.columns[mst])], list(data.columns[mst])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimentionality reduction - Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "colors = ['#e6194b', '#3cb44b', '#ffe119', '#4363d8', \n",
    "              '#f58231', '#911eb4', '#46f0f0', '#f032e6', \n",
    "              '#bcf60c', '#fabebe', '#008080', '#e6beff', \n",
    "              '#9a6324', '#fffac8', '#800000', '#aaffc3', \n",
    "              '#808000', '#ffd8b1', '#000075', '#808080', \n",
    "              '#ffffff', \"#fe4a49\", \"#BD3430\", \"#fed766\"]\n",
    "\n",
    "#------------\n",
    "#---T-SNE----\n",
    "#------------\n",
    "def make_tsne(data_scaled, label, title, learning_rate=100, perplexity=20):\n",
    "    #tsne = TSNE(random_state=1)\n",
    "    tsne = TSNE(learning_rate=learning_rate,perplexity=perplexity, init='pca')\n",
    "    \n",
    "    #use fit_transform instead of fit, as TSNE has no transform method\n",
    "    digits_tsne = tsne.fit_transform(data_scaled)\n",
    "    #create color palette\n",
    "    palette = sns.color_palette(None, max(list(set(label))) + 1)\n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "    #print(len(palette))\n",
    "    c_color = [palette[x] if x != -1 else (0,0,0) for x in label]\n",
    "    \n",
    "    plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "    plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "    plt.scatter(digits_tsne[:, 0],digits_tsne[:, 1],c=c_color)\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('t-SNE projection {}'.format(title), fontsize=24)\n",
    "    \"\"\"\n",
    "    for i in range(len(digits_tsne[:, 0])):\n",
    "        # actually plot the digits as text instead of using scatter\n",
    "        plt.text(digits_tsne[i, 0], digits_tsne[i, 1],str(target[i]),\n",
    "                     color = colors[target[i]],\n",
    "                     fontdict={'weight': 'bold', 'size': 9})\n",
    "    \"\"\"\n",
    "    plt.xlabel(\"t-SNE feature 0\")\n",
    "    plt.xlabel(\"t-SNE feature 1\")\n",
    "    plt.show()\n",
    "    return digits_tsne\n",
    "\n",
    "#------------\n",
    "#----UMAP----\n",
    "#------------\n",
    "def perofrm_umap_clustering(data, labels,  title, n_neighbors=25, n_components=2,):\n",
    "    #reducer = umap.UMAP(n_neighbors=25,metric=\"manhattan\")\n",
    "    reducer = umap.UMAP(n_neighbors=n_neighbors, n_components= n_components, metric = \"euclidean\")#metric=\"euclidean\")\n",
    "    embedding = reducer.fit_transform(data)\n",
    "    embedding.shape\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        c=[sns.color_palette()[x] for x in labels])\n",
    "    plt.gca().set_aspect('equal', 'datalim')\n",
    "    plt.title('UMAP projection {}'.format(title), fontsize=24)\n",
    "    plt.show()\n",
    "    return embedding\n",
    "\n",
    "#------------\n",
    "#---DBSCAN---\n",
    "#------------\n",
    "def dbscan_clustering(positions, eps, comp_filter, target, scale=False):\n",
    "    #dbscan = DBSCAN(eps=0.0058)\n",
    "    #dbscan = DBSCAN(eps=0.0050)\n",
    "    #dbscan = DBSCAN(eps=0.0048)\n",
    "    #dbscan = DBSCAN(eps=0.0049)\n",
    "    dbscan = DBSCAN(eps=eps)\n",
    "    if scale:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(positions)\n",
    "        p_scaled = scaler.transform(positions)\n",
    "    else:\n",
    "        p_scaled = positions\n",
    "    clusters = dbscan.fit_predict(p_scaled)\n",
    "    \n",
    "    #print(\"Cluster memberships:\\n{}\".format(clusters))\n",
    "    #print(\"Unique slucter ids:{}\\n\\n\".format(set(clusters)))\n",
    "    #cmap = plt.cm.rainbow\n",
    "    #norm = matplotlib.colors.Normalize(vmin=1.5, vmax=4.5)\n",
    "    #print([(x/256.0, list(clusters).count(x)) for x in set(clusters)])\n",
    "    #print(len(sns.color_palette()))\n",
    "    #[print(sns.color_palette()[x]) for x in clusters]\n",
    "    \n",
    "    palette = sns.color_palette(None, len(set(clusters)))\n",
    "    \n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "    c_color = [palette[x] if x != -1 else (0,0,0) for x in clusters]\n",
    "    plt.scatter(p_scaled[:, 0], p_scaled[:, 1], color=c_color, cmap=mglearn.cm3, s=20)\n",
    "    #plt.scatter(p_scaled[clusters == 0, 0], p_scaled[clusters == 0, 1], color=colors[0], cmap=mglearn.cm3, s=20)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n",
    "    plt.show()\n",
    "    \n",
    "    components = defaultdict(lambda: set())\n",
    "    bot_comp = defaultdict(lambda:set())\n",
    "    clear_comp = defaultdict(lambda:set())\n",
    "    comp_size = []\n",
    "    \"\"\"\n",
    "    for i in range(len(clusters)):\n",
    "        #if target[i] == 1:\n",
    "        #    #print(clusters[i])\n",
    "        comp_size.append(clusters[i])\n",
    "    comp_size = set(comp_size)\n",
    "    \"\"\"\n",
    "    #max_comp = max(set(comp_size), key = comp_size.count)\n",
    "    #for i in range(len(clusters)):\n",
    "    #    if clusters[i] in comp_size :\n",
    "    #        #print(user_ids[i])\n",
    "    #        c_o[clusters[i]].add(i)\n",
    "    #        if target[i] == 1:\n",
    "    #            b[clusters[i]].add(i)\n",
    "    return clusters\n",
    "    for i in range(len(clusters)):\n",
    "        if clusters[i] == -1:\n",
    "            continue\n",
    "        components[clusters[i]].add(i)\n",
    "        if target[i] == 1:\n",
    "            bot_comp[clusters[i]].add(i)\n",
    "        if target[i] == 0:\n",
    "            clear_comp[clusters[i]].add(i)\n",
    "    \n",
    "    max_tp_bot = []\n",
    "    for comp in bot_comp:\n",
    "        if len(components[comp]) > comp_filter and len(bot_comp[comp])/len(components[comp]) > 0.5:\n",
    "            max_tp_bot.append((len(bot_comp[comp])/len(components[comp])) *100.0)\n",
    "            #print(\"TP :{} FP :{} Bperc: {} Comp size:{}\".format((len(b[comp])/len(c_o[comp])) *100.0, ((len(c_o[comp]) - len(b[comp]))/len(c_o[comp])) * 100.0 , len(b[comp])/len(target[target == 1]), len(c_o[comp])))\n",
    "    print(\"Bot accuracy Max:{} AVG:{} Min:{}\".format(max(max_tp_bot), sum(max_tp_bot)/len(max_tp_bot) , min(max_tp_bot)))\n",
    "    max_tp_clear = []\n",
    "    for comp in clear_comp:\n",
    "        if len(components[comp]) > comp_filter and len(clear_comp[comp])/len(components[comp]) > 0.5:\n",
    "            max_tp_clear.append((len(clear_comp[comp])/len(components[comp])) *100.0)\n",
    "    print(\"Clear accuracy Max:{} AVG:{} Min:{}\".format(max(max_tp_clear), sum(max_tp_clear)/len(max_tp_clear) , min(max_tp_clear)))\n",
    "\n",
    "#------------\n",
    "#---DBSCAN--- !!!!!!with clustering sampling !!!!!!!\n",
    "#------------\n",
    "def dbscan_clustering_with_smaple(positions, eps, comp_filter, bot_indx, clear_indx, target=None):\n",
    "    #dbscan = DBSCAN(eps=0.0058)\n",
    "    #dbscan = DBSCAN(eps=0.0050)\n",
    "    #dbscan = DBSCAN(eps=0.0048)\n",
    "    #dbscan = DBSCAN(eps=0.0049)\n",
    "    dbscan = DBSCAN(eps=eps)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(positions)\n",
    "    p_scaled = scaler.transform(positions)\n",
    "    clusters = dbscan.fit_predict(p_scaled)\n",
    "    #print(\"Cluster memberships:\\n{}\".format(clusters))\n",
    "    #print(\"Unique slucter ids:{}\\n\\n\".format(set(clusters)))\n",
    "    #cmap = plt.cm.rainbow\n",
    "    #norm = matplotlib.colors.Normalize(vmin=1.5, vmax=4.5)\n",
    "    #print([(x/256.0, list(clusters).count(x)) for x in set(clusters)])\n",
    "    #print(len(sns.color_palette()))\n",
    "    #[print(sns.color_palette()[x]) for x in clusters]\n",
    "   \n",
    "    palette = sns.color_palette(None, len(set(clusters)))\n",
    "    \n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "    c_color = [palette[x] if x != -1 else (0,0,0) for x in clusters]\n",
    "    plt.scatter(p_scaled[:, 0], p_scaled[:, 1], color=c_color, cmap=mglearn.cm3, s=20)\n",
    "    #plt.scatter(p_scaled[clusters == 0, 0], p_scaled[clusters == 0, 1], color=colors[0], cmap=mglearn.cm3, s=20)\n",
    "    plt.xlabel(\"Feature 0\")\n",
    "    plt.ylabel(\"Feature 1\")\n",
    "    plt.show()\n",
    "    \n",
    "    components = defaultdict(lambda: set())\n",
    "    bot_comp = defaultdict(lambda:set())\n",
    "    clear_comp = defaultdict(lambda:set())\n",
    "    comp_size = []\n",
    "    \"\"\"\n",
    "    for i in range(len(clusters)):\n",
    "        #if target[i] == 1:\n",
    "        #    #print(clusters[i])\n",
    "        comp_size.append(clusters[i])\n",
    "    comp_size = set(comp_size)\n",
    "    \"\"\"\n",
    "    #max_comp = max(set(comp_size), key = comp_size.count)\n",
    "    #for i in range(len(clusters)):\n",
    "    #    if clusters[i] in comp_size :\n",
    "    #        #print(user_ids[i])\n",
    "    #        c_o[clusters[i]].add(i)\n",
    "    #        if target[i] == 1:\n",
    "    #            b[clusters[i]].add(i)\n",
    "    comp_all = defaultdict(lambda: set())\n",
    "    \n",
    "    \n",
    "    for i in range(len(clusters)):\n",
    "        if clusters[i] == -1:\n",
    "            continue\n",
    "        comp_all[clusters[i]].add(i)\n",
    "        if i in bot_indx or i in clear_indx:\n",
    "            continue\n",
    "        components[clusters[i]].add(i)\n",
    "        if target[i] == 1:\n",
    "            bot_comp[clusters[i]].add(i)\n",
    "        if target[i] == 0:\n",
    "            clear_comp[clusters[i]].add(i)\n",
    "    \n",
    "    max_tp = []\n",
    "    max_fp = []\n",
    "    bot_indx = set(bot_indx)\n",
    "    clear_indx = set(clear_indx)\n",
    "    \n",
    "        \n",
    "    for comp in bot_comp:\n",
    "        #check comp size and purity \n",
    "        if len(components[comp]) > comp_filter and len(bot_comp[comp])/len(components[comp]) > 0.6:\n",
    "            #at this point componnent is labeled as bot component\n",
    "            \n",
    "            sample_bot_comp = len(comp_all[comp].intersection(bot_indx))\n",
    "            sample_clear_comp = len(comp_all[comp].intersection(clear_indx))\n",
    "            if sample_bot_comp != 0 :\n",
    "                max_tp.append( (sample_bot_comp / (sample_bot_comp+sample_clear_comp)) *100.0)\n",
    "                print(comp)\n",
    "                \n",
    "            if sample_clear_comp != 0 :\n",
    "                max_fp.append( (sample_clear_comp / (sample_clear_comp+sample_bot_comp)) *100.0 )\n",
    "            #print(\"TP :{} FP :{} Bperc: {} Comp size:{}\".format((len(b[comp])/len(c_o[comp])) *100.0, ((len(c_o[comp]) - len(b[comp]))/len(c_o[comp])) * 100.0 , len(b[comp])/len(target[target == 1]), len(c_o[comp])))\n",
    "    plt.figure()\n",
    "    for i in comp_all[9]:\n",
    "        if i in bot_indx:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], color = \"#a11515\")\n",
    "        elif i in clear_indx:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], color = \"#a11515\")\n",
    "        elif i in clear_comp[1]:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], c = \"#167a19\")\n",
    "        elif i in bot_comp[1]:\n",
    "            plt.scatter(p_scaled[i, 0], p_scaled[i, 1], c = \"#a11515\")\n",
    "    plt.show()\n",
    "    if len(max_tp) != 0:\n",
    "        print(\"Bot TP accuracy Max:{} AVG:{} Min:{} len:{}\".format(max(max_tp), sum(max_tp)/len(max_tp) , min(max_tp), len(max_tp)))\n",
    "    else:\n",
    "        print(\"Bot TP is Zero\")\n",
    "    if len(max_fp) != 0:\n",
    "        print(\"Bot FP Max:{} AVG:{} Min:{} len:{}\".format(max(max_fp), sum(max_fp)/len(max_fp) , min(max_fp), len(max_fp)))\n",
    "    else:\n",
    "        print(\"Bot FP is Zero\")\n",
    "    #max_tp_clear = []\n",
    "    #for comp in clear_comp:\n",
    "    #    if len(components[comp]) > comp_filter and len(clear_comp[comp])/len(components[comp]) > 0.9:\n",
    "    #        max_tp_clear.append((len(clear_comp[comp])/len(components[comp])) *100.0)\n",
    "    #print(\"Clear accuracy Max:{} AVG:{} Min:{}\".format(max(max_tp_clear), sum(max_tp_clear)/len(max_tp_clear) , min(max_tp_clear)))\n",
    "\n",
    "    \n",
    "#------------\n",
    "#-Mean-Shift- \n",
    "#------------\n",
    "def mean_shift(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    p_scaled = scaler.transform(data)\n",
    "    bandwidth = estimate_bandwidth(p_scaled, quantile=0.05, n_samples=500)\n",
    "    meanshift = MeanShift(bandwidth=bandwidth)\n",
    "    meanshift.fit(p_scaled)\n",
    "    labels = meanshift.labels_\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "    P = meanshift.predict(p_scaled)\n",
    "    palette = sns.color_palette(None, len(P))\n",
    "    col = [palette[x] if x != -1 else (0,0,0) for x in P]\n",
    "    plt.scatter(p_scaled[:,0], p_scaled[:,1], c=col, marker=\"o\", picker=True)\n",
    "    plt.title(f'Estimated number of clusters = {n_clusters_}')\n",
    "    plt.xlabel('Temperature yesterday')\n",
    "    plt.ylabel('Temperature today')\n",
    "    plt.show()\n",
    "    \n",
    "    c_o = defaultdict(lambda: set())\n",
    "    b = defaultdict(lambda:set())\n",
    "    comp_size = []\n",
    "    for i in range(len(P)):\n",
    "        if target[i] == 1:\n",
    "            #print(clusters[i])\n",
    "            comp_size.append(P[i])\n",
    "    comp_size = set(comp_size)\n",
    "    \n",
    "    #max_comp = max(set(comp_size), key = comp_size.count)\n",
    "    for i in range(len(P)):\n",
    "        if P[i] in comp_size :\n",
    "            #print(user_ids[i])\n",
    "            c_o[P[i]].add(i)\n",
    "            if target[i] == 1:\n",
    "                b[P[i]].add(i)\n",
    "    for comp in c_o:\n",
    "        if len(b[comp]) > 30 and len(c_o[comp]) > 50:\n",
    "            print(\"TP :{} FP :{} Bperc: {}\".format((len(b[comp])/len(c_o[comp])) *100.0, ((len(c_o[comp]) - len(b[comp]))/len(c_o[comp])) * 100.0 , len(b[comp])/len(target[target == 1]) ))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine features (Database, Graph and new labels ) and store them into csv file \n",
    "### !!!!\n",
    "### No need to run , execute next cell where data is readed from CSV file by filename\n",
    "### !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for duplicates :11836 and 11836. Should be equeal\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_rt_tw_0</th>\n",
       "      <th>daily_rt_tw_1</th>\n",
       "      <th>daily_rt_tw_2</th>\n",
       "      <th>daily_rt_tw_3</th>\n",
       "      <th>daily_rt_tw_4</th>\n",
       "      <th>daily_rt_tw_5</th>\n",
       "      <th>daily_rt_tw_6</th>\n",
       "      <th>daily_rt_0</th>\n",
       "      <th>daily_rt_1</th>\n",
       "      <th>daily_rt_2</th>\n",
       "      <th>daily_rt_3</th>\n",
       "      <th>daily_rt_4</th>\n",
       "      <th>daily_rt_5</th>\n",
       "      <th>daily_rt_6</th>\n",
       "      <th>daily_tw_0</th>\n",
       "      <th>daily_tw_1</th>\n",
       "      <th>daily_tw_2</th>\n",
       "      <th>daily_tw_3</th>\n",
       "      <th>daily_tw_4</th>\n",
       "      <th>daily_tw_5</th>\n",
       "      <th>daily_tw_6</th>\n",
       "      <th>hour_rt_tw_0</th>\n",
       "      <th>hour_rt_tw_1</th>\n",
       "      <th>hour_rt_tw_2</th>\n",
       "      <th>hour_rt_tw_3</th>\n",
       "      <th>hour_rt_tw_4</th>\n",
       "      <th>hour_rt_tw_5</th>\n",
       "      <th>hour_rt_tw_6</th>\n",
       "      <th>hour_rt_tw_7</th>\n",
       "      <th>hour_rt_tw_8</th>\n",
       "      <th>hour_rt_tw_9</th>\n",
       "      <th>hour_rt_tw_10</th>\n",
       "      <th>hour_rt_tw_11</th>\n",
       "      <th>hour_rt_tw_12</th>\n",
       "      <th>hour_rt_tw_13</th>\n",
       "      <th>hour_rt_tw_14</th>\n",
       "      <th>hour_rt_tw_15</th>\n",
       "      <th>hour_rt_tw_16</th>\n",
       "      <th>hour_rt_tw_17</th>\n",
       "      <th>hour_rt_tw_18</th>\n",
       "      <th>hour_rt_tw_19</th>\n",
       "      <th>hour_rt_tw_20</th>\n",
       "      <th>hour_rt_tw_21</th>\n",
       "      <th>hour_rt_tw_22</th>\n",
       "      <th>hour_rt_tw_23</th>\n",
       "      <th>hour_tw_0</th>\n",
       "      <th>hour_tw_1</th>\n",
       "      <th>hour_tw_2</th>\n",
       "      <th>hour_tw_3</th>\n",
       "      <th>hour_tw_4</th>\n",
       "      <th>hour_tw_5</th>\n",
       "      <th>hour_tw_6</th>\n",
       "      <th>hour_tw_7</th>\n",
       "      <th>hour_tw_8</th>\n",
       "      <th>hour_tw_9</th>\n",
       "      <th>hour_tw_10</th>\n",
       "      <th>hour_tw_11</th>\n",
       "      <th>hour_tw_12</th>\n",
       "      <th>hour_tw_13</th>\n",
       "      <th>hour_tw_14</th>\n",
       "      <th>hour_tw_15</th>\n",
       "      <th>hour_tw_16</th>\n",
       "      <th>hour_tw_17</th>\n",
       "      <th>hour_tw_18</th>\n",
       "      <th>hour_tw_19</th>\n",
       "      <th>hour_tw_20</th>\n",
       "      <th>hour_tw_21</th>\n",
       "      <th>hour_tw_22</th>\n",
       "      <th>hour_tw_23</th>\n",
       "      <th>hour_rt_0</th>\n",
       "      <th>hour_rt_1</th>\n",
       "      <th>hour_rt_2</th>\n",
       "      <th>hour_rt_3</th>\n",
       "      <th>hour_rt_4</th>\n",
       "      <th>hour_rt_5</th>\n",
       "      <th>hour_rt_6</th>\n",
       "      <th>hour_rt_7</th>\n",
       "      <th>hour_rt_8</th>\n",
       "      <th>hour_rt_9</th>\n",
       "      <th>hour_rt_10</th>\n",
       "      <th>hour_rt_11</th>\n",
       "      <th>hour_rt_12</th>\n",
       "      <th>hour_rt_13</th>\n",
       "      <th>hour_rt_14</th>\n",
       "      <th>hour_rt_15</th>\n",
       "      <th>hour_rt_16</th>\n",
       "      <th>hour_rt_17</th>\n",
       "      <th>hour_rt_18</th>\n",
       "      <th>hour_rt_19</th>\n",
       "      <th>hour_rt_20</th>\n",
       "      <th>hour_rt_21</th>\n",
       "      <th>hour_rt_22</th>\n",
       "      <th>hour_rt_23</th>\n",
       "      <th>mst_fr_ment_tw_1</th>\n",
       "      <th>mst_fr_ment_tw_2</th>\n",
       "      <th>mst_fr_ment_tw_3</th>\n",
       "      <th>mst_fr_ment_rt_1</th>\n",
       "      <th>mst_fr_ment_rt_2</th>\n",
       "      <th>mst_fr_ment_rt_3</th>\n",
       "      <th>mst_fr_hs_tw_1</th>\n",
       "      <th>mst_fr_hs_tw_2</th>\n",
       "      <th>mst_fr_hs_tw_3</th>\n",
       "      <th>mst_fr_hs_rt_1</th>\n",
       "      <th>mst_fr_hs_rt_2</th>\n",
       "      <th>mst_fr_hs_rt_3</th>\n",
       "      <th>tw_urls_avg</th>\n",
       "      <th>tw_urls_std</th>\n",
       "      <th>rt_urls_avg</th>\n",
       "      <th>rt_urls_std</th>\n",
       "      <th>tw_hash_avg</th>\n",
       "      <th>tw_hash_std</th>\n",
       "      <th>tw_ment_avg</th>\n",
       "      <th>tw_ment_std</th>\n",
       "      <th>rt_hash_avg</th>\n",
       "      <th>rt_hash_std</th>\n",
       "      <th>rt_ment_avg</th>\n",
       "      <th>rt_ment_std</th>\n",
       "      <th>rt_time_avg</th>\n",
       "      <th>rt_time_min</th>\n",
       "      <th>rt_time_max</th>\n",
       "      <th>rt_time_std</th>\n",
       "      <th>rt_avg</th>\n",
       "      <th>tw_avg</th>\n",
       "      <th>tw_rt_ration</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>favourites</th>\n",
       "      <th>listed</th>\n",
       "      <th>statuses</th>\n",
       "      <th>followers.1</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>name_len</th>\n",
       "      <th>name_screen_sim</th>\n",
       "      <th>geo</th>\n",
       "      <th>protected</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>description_len</th>\n",
       "      <th>bckg_img</th>\n",
       "      <th>default_prof</th>\n",
       "      <th>entities</th>\n",
       "      <th>rt_self</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>out_degree</th>\n",
       "      <th>w_in_degree</th>\n",
       "      <th>w_out_degree</th>\n",
       "      <th>w_degree</th>\n",
       "      <th>target</th>\n",
       "      <th>user_id</th>\n",
       "      <th>hs_tw_v_0</th>\n",
       "      <th>hs_tw_v_1</th>\n",
       "      <th>hs_tw_v_2</th>\n",
       "      <th>hs_tw_v_3</th>\n",
       "      <th>hs_tw_v_4</th>\n",
       "      <th>hs_tw_v_5</th>\n",
       "      <th>hs_tw_v_6</th>\n",
       "      <th>hs_tw_v_7</th>\n",
       "      <th>hs_tw_v_8</th>\n",
       "      <th>hs_tw_v_9</th>\n",
       "      <th>hs_rt_v_0</th>\n",
       "      <th>hs_rt_v_1</th>\n",
       "      <th>hs_rt_v_2</th>\n",
       "      <th>hs_rt_v_3</th>\n",
       "      <th>hs_rt_v_4</th>\n",
       "      <th>hs_rt_v_5</th>\n",
       "      <th>hs_rt_v_6</th>\n",
       "      <th>hs_rt_v_7</th>\n",
       "      <th>hs_rt_v_8</th>\n",
       "      <th>hs_rt_v_9</th>\n",
       "      <th>ment_tw_v_0</th>\n",
       "      <th>ment_tw_v_1</th>\n",
       "      <th>ment_tw_v_2</th>\n",
       "      <th>ment_tw_v_3</th>\n",
       "      <th>ment_tw_v_4</th>\n",
       "      <th>ment_tw_v_5</th>\n",
       "      <th>ment_tw_v_6</th>\n",
       "      <th>ment_tw_v_7</th>\n",
       "      <th>ment_tw_v_8</th>\n",
       "      <th>ment_tw_v_9</th>\n",
       "      <th>ment_rt_v_0</th>\n",
       "      <th>ment_rt_v_1</th>\n",
       "      <th>ment_rt_v_2</th>\n",
       "      <th>ment_rt_v_3</th>\n",
       "      <th>ment_rt_v_4</th>\n",
       "      <th>ment_rt_v_5</th>\n",
       "      <th>ment_rt_v_6</th>\n",
       "      <th>ment_rt_v_7</th>\n",
       "      <th>ment_rt_v_8</th>\n",
       "      <th>ment_rt_v_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.178988</td>\n",
       "      <td>0.112840</td>\n",
       "      <td>0.128405</td>\n",
       "      <td>0.070039</td>\n",
       "      <td>0.182879</td>\n",
       "      <td>0.128405</td>\n",
       "      <td>0.198444</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.175141</td>\n",
       "      <td>0.124294</td>\n",
       "      <td>0.158192</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.180791</td>\n",
       "      <td>0.107345</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>0.042802</td>\n",
       "      <td>0.035019</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.038911</td>\n",
       "      <td>0.046693</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>0.019455</td>\n",
       "      <td>0.042802</td>\n",
       "      <td>0.073930</td>\n",
       "      <td>0.077821</td>\n",
       "      <td>0.066148</td>\n",
       "      <td>0.105058</td>\n",
       "      <td>0.116732</td>\n",
       "      <td>0.163424</td>\n",
       "      <td>0.054475</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028249</td>\n",
       "      <td>0.039548</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.011299</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.056497</td>\n",
       "      <td>0.084746</td>\n",
       "      <td>0.073446</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.129944</td>\n",
       "      <td>0.146893</td>\n",
       "      <td>0.045198</td>\n",
       "      <td>0.00565</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.012500</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.112500</td>\n",
       "      <td>0.087500</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.0125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>5.980586</td>\n",
       "      <td>6.887675</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>5.980586</td>\n",
       "      <td>9.177593</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>6.667747</td>\n",
       "      <td>3.043134</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>4.803376</td>\n",
       "      <td>6.667747</td>\n",
       "      <td>0.169492</td>\n",
       "      <td>0.411693</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>5.016949</td>\n",
       "      <td>5.633836</td>\n",
       "      <td>1.988701</td>\n",
       "      <td>2.665960</td>\n",
       "      <td>3.962500</td>\n",
       "      <td>4.602988</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>2.915476</td>\n",
       "      <td>171.483958</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>1292.233333</td>\n",
       "      <td>330.839721</td>\n",
       "      <td>2.758621</td>\n",
       "      <td>6.103448</td>\n",
       "      <td>2.212500</td>\n",
       "      <td>0</td>\n",
       "      <td>343</td>\n",
       "      <td>7349</td>\n",
       "      <td>0</td>\n",
       "      <td>3502</td>\n",
       "      <td>343</td>\n",
       "      <td>151</td>\n",
       "      <td>14</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>166</td>\n",
       "      <td>742</td>\n",
       "      <td>1</td>\n",
       "      <td>1258962548503740425</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.951101</td>\n",
       "      <td>2.923159</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>9.150005</td>\n",
       "      <td>9.552075</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>6.059891</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.322876</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>2.061553</td>\n",
       "      <td>169.037500</td>\n",
       "      <td>7.183333</td>\n",
       "      <td>432.416667</td>\n",
       "      <td>241.493538</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>1463</td>\n",
       "      <td>0</td>\n",
       "      <td>1057</td>\n",
       "      <td>172</td>\n",
       "      <td>403</td>\n",
       "      <td>21</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1277260249687261186</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>-2.666591</td>\n",
       "      <td>3.297360</td>\n",
       "      <td>0.108515</td>\n",
       "      <td>1.148872</td>\n",
       "      <td>-4.909317</td>\n",
       "      <td>4.969856</td>\n",
       "      <td>2.520902</td>\n",
       "      <td>0.120532</td>\n",
       "      <td>-3.473754</td>\n",
       "      <td>-1.942731</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>11.871754</td>\n",
       "      <td>6.725062</td>\n",
       "      <td>5.916363</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>2.488408</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>2.049390</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.264911</td>\n",
       "      <td>513.326667</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>1020.416667</td>\n",
       "      <td>635.460221</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>281</td>\n",
       "      <td>36991</td>\n",
       "      <td>4</td>\n",
       "      <td>37368</td>\n",
       "      <td>281</td>\n",
       "      <td>462</td>\n",
       "      <td>15</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>746102335915556868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.736998</td>\n",
       "      <td>-0.069259</td>\n",
       "      <td>-0.292788</td>\n",
       "      <td>-0.561506</td>\n",
       "      <td>-0.082709</td>\n",
       "      <td>0.042723</td>\n",
       "      <td>0.180475</td>\n",
       "      <td>-0.226081</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.524840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.05000</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.896686</td>\n",
       "      <td>7.102824</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>5.161735</td>\n",
       "      <td>5.840240</td>\n",
       "      <td>6.162355</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>3.585302</td>\n",
       "      <td>12.348678</td>\n",
       "      <td>4.551958</td>\n",
       "      <td>8.989075</td>\n",
       "      <td>7.998400</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.836660</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.987461</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>2.692582</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>362.737500</td>\n",
       "      <td>12.116667</td>\n",
       "      <td>733.633333</td>\n",
       "      <td>475.360941</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>8697</td>\n",
       "      <td>0</td>\n",
       "      <td>4739</td>\n",
       "      <td>136</td>\n",
       "      <td>163</td>\n",
       "      <td>4</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1231691345271586824</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>-4.284786</td>\n",
       "      <td>7.481483</td>\n",
       "      <td>6.267010</td>\n",
       "      <td>4.508372</td>\n",
       "      <td>11.494592</td>\n",
       "      <td>-1.333691</td>\n",
       "      <td>-2.366664</td>\n",
       "      <td>0.944676</td>\n",
       "      <td>-3.689538</td>\n",
       "      <td>-7.882071</td>\n",
       "      <td>1.424672</td>\n",
       "      <td>0.269191</td>\n",
       "      <td>2.436769</td>\n",
       "      <td>3.364856</td>\n",
       "      <td>-2.865908</td>\n",
       "      <td>5.763546</td>\n",
       "      <td>3.289723</td>\n",
       "      <td>-7.134625</td>\n",
       "      <td>-1.938984</td>\n",
       "      <td>3.373724</td>\n",
       "      <td>-0.913174</td>\n",
       "      <td>4.238219</td>\n",
       "      <td>-0.241484</td>\n",
       "      <td>4.171506</td>\n",
       "      <td>-2.921207</td>\n",
       "      <td>6.198004</td>\n",
       "      <td>3.403417</td>\n",
       "      <td>-6.699732</td>\n",
       "      <td>-2.091649</td>\n",
       "      <td>-0.219933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.896686</td>\n",
       "      <td>10.566221</td>\n",
       "      <td>10.750414</td>\n",
       "      <td>6.268109</td>\n",
       "      <td>5.161735</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>1.266133</td>\n",
       "      <td>6.516144</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.845154</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1.195229</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>697.972222</td>\n",
       "      <td>521.350000</td>\n",
       "      <td>861.083333</td>\n",
       "      <td>711.683184</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.241379</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>241</td>\n",
       "      <td>5757</td>\n",
       "      <td>0</td>\n",
       "      <td>9201</td>\n",
       "      <td>241</td>\n",
       "      <td>492</td>\n",
       "      <td>2</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>1518469190</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>3.493951</td>\n",
       "      <td>7.603045</td>\n",
       "      <td>-1.862264</td>\n",
       "      <td>10.015945</td>\n",
       "      <td>-0.595800</td>\n",
       "      <td>4.668619</td>\n",
       "      <td>3.420092</td>\n",
       "      <td>-1.148644</td>\n",
       "      <td>-2.369110</td>\n",
       "      <td>-4.642322</td>\n",
       "      <td>1.424672</td>\n",
       "      <td>0.269191</td>\n",
       "      <td>2.436769</td>\n",
       "      <td>3.364856</td>\n",
       "      <td>-2.865908</td>\n",
       "      <td>5.763546</td>\n",
       "      <td>3.289723</td>\n",
       "      <td>-7.134625</td>\n",
       "      <td>-1.938984</td>\n",
       "      <td>3.373724</td>\n",
       "      <td>2.414909</td>\n",
       "      <td>-3.960131</td>\n",
       "      <td>-0.615269</td>\n",
       "      <td>2.886237</td>\n",
       "      <td>-8.917027</td>\n",
       "      <td>2.754604</td>\n",
       "      <td>-1.793620</td>\n",
       "      <td>-7.303990</td>\n",
       "      <td>-5.697265</td>\n",
       "      <td>4.512267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>7.810123</td>\n",
       "      <td>11.525478</td>\n",
       "      <td>8.102567</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>4.150451</td>\n",
       "      <td>3.265572</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.522233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.636364</td>\n",
       "      <td>3.343923</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.674200</td>\n",
       "      <td>211.589394</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>1145.433333</td>\n",
       "      <td>378.904435</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "      <td>88977</td>\n",
       "      <td>0</td>\n",
       "      <td>15392</td>\n",
       "      <td>341</td>\n",
       "      <td>963</td>\n",
       "      <td>5</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1043726264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.481577</td>\n",
       "      <td>-4.951790</td>\n",
       "      <td>11.181436</td>\n",
       "      <td>8.863973</td>\n",
       "      <td>3.241972</td>\n",
       "      <td>-4.242408</td>\n",
       "      <td>-0.542843</td>\n",
       "      <td>3.072947</td>\n",
       "      <td>-7.411512</td>\n",
       "      <td>-3.821877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.205255</td>\n",
       "      <td>-13.955618</td>\n",
       "      <td>1.322916</td>\n",
       "      <td>3.720930</td>\n",
       "      <td>-6.821259</td>\n",
       "      <td>-13.958460</td>\n",
       "      <td>-3.371006</td>\n",
       "      <td>-5.193164</td>\n",
       "      <td>2.854197</td>\n",
       "      <td>0.453024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.011174</td>\n",
       "      <td>4.176059</td>\n",
       "      <td>4.977433</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>2.571623</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>2.569047</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.341641</td>\n",
       "      <td>337.120000</td>\n",
       "      <td>50.016667</td>\n",
       "      <td>962.716667</td>\n",
       "      <td>444.474572</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>266</td>\n",
       "      <td>4146</td>\n",
       "      <td>4</td>\n",
       "      <td>2577</td>\n",
       "      <td>266</td>\n",
       "      <td>564</td>\n",
       "      <td>9</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>2542010298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.282885</td>\n",
       "      <td>4.259559</td>\n",
       "      <td>4.080622</td>\n",
       "      <td>8.733304</td>\n",
       "      <td>-4.353861</td>\n",
       "      <td>4.428511</td>\n",
       "      <td>0.595667</td>\n",
       "      <td>-2.255075</td>\n",
       "      <td>-0.706327</td>\n",
       "      <td>-2.917101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>4.977433</td>\n",
       "      <td>5.120814</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.415885</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>5.213256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>1.936492</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.033333</td>\n",
       "      <td>0.316667</td>\n",
       "      <td>19.533333</td>\n",
       "      <td>9.885420</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>22348</td>\n",
       "      <td>0</td>\n",
       "      <td>29246</td>\n",
       "      <td>96</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2520645596</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.481577</td>\n",
       "      <td>-4.951790</td>\n",
       "      <td>11.181436</td>\n",
       "      <td>8.863973</td>\n",
       "      <td>3.241972</td>\n",
       "      <td>-4.242408</td>\n",
       "      <td>-0.542843</td>\n",
       "      <td>3.072947</td>\n",
       "      <td>-7.411512</td>\n",
       "      <td>-3.821877</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.206959</td>\n",
       "      <td>-2.618856</td>\n",
       "      <td>1.212161</td>\n",
       "      <td>-3.037526</td>\n",
       "      <td>-1.402811</td>\n",
       "      <td>1.444823</td>\n",
       "      <td>12.596374</td>\n",
       "      <td>0.780529</td>\n",
       "      <td>6.377641</td>\n",
       "      <td>-2.976061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.803406</td>\n",
       "      <td>2.923159</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.923159</td>\n",
       "      <td>4.669607</td>\n",
       "      <td>8.837133</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>7.183257</td>\n",
       "      <td>7.119354</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>2.488408</td>\n",
       "      <td>3.265572</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.816497</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.972092</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.577350</td>\n",
       "      <td>3.222222</td>\n",
       "      <td>5.364492</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.452966</td>\n",
       "      <td>317.183333</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>1312.000000</td>\n",
       "      <td>593.041953</td>\n",
       "      <td>0.310345</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td>2580</td>\n",
       "      <td>4078</td>\n",
       "      <td>97</td>\n",
       "      <td>122644</td>\n",
       "      <td>2580</td>\n",
       "      <td>961</td>\n",
       "      <td>11</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>97</td>\n",
       "      <td>186</td>\n",
       "      <td>0</td>\n",
       "      <td>14811111</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>-1.483979</td>\n",
       "      <td>1.254190</td>\n",
       "      <td>9.255416</td>\n",
       "      <td>5.533646</td>\n",
       "      <td>1.226344</td>\n",
       "      <td>4.875967</td>\n",
       "      <td>0.917352</td>\n",
       "      <td>-4.030603</td>\n",
       "      <td>-2.897294</td>\n",
       "      <td>-0.845497</td>\n",
       "      <td>-5.901112</td>\n",
       "      <td>3.408430</td>\n",
       "      <td>5.464797</td>\n",
       "      <td>5.483321</td>\n",
       "      <td>-3.814475</td>\n",
       "      <td>3.054348</td>\n",
       "      <td>1.276991</td>\n",
       "      <td>0.458579</td>\n",
       "      <td>-1.790708</td>\n",
       "      <td>-1.426023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>2.084814</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1117713</td>\n",
       "      <td>8621</td>\n",
       "      <td>1244</td>\n",
       "      <td>24671</td>\n",
       "      <td>1117713</td>\n",
       "      <td>597</td>\n",
       "      <td>14</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>569</td>\n",
       "      <td>0</td>\n",
       "      <td>903</td>\n",
       "      <td>1</td>\n",
       "      <td>904</td>\n",
       "      <td>0</td>\n",
       "      <td>1064042478</td>\n",
       "      <td>-6.481514</td>\n",
       "      <td>-0.044037</td>\n",
       "      <td>7.989522</td>\n",
       "      <td>11.762167</td>\n",
       "      <td>6.681043</td>\n",
       "      <td>-3.275398</td>\n",
       "      <td>4.377571</td>\n",
       "      <td>5.375144</td>\n",
       "      <td>-5.796048</td>\n",
       "      <td>-2.582479</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11836 rows  189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       daily_rt_tw_0  daily_rt_tw_1  daily_rt_tw_2  daily_rt_tw_3  \\\n",
       "0           0.178988       0.112840       0.128405       0.070039   \n",
       "1           0.000000       0.400000       0.000000       0.400000   \n",
       "2           0.100000       0.200000       0.000000       0.200000   \n",
       "3           0.000000       0.250000       0.041667       0.041667   \n",
       "4           0.000000       0.400000       0.200000       0.100000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.090909       0.363636       0.090909       0.181818   \n",
       "11832       0.100000       0.100000       0.100000       0.200000   \n",
       "11833       0.250000       0.000000       0.250000       0.500000   \n",
       "11834       0.133333       0.000000       0.133333       0.266667   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       daily_rt_tw_4  daily_rt_tw_5  daily_rt_tw_6  daily_rt_0  daily_rt_1  \\\n",
       "0           0.182879       0.128405       0.198444    0.187500    0.087500   \n",
       "1           0.000000       0.000000       0.200000    0.000000    0.250000   \n",
       "2           0.100000       0.200000       0.200000    0.100000    0.200000   \n",
       "3           0.375000       0.125000       0.166667    0.000000    0.250000   \n",
       "4           0.100000       0.200000       0.000000    0.000000    0.000000   \n",
       "...              ...            ...            ...         ...         ...   \n",
       "11831       0.000000       0.272727       0.000000    0.090909    0.363636   \n",
       "11832       0.400000       0.000000       0.100000    0.100000    0.100000   \n",
       "11833       0.000000       0.000000       0.000000    0.250000    0.000000   \n",
       "11834       0.400000       0.000000       0.066667    0.222222    0.000000   \n",
       "11835       0.000000       1.000000       0.000000    0.000000    0.000000   \n",
       "\n",
       "       daily_rt_2  daily_rt_3  daily_rt_4  daily_rt_5  daily_rt_6  daily_tw_0  \\\n",
       "0        0.062500    0.000000    0.187500    0.175000    0.300000    0.175141   \n",
       "1        0.000000    0.500000    0.000000    0.000000    0.250000    0.000000   \n",
       "2        0.000000    0.200000    0.100000    0.200000    0.200000    0.000000   \n",
       "3        0.250000    0.250000    0.000000    0.250000    0.000000    0.000000   \n",
       "4        0.666667    0.000000    0.000000    0.333333    0.000000    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.090909    0.181818    0.000000    0.272727    0.000000    0.000000   \n",
       "11832    0.100000    0.200000    0.400000    0.000000    0.100000    0.000000   \n",
       "11833    0.250000    0.500000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834    0.111111    0.222222    0.333333    0.000000    0.111111    0.000000   \n",
       "11835    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       daily_tw_1  daily_tw_2  daily_tw_3  daily_tw_4  daily_tw_5  daily_tw_6  \\\n",
       "0        0.124294    0.158192    0.101695    0.180791    0.107345    0.152542   \n",
       "1        1.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2        0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "3        0.250000    0.000000    0.000000    0.450000    0.100000    0.200000   \n",
       "4        0.571429    0.000000    0.142857    0.142857    0.142857    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11832    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11833    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834    0.000000    0.166667    0.333333    0.500000    0.000000    0.000000   \n",
       "11835    0.000000    0.000000    0.000000    0.000000    1.000000    0.000000   \n",
       "\n",
       "       hour_rt_tw_0  hour_rt_tw_1  hour_rt_tw_2  hour_rt_tw_3  hour_rt_tw_4  \\\n",
       "0               0.0      0.000000           0.0      0.019455      0.015564   \n",
       "1               0.0      0.000000           0.0      0.000000      0.000000   \n",
       "2               0.0      0.100000           0.0      0.100000      0.000000   \n",
       "3               0.0      0.000000           0.0      0.041667      0.083333   \n",
       "4               0.0      0.000000           0.1      0.100000      0.000000   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "11831           0.0      0.272727           0.0      0.272727      0.000000   \n",
       "11832           0.1      0.000000           0.1      0.000000      0.000000   \n",
       "11833           0.0      0.500000           0.0      0.000000      0.000000   \n",
       "11834           0.0      0.000000           0.0      0.000000      0.000000   \n",
       "11835           1.0      0.000000           0.0      0.000000      0.000000   \n",
       "\n",
       "       hour_rt_tw_5  hour_rt_tw_6  hour_rt_tw_7  hour_rt_tw_8  hour_rt_tw_9  \\\n",
       "0          0.042802      0.035019      0.007782      0.038911      0.046693   \n",
       "1          0.000000      0.000000      0.000000      0.200000      0.000000   \n",
       "2          0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "3          0.208333      0.000000      0.083333      0.000000      0.000000   \n",
       "4          0.000000      0.000000      0.100000      0.000000      0.000000   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "11831      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "11832      0.100000      0.000000      0.100000      0.000000      0.000000   \n",
       "11833      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "11834      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "11835      0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "\n",
       "       hour_rt_tw_10  hour_rt_tw_11  hour_rt_tw_12  hour_rt_tw_13  \\\n",
       "0           0.054475       0.011673       0.019455       0.042802   \n",
       "1           0.000000       0.000000       0.000000       0.000000   \n",
       "2           0.000000       0.100000       0.000000       0.200000   \n",
       "3           0.000000       0.000000       0.000000       0.000000   \n",
       "4           0.000000       0.500000       0.000000       0.200000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.000000       0.000000       0.000000       0.000000   \n",
       "11832       0.000000       0.000000       0.100000       0.200000   \n",
       "11833       0.000000       0.000000       0.000000       0.000000   \n",
       "11834       0.000000       0.066667       0.066667       0.133333   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       hour_rt_tw_14  hour_rt_tw_15  hour_rt_tw_16  hour_rt_tw_17  \\\n",
       "0           0.073930       0.077821       0.066148       0.105058   \n",
       "1           0.000000       0.000000       0.000000       0.000000   \n",
       "2           0.200000       0.000000       0.000000       0.000000   \n",
       "3           0.000000       0.041667       0.041667       0.000000   \n",
       "4           0.000000       0.000000       0.000000       0.000000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.000000       0.181818       0.000000       0.000000   \n",
       "11832       0.000000       0.100000       0.000000       0.000000   \n",
       "11833       0.000000       0.000000       0.000000       0.000000   \n",
       "11834       0.133333       0.000000       0.333333       0.133333   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       hour_rt_tw_18  hour_rt_tw_19  hour_rt_tw_20  hour_rt_tw_21  \\\n",
       "0           0.116732       0.163424       0.054475       0.007782   \n",
       "1           0.000000       0.000000       0.200000       0.200000   \n",
       "2           0.000000       0.000000       0.100000       0.000000   \n",
       "3           0.000000       0.000000       0.041667       0.041667   \n",
       "4           0.000000       0.000000       0.000000       0.000000   \n",
       "...              ...            ...            ...            ...   \n",
       "11831       0.000000       0.000000       0.000000       0.000000   \n",
       "11832       0.000000       0.200000       0.000000       0.000000   \n",
       "11833       0.000000       0.000000       0.000000       0.250000   \n",
       "11834       0.066667       0.000000       0.066667       0.000000   \n",
       "11835       0.000000       0.000000       0.000000       0.000000   \n",
       "\n",
       "       hour_rt_tw_22  hour_rt_tw_23  hour_tw_0  hour_tw_1  hour_tw_2  \\\n",
       "0           0.000000       0.000000        0.0        0.0   0.000000   \n",
       "1           0.000000       0.400000        0.0        0.0   0.000000   \n",
       "2           0.100000       0.100000        0.0        0.0   0.000000   \n",
       "3           0.375000       0.041667        0.0        0.0   0.000000   \n",
       "4           0.000000       0.000000        0.0        0.0   0.142857   \n",
       "...              ...            ...        ...        ...        ...   \n",
       "11831       0.181818       0.090909        0.0        0.0   0.000000   \n",
       "11832       0.000000       0.000000        0.0        0.0   0.000000   \n",
       "11833       0.250000       0.000000        0.0        0.0   0.000000   \n",
       "11834       0.000000       0.000000        0.0        0.0   0.000000   \n",
       "11835       0.000000       0.000000        1.0        0.0   0.000000   \n",
       "\n",
       "       hour_tw_3  hour_tw_4  hour_tw_5  hour_tw_6  hour_tw_7  hour_tw_8  \\\n",
       "0       0.028249    0.00565   0.050847   0.045198   0.000000   0.028249   \n",
       "1       0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "2       0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "3       0.050000    0.05000   0.250000   0.000000   0.000000   0.000000   \n",
       "4       0.142857    0.00000   0.000000   0.000000   0.142857   0.000000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11832   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11833   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11834   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "11835   0.000000    0.00000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "       hour_tw_9  hour_tw_10  hour_tw_11  hour_tw_12  hour_tw_13  hour_tw_14  \\\n",
       "0       0.039548    0.056497    0.011299    0.022599    0.056497    0.084746   \n",
       "1       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "2       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "3       0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "4       0.000000    0.000000    0.428571    0.000000    0.142857    0.000000   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "11831   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11832   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11833   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834   0.000000    0.000000    0.000000    0.166667    0.166667    0.166667   \n",
       "11835   0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       hour_tw_15  hour_tw_16  hour_tw_17  hour_tw_18  hour_tw_19  hour_tw_20  \\\n",
       "0        0.073446    0.067797    0.101695    0.129944    0.146893    0.045198   \n",
       "1        0.000000    0.000000    0.000000    0.000000    0.000000    1.000000   \n",
       "2        0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "3        0.050000    0.000000    0.000000    0.000000    0.000000    0.050000   \n",
       "4        0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11832    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11833    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "11834    0.000000    0.166667    0.166667    0.000000    0.000000    0.166667   \n",
       "11835    0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "\n",
       "       hour_tw_21  hour_tw_22  hour_tw_23  hour_rt_0  hour_rt_1  hour_rt_2  \\\n",
       "0         0.00565        0.00        0.00        0.0   0.000000        0.0   \n",
       "1         0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "2         0.00000        0.00        0.00        0.0   0.100000        0.0   \n",
       "3         0.05000        0.45        0.05        0.0   0.000000        0.0   \n",
       "4         0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "...           ...         ...         ...        ...        ...        ...   \n",
       "11831     0.00000        0.00        0.00        0.0   0.272727        0.0   \n",
       "11832     0.00000        0.00        0.00        0.1   0.000000        0.1   \n",
       "11833     0.00000        0.00        0.00        0.0   0.500000        0.0   \n",
       "11834     0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "11835     0.00000        0.00        0.00        0.0   0.000000        0.0   \n",
       "\n",
       "       hour_rt_3  hour_rt_4  hour_rt_5  hour_rt_6  hour_rt_7  hour_rt_8  \\\n",
       "0       0.000000     0.0375      0.025     0.0125      0.025     0.0625   \n",
       "1       0.000000     0.0000      0.000     0.0000      0.000     0.2500   \n",
       "2       0.100000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "3       0.000000     0.2500      0.000     0.0000      0.500     0.0000   \n",
       "4       0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.272727     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "11832   0.000000     0.0000      0.100     0.0000      0.100     0.0000   \n",
       "11833   0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "11834   0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "11835   0.000000     0.0000      0.000     0.0000      0.000     0.0000   \n",
       "\n",
       "       hour_rt_9  hour_rt_10  hour_rt_11  hour_rt_12  hour_rt_13  hour_rt_14  \\\n",
       "0         0.0625        0.05    0.012500      0.0125    0.012500    0.050000   \n",
       "1         0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "2         0.0000        0.00    0.100000      0.0000    0.200000    0.200000   \n",
       "3         0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "4         0.0000        0.00    0.666667      0.0000    0.333333    0.000000   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "11831     0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "11832     0.0000        0.00    0.000000      0.1000    0.200000    0.000000   \n",
       "11833     0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "11834     0.0000        0.00    0.111111      0.0000    0.111111    0.111111   \n",
       "11835     0.0000        0.00    0.000000      0.0000    0.000000    0.000000   \n",
       "\n",
       "       hour_rt_15  hour_rt_16  hour_rt_17  hour_rt_18  hour_rt_19  hour_rt_20  \\\n",
       "0        0.087500    0.062500    0.112500    0.087500         0.2       0.075   \n",
       "1        0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "2        0.000000    0.000000    0.000000    0.000000         0.0       0.100   \n",
       "3        0.000000    0.250000    0.000000    0.000000         0.0       0.000   \n",
       "4        0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "11831    0.181818    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "11832    0.100000    0.000000    0.000000    0.000000         0.2       0.000   \n",
       "11833    0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "11834    0.000000    0.444444    0.111111    0.111111         0.0       0.000   \n",
       "11835    0.000000    0.000000    0.000000    0.000000         0.0       0.000   \n",
       "\n",
       "       hour_rt_21  hour_rt_22  hour_rt_23  mst_fr_ment_tw_1  mst_fr_ment_tw_2  \\\n",
       "0          0.0125    0.000000    0.000000          2.011174          5.980586   \n",
       "1          0.2500    0.000000    0.500000          5.951101          2.923159   \n",
       "2          0.0000    0.100000    0.100000         -0.100000         -0.100000   \n",
       "3          0.0000    0.000000    0.000000          6.896686          7.102824   \n",
       "4          0.0000    0.000000    0.000000          6.896686         10.566221   \n",
       "...           ...         ...         ...               ...               ...   \n",
       "11831      0.0000    0.181818    0.090909         -0.100000         -0.100000   \n",
       "11832      0.0000    0.000000    0.000000         -0.100000         -0.100000   \n",
       "11833      0.2500    0.250000    0.000000         -0.100000         -0.100000   \n",
       "11834      0.0000    0.000000    0.000000          4.803406          2.923159   \n",
       "11835      0.0000    0.000000    0.000000         -0.100000         -0.100000   \n",
       "\n",
       "       mst_fr_ment_tw_3  mst_fr_ment_rt_1  mst_fr_ment_rt_2  mst_fr_ment_rt_3  \\\n",
       "0              6.887675          2.011174          5.980586          9.177593   \n",
       "1              2.011174          2.011174          9.150005          9.552075   \n",
       "2             -0.100000         11.871754          6.725062          5.916363   \n",
       "3              2.011174          5.161735          5.840240          6.162355   \n",
       "4             10.750414          6.268109          5.161735         -0.100000   \n",
       "...                 ...               ...               ...               ...   \n",
       "11831         -0.100000          7.810123         11.525478          8.102567   \n",
       "11832         -0.100000          2.011174          4.176059          4.977433   \n",
       "11833         -0.100000          4.977433          5.120814         -0.100000   \n",
       "11834         -0.100000          2.923159          4.669607          8.837133   \n",
       "11835         -0.100000         -0.100000         -0.100000         -0.100000   \n",
       "\n",
       "       mst_fr_hs_tw_1  mst_fr_hs_tw_2  mst_fr_hs_tw_3  mst_fr_hs_rt_1  \\\n",
       "0            1.266133        6.667747        3.043134        1.266133   \n",
       "1            1.266133       -0.100000       -0.100000        1.266133   \n",
       "2           -0.100000       -0.100000       -0.100000        1.266133   \n",
       "3            1.266133        3.585302       12.348678        4.551958   \n",
       "4            1.266133       -0.100000       -0.100000        1.266133   \n",
       "...               ...             ...             ...             ...   \n",
       "11831       -0.100000       -0.100000       -0.100000        2.415885   \n",
       "11832       -0.100000       -0.100000       -0.100000        2.084814   \n",
       "11833       -0.100000       -0.100000       -0.100000        2.415885   \n",
       "11834        2.084814        7.183257        7.119354        2.084814   \n",
       "11835        2.084814       -0.100000       -0.100000       -0.100000   \n",
       "\n",
       "       mst_fr_hs_rt_2  mst_fr_hs_rt_3  tw_urls_avg  tw_urls_std  rt_urls_avg  \\\n",
       "0            4.803376        6.667747     0.169492     0.411693     0.187500   \n",
       "1            6.059891       -0.100000     0.000000     0.000000     0.000000   \n",
       "2            2.415885        2.488408     0.000000     0.000000     0.400000   \n",
       "3            8.989075        7.998400     0.700000     0.836660     0.750000   \n",
       "4            6.516144       -0.100000     0.285714     0.534522     0.666667   \n",
       "...               ...             ...          ...          ...          ...   \n",
       "11831        4.150451        3.265572     0.000000     0.000000     0.272727   \n",
       "11832        2.571623        2.415885     0.000000     0.000000     0.400000   \n",
       "11833        2.084814        5.213256     0.000000     0.000000     0.250000   \n",
       "11834        2.488408        3.265572     0.666667     0.816497     0.666667   \n",
       "11835       -0.100000       -0.100000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       rt_urls_std  tw_hash_avg  tw_hash_std  tw_ment_avg  tw_ment_std  \\\n",
       "0         0.433013     5.016949     5.633836     1.988701     2.665960   \n",
       "1         0.000000     1.000000     1.000000     3.000000     3.000000   \n",
       "2         0.632456     0.000000     0.000000     0.000000     0.000000   \n",
       "3         0.866025     0.500000     0.836660     1.750000     1.987461   \n",
       "4         0.816497     0.714286     0.845154     1.142857     1.195229   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     0.522233     0.000000     0.000000     0.000000     0.000000   \n",
       "11832     0.632456     0.000000     0.000000     0.000000     0.000000   \n",
       "11833     0.500000     0.000000     0.000000     0.000000     0.000000   \n",
       "11834     0.816497     2.500000     2.972092     0.333333     0.577350   \n",
       "11835     0.000000     1.000000     1.000000     0.000000     0.000000   \n",
       "\n",
       "       rt_hash_avg  rt_hash_std  rt_ment_avg  rt_ment_std  rt_time_avg  \\\n",
       "0         3.962500     4.602988     1.900000     2.915476   171.483958   \n",
       "1         1.250000     1.322876     1.250000     2.061553   169.037500   \n",
       "2         1.600000     2.049390     0.800000     1.264911   513.326667   \n",
       "3         2.250000     2.692582     0.750000     1.500000   362.737500   \n",
       "4         0.666667     1.154701     0.666667     1.154701   697.972222   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     2.636364     3.343923     0.454545     0.674200   211.589394   \n",
       "11832     2.200000     2.569047     0.800000     1.341641   337.120000   \n",
       "11833     1.750000     1.936492     0.500000     1.000000     6.033333   \n",
       "11834     3.222222     5.364492     1.000000     1.452966   317.183333   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       rt_time_min  rt_time_max  rt_time_std    rt_avg    tw_avg  \\\n",
       "0         0.166667  1292.233333   330.839721  2.758621  6.103448   \n",
       "1         7.183333   432.416667   241.493538  0.137931  0.034483   \n",
       "2         1.600000  1020.416667   635.460221  0.344828  0.000000   \n",
       "3        12.116667   733.633333   475.360941  0.137931  0.689655   \n",
       "4       521.350000   861.083333   711.683184  0.103448  0.241379   \n",
       "...            ...          ...          ...       ...       ...   \n",
       "11831     5.583333  1145.433333   378.904435  0.379310  0.000000   \n",
       "11832    50.016667   962.716667   444.474572  0.344828  0.000000   \n",
       "11833     0.316667    19.533333     9.885420  0.137931  0.000000   \n",
       "11834     0.566667  1312.000000   593.041953  0.310345  0.206897   \n",
       "11835     0.000000     0.000000     0.000000  0.000000  0.034483   \n",
       "\n",
       "       tw_rt_ration  verified  followers  favourites  listed  statuses  \\\n",
       "0          2.212500         0        343        7349       0      3502   \n",
       "1          0.250000         0        172        1463       0      1057   \n",
       "2          0.000000         0        281       36991       4     37368   \n",
       "3          5.000000         0        136        8697       0      4739   \n",
       "4          2.333333         0        241        5757       0      9201   \n",
       "...             ...       ...        ...         ...     ...       ...   \n",
       "11831      0.000000         0        341       88977       0     15392   \n",
       "11832      0.000000         0        266        4146       4      2577   \n",
       "11833      0.000000         0         96       22348       0     29246   \n",
       "11834      0.666667         0       2580        4078      97    122644   \n",
       "11835      0.000000         1    1117713        8621    1244     24671   \n",
       "\n",
       "       followers.1  friends_count  name_len  name_screen_sim  geo  protected  \\\n",
       "0              343            151        14         0.416667    0          0   \n",
       "1              172            403        21         0.461538    0          0   \n",
       "2              281            462        15         0.312500    0          0   \n",
       "3              136            163         4         0.800000    0          0   \n",
       "4              241            492         2         0.111111    0          0   \n",
       "...            ...            ...       ...              ...  ...        ...   \n",
       "11831          341            963         5         0.090909    0          0   \n",
       "11832          266            564         9         0.333333    0          0   \n",
       "11833           96            159         3         0.090909    0          0   \n",
       "11834         2580            961        11         0.150000    0          0   \n",
       "11835      1117713            597        14         0.666667    1          0   \n",
       "\n",
       "       location  description  description_len  bckg_img  default_prof  \\\n",
       "0             0            1               86         1             1   \n",
       "1             0            1               83         1             1   \n",
       "2             0            1               17         1             1   \n",
       "3             1            1               66         1             1   \n",
       "4             0            1               25         1             1   \n",
       "...         ...          ...              ...       ...           ...   \n",
       "11831         1            0                0         1             0   \n",
       "11832         0            1               46         1             1   \n",
       "11833         0            0                0         1             1   \n",
       "11834         1            1               61         1             0   \n",
       "11835         0            1               31         1             0   \n",
       "\n",
       "       entities  rt_self in_degree out_degree w_in_degree w_out_degree  \\\n",
       "0             0        1       115          0         576          166   \n",
       "1             1        0         0          0           0           11   \n",
       "2             0        0         0         34           0           47   \n",
       "3             0        0         6         12           8           13   \n",
       "4             0        0        10         15          19           15   \n",
       "...         ...      ...       ...        ...         ...          ...   \n",
       "11831         0        0         0         31           0           41   \n",
       "11832         0        0         0         25           0           28   \n",
       "11833         0        0         0         20           0           30   \n",
       "11834         0        0        53          0          89           97   \n",
       "11835         1        0       569          0         903            1   \n",
       "\n",
       "      w_degree  target              user_id  hs_tw_v_0  hs_tw_v_1  hs_tw_v_2  \\\n",
       "0          742       1  1258962548503740425   3.493951   7.603045  -1.862264   \n",
       "1           11       1  1277260249687261186   3.493951   7.603045  -1.862264   \n",
       "2           47       1   746102335915556868   0.000000   0.000000   0.000000   \n",
       "3           21       1  1231691345271586824   3.493951   7.603045  -1.862264   \n",
       "4           34       1           1518469190   3.493951   7.603045  -1.862264   \n",
       "...        ...     ...                  ...        ...        ...        ...   \n",
       "11831       41       0           1043726264   0.000000   0.000000   0.000000   \n",
       "11832       28       0           2542010298   0.000000   0.000000   0.000000   \n",
       "11833       30       0           2520645596   0.000000   0.000000   0.000000   \n",
       "11834      186       0             14811111  -6.481514  -0.044037   7.989522   \n",
       "11835      904       0           1064042478  -6.481514  -0.044037   7.989522   \n",
       "\n",
       "       hs_tw_v_3  hs_tw_v_4  hs_tw_v_5  hs_tw_v_6  hs_tw_v_7  hs_tw_v_8  \\\n",
       "0      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "1      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "2       0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "3      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "4      10.015945  -0.595800   4.668619   3.420092  -1.148644  -2.369110   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11832   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11833   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "11834  11.762167   6.681043  -3.275398   4.377571   5.375144  -5.796048   \n",
       "11835  11.762167   6.681043  -3.275398   4.377571   5.375144  -5.796048   \n",
       "\n",
       "       hs_tw_v_9  hs_rt_v_0  hs_rt_v_1  hs_rt_v_2  hs_rt_v_3  hs_rt_v_4  \\\n",
       "0      -4.642322   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "1      -4.642322   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "2       0.000000   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "3      -4.642322  -4.284786   7.481483   6.267010   4.508372  11.494592   \n",
       "4      -4.642322   3.493951   7.603045  -1.862264  10.015945  -0.595800   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "11831   0.000000  -5.481577  -4.951790  11.181436   8.863973   3.241972   \n",
       "11832   0.000000  -6.481514  -0.044037   7.989522  11.762167   6.681043   \n",
       "11833   0.000000  -5.481577  -4.951790  11.181436   8.863973   3.241972   \n",
       "11834  -2.582479  -6.481514  -0.044037   7.989522  11.762167   6.681043   \n",
       "11835  -2.582479   0.000000   0.000000   0.000000   0.000000   0.000000   \n",
       "\n",
       "       hs_rt_v_5  hs_rt_v_6  hs_rt_v_7  hs_rt_v_8  hs_rt_v_9  ment_tw_v_0  \\\n",
       "0       4.668619   3.420092  -1.148644  -2.369110  -4.642322     0.282885   \n",
       "1       4.668619   3.420092  -1.148644  -2.369110  -4.642322    -2.666591   \n",
       "2       4.668619   3.420092  -1.148644  -2.369110  -4.642322     0.000000   \n",
       "3      -1.333691  -2.366664   0.944676  -3.689538  -7.882071     1.424672   \n",
       "4       4.668619   3.420092  -1.148644  -2.369110  -4.642322     1.424672   \n",
       "...          ...        ...        ...        ...        ...          ...   \n",
       "11831  -4.242408  -0.542843   3.072947  -7.411512  -3.821877     0.000000   \n",
       "11832  -3.275398   4.377571   5.375144  -5.796048  -2.582479     0.000000   \n",
       "11833  -4.242408  -0.542843   3.072947  -7.411512  -3.821877     0.000000   \n",
       "11834  -3.275398   4.377571   5.375144  -5.796048  -2.582479    -1.483979   \n",
       "11835   0.000000   0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "\n",
       "       ment_tw_v_1  ment_tw_v_2  ment_tw_v_3  ment_tw_v_4  ment_tw_v_5  \\\n",
       "0         4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "1         3.297360     0.108515     1.148872    -4.909317     4.969856   \n",
       "2         0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "3         0.269191     2.436769     3.364856    -2.865908     5.763546   \n",
       "4         0.269191     2.436769     3.364856    -2.865908     5.763546   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "11832     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "11833     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "11834     1.254190     9.255416     5.533646     1.226344     4.875967   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       ment_tw_v_6  ment_tw_v_7  ment_tw_v_8  ment_tw_v_9  ment_rt_v_0  \\\n",
       "0         0.595667    -2.255075    -0.706327    -2.917101     0.282885   \n",
       "1         2.520902     0.120532    -3.473754    -1.942731     0.282885   \n",
       "2         0.000000     0.000000     0.000000     0.000000     0.736998   \n",
       "3         3.289723    -7.134625    -1.938984     3.373724    -0.913174   \n",
       "4         3.289723    -7.134625    -1.938984     3.373724     2.414909   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831     0.000000     0.000000     0.000000     0.000000    -4.205255   \n",
       "11832     0.000000     0.000000     0.000000     0.000000     0.282885   \n",
       "11833     0.000000     0.000000     0.000000     0.000000    -6.206959   \n",
       "11834     0.917352    -4.030603    -2.897294    -0.845497    -5.901112   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       ment_rt_v_1  ment_rt_v_2  ment_rt_v_3  ment_rt_v_4  ment_rt_v_5  \\\n",
       "0         4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "1         4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "2        -0.069259    -0.292788    -0.561506    -0.082709     0.042723   \n",
       "3         4.238219    -0.241484     4.171506    -2.921207     6.198004   \n",
       "4        -3.960131    -0.615269     2.886237    -8.917027     2.754604   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11831   -13.955618     1.322916     3.720930    -6.821259   -13.958460   \n",
       "11832     4.259559     4.080622     8.733304    -4.353861     4.428511   \n",
       "11833    -2.618856     1.212161    -3.037526    -1.402811     1.444823   \n",
       "11834     3.408430     5.464797     5.483321    -3.814475     3.054348   \n",
       "11835     0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "\n",
       "       ment_rt_v_6  ment_rt_v_7  ment_rt_v_8  ment_rt_v_9  \n",
       "0         0.595667    -2.255075    -0.706327    -2.917101  \n",
       "1         0.595667    -2.255075    -0.706327    -2.917101  \n",
       "2         0.180475    -0.226081     0.147200     0.524840  \n",
       "3         3.403417    -6.699732    -2.091649    -0.219933  \n",
       "4        -1.793620    -7.303990    -5.697265     4.512267  \n",
       "...            ...          ...          ...          ...  \n",
       "11831    -3.371006    -5.193164     2.854197     0.453024  \n",
       "11832     0.595667    -2.255075    -0.706327    -2.917101  \n",
       "11833    12.596374     0.780529     6.377641    -2.976061  \n",
       "11834     1.276991     0.458579    -1.790708    -1.426023  \n",
       "11835     0.000000     0.000000     0.000000     0.000000  \n",
       "\n",
       "[11836 rows x 189 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#---------------------!!!-------------------\n",
    "#-----------------IMPORTANT-----------------\n",
    "#Execute ONLY of case of data/labels changes\n",
    "#-----------------IMPORTANT-----------------\n",
    "#-------------------------------------------\n",
    "\n",
    "combine_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution main frame\n",
    "### 1) Read CSV file into dataframe \n",
    "### 2) Balance data by target (50/50)\n",
    "### 3) Perform One-Hot of text features\n",
    "### As result, return dataframe, target of the dataframe and user_ids vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500, 187)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_rt_tw_0</th>\n",
       "      <th>daily_rt_tw_1</th>\n",
       "      <th>daily_rt_tw_2</th>\n",
       "      <th>daily_rt_tw_3</th>\n",
       "      <th>daily_rt_tw_4</th>\n",
       "      <th>daily_rt_tw_5</th>\n",
       "      <th>daily_rt_tw_6</th>\n",
       "      <th>daily_rt_0</th>\n",
       "      <th>daily_rt_1</th>\n",
       "      <th>daily_rt_2</th>\n",
       "      <th>daily_rt_3</th>\n",
       "      <th>daily_rt_4</th>\n",
       "      <th>daily_rt_5</th>\n",
       "      <th>daily_rt_6</th>\n",
       "      <th>daily_tw_0</th>\n",
       "      <th>daily_tw_1</th>\n",
       "      <th>daily_tw_2</th>\n",
       "      <th>daily_tw_3</th>\n",
       "      <th>daily_tw_4</th>\n",
       "      <th>daily_tw_5</th>\n",
       "      <th>daily_tw_6</th>\n",
       "      <th>hour_rt_tw_0</th>\n",
       "      <th>hour_rt_tw_1</th>\n",
       "      <th>hour_rt_tw_2</th>\n",
       "      <th>hour_rt_tw_3</th>\n",
       "      <th>hour_rt_tw_4</th>\n",
       "      <th>hour_rt_tw_5</th>\n",
       "      <th>hour_rt_tw_6</th>\n",
       "      <th>hour_rt_tw_7</th>\n",
       "      <th>hour_rt_tw_8</th>\n",
       "      <th>hour_rt_tw_9</th>\n",
       "      <th>hour_rt_tw_10</th>\n",
       "      <th>hour_rt_tw_11</th>\n",
       "      <th>hour_rt_tw_12</th>\n",
       "      <th>hour_rt_tw_13</th>\n",
       "      <th>hour_rt_tw_14</th>\n",
       "      <th>hour_rt_tw_15</th>\n",
       "      <th>hour_rt_tw_16</th>\n",
       "      <th>hour_rt_tw_17</th>\n",
       "      <th>hour_rt_tw_18</th>\n",
       "      <th>hour_rt_tw_19</th>\n",
       "      <th>hour_rt_tw_20</th>\n",
       "      <th>hour_rt_tw_21</th>\n",
       "      <th>hour_rt_tw_22</th>\n",
       "      <th>hour_rt_tw_23</th>\n",
       "      <th>hour_tw_0</th>\n",
       "      <th>hour_tw_1</th>\n",
       "      <th>hour_tw_2</th>\n",
       "      <th>hour_tw_3</th>\n",
       "      <th>hour_tw_4</th>\n",
       "      <th>hour_tw_5</th>\n",
       "      <th>hour_tw_6</th>\n",
       "      <th>hour_tw_7</th>\n",
       "      <th>hour_tw_8</th>\n",
       "      <th>hour_tw_9</th>\n",
       "      <th>hour_tw_10</th>\n",
       "      <th>hour_tw_11</th>\n",
       "      <th>hour_tw_12</th>\n",
       "      <th>hour_tw_13</th>\n",
       "      <th>hour_tw_14</th>\n",
       "      <th>hour_tw_15</th>\n",
       "      <th>hour_tw_16</th>\n",
       "      <th>hour_tw_17</th>\n",
       "      <th>hour_tw_18</th>\n",
       "      <th>hour_tw_19</th>\n",
       "      <th>hour_tw_20</th>\n",
       "      <th>hour_tw_21</th>\n",
       "      <th>hour_tw_22</th>\n",
       "      <th>hour_tw_23</th>\n",
       "      <th>hour_rt_0</th>\n",
       "      <th>hour_rt_1</th>\n",
       "      <th>hour_rt_2</th>\n",
       "      <th>hour_rt_3</th>\n",
       "      <th>hour_rt_4</th>\n",
       "      <th>hour_rt_5</th>\n",
       "      <th>hour_rt_6</th>\n",
       "      <th>hour_rt_7</th>\n",
       "      <th>hour_rt_8</th>\n",
       "      <th>hour_rt_9</th>\n",
       "      <th>hour_rt_10</th>\n",
       "      <th>hour_rt_11</th>\n",
       "      <th>hour_rt_12</th>\n",
       "      <th>hour_rt_13</th>\n",
       "      <th>hour_rt_14</th>\n",
       "      <th>hour_rt_15</th>\n",
       "      <th>hour_rt_16</th>\n",
       "      <th>hour_rt_17</th>\n",
       "      <th>hour_rt_18</th>\n",
       "      <th>hour_rt_19</th>\n",
       "      <th>hour_rt_20</th>\n",
       "      <th>hour_rt_21</th>\n",
       "      <th>hour_rt_22</th>\n",
       "      <th>hour_rt_23</th>\n",
       "      <th>mst_fr_ment_tw_1</th>\n",
       "      <th>mst_fr_ment_tw_2</th>\n",
       "      <th>mst_fr_ment_tw_3</th>\n",
       "      <th>mst_fr_ment_rt_1</th>\n",
       "      <th>mst_fr_ment_rt_2</th>\n",
       "      <th>mst_fr_ment_rt_3</th>\n",
       "      <th>mst_fr_hs_tw_1</th>\n",
       "      <th>mst_fr_hs_tw_2</th>\n",
       "      <th>mst_fr_hs_tw_3</th>\n",
       "      <th>mst_fr_hs_rt_1</th>\n",
       "      <th>mst_fr_hs_rt_2</th>\n",
       "      <th>mst_fr_hs_rt_3</th>\n",
       "      <th>tw_urls_avg</th>\n",
       "      <th>tw_urls_std</th>\n",
       "      <th>rt_urls_avg</th>\n",
       "      <th>rt_urls_std</th>\n",
       "      <th>tw_hash_avg</th>\n",
       "      <th>tw_hash_std</th>\n",
       "      <th>tw_ment_avg</th>\n",
       "      <th>tw_ment_std</th>\n",
       "      <th>rt_hash_avg</th>\n",
       "      <th>rt_hash_std</th>\n",
       "      <th>rt_ment_avg</th>\n",
       "      <th>rt_ment_std</th>\n",
       "      <th>rt_time_avg</th>\n",
       "      <th>rt_time_min</th>\n",
       "      <th>rt_time_max</th>\n",
       "      <th>rt_time_std</th>\n",
       "      <th>rt_avg</th>\n",
       "      <th>tw_avg</th>\n",
       "      <th>tw_rt_ration</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>favourites</th>\n",
       "      <th>listed</th>\n",
       "      <th>statuses</th>\n",
       "      <th>followers.1</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>name_len</th>\n",
       "      <th>name_screen_sim</th>\n",
       "      <th>geo</th>\n",
       "      <th>protected</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>description_len</th>\n",
       "      <th>bckg_img</th>\n",
       "      <th>default_prof</th>\n",
       "      <th>entities</th>\n",
       "      <th>rt_self</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>out_degree</th>\n",
       "      <th>w_in_degree</th>\n",
       "      <th>w_out_degree</th>\n",
       "      <th>w_degree</th>\n",
       "      <th>hs_tw_v_0</th>\n",
       "      <th>hs_tw_v_1</th>\n",
       "      <th>hs_tw_v_2</th>\n",
       "      <th>hs_tw_v_3</th>\n",
       "      <th>hs_tw_v_4</th>\n",
       "      <th>hs_tw_v_5</th>\n",
       "      <th>hs_tw_v_6</th>\n",
       "      <th>hs_tw_v_7</th>\n",
       "      <th>hs_tw_v_8</th>\n",
       "      <th>hs_tw_v_9</th>\n",
       "      <th>hs_rt_v_0</th>\n",
       "      <th>hs_rt_v_1</th>\n",
       "      <th>hs_rt_v_2</th>\n",
       "      <th>hs_rt_v_3</th>\n",
       "      <th>hs_rt_v_4</th>\n",
       "      <th>hs_rt_v_5</th>\n",
       "      <th>hs_rt_v_6</th>\n",
       "      <th>hs_rt_v_7</th>\n",
       "      <th>hs_rt_v_8</th>\n",
       "      <th>hs_rt_v_9</th>\n",
       "      <th>ment_tw_v_0</th>\n",
       "      <th>ment_tw_v_1</th>\n",
       "      <th>ment_tw_v_2</th>\n",
       "      <th>ment_tw_v_3</th>\n",
       "      <th>ment_tw_v_4</th>\n",
       "      <th>ment_tw_v_5</th>\n",
       "      <th>ment_tw_v_6</th>\n",
       "      <th>ment_tw_v_7</th>\n",
       "      <th>ment_tw_v_8</th>\n",
       "      <th>ment_tw_v_9</th>\n",
       "      <th>ment_rt_v_0</th>\n",
       "      <th>ment_rt_v_1</th>\n",
       "      <th>ment_rt_v_2</th>\n",
       "      <th>ment_rt_v_3</th>\n",
       "      <th>ment_rt_v_4</th>\n",
       "      <th>ment_rt_v_5</th>\n",
       "      <th>ment_rt_v_6</th>\n",
       "      <th>ment_rt_v_7</th>\n",
       "      <th>ment_rt_v_8</th>\n",
       "      <th>ment_rt_v_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.033771</td>\n",
       "      <td>-0.288698</td>\n",
       "      <td>0.707072</td>\n",
       "      <td>0.444093</td>\n",
       "      <td>0.254897</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>-0.292344</td>\n",
       "      <td>0.041097</td>\n",
       "      <td>-0.181523</td>\n",
       "      <td>0.683446</td>\n",
       "      <td>0.520329</td>\n",
       "      <td>0.367691</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.189205</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>0.185990</td>\n",
       "      <td>0.823534</td>\n",
       "      <td>0.325808</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>0.933970</td>\n",
       "      <td>0.668776</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>0.347962</td>\n",
       "      <td>0.343309</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>0.296254</td>\n",
       "      <td>0.318855</td>\n",
       "      <td>0.316542</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>0.228385</td>\n",
       "      <td>0.811238</td>\n",
       "      <td>0.359363</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>0.926646</td>\n",
       "      <td>0.682208</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>0.385695</td>\n",
       "      <td>0.394744</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>0.370076</td>\n",
       "      <td>0.399188</td>\n",
       "      <td>0.360211</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>-0.595482</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.378457</td>\n",
       "      <td>0.946685</td>\n",
       "      <td>1.737489</td>\n",
       "      <td>-0.595259</td>\n",
       "      <td>-0.590192</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.501843</td>\n",
       "      <td>-0.133677</td>\n",
       "      <td>-0.043415</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>0.788158</td>\n",
       "      <td>0.839594</td>\n",
       "      <td>-0.642387</td>\n",
       "      <td>-0.639803</td>\n",
       "      <td>-0.357959</td>\n",
       "      <td>-0.372066</td>\n",
       "      <td>-0.245943</td>\n",
       "      <td>-0.038697</td>\n",
       "      <td>-0.282074</td>\n",
       "      <td>-0.168967</td>\n",
       "      <td>0.515613</td>\n",
       "      <td>-0.306350</td>\n",
       "      <td>1.209928</td>\n",
       "      <td>0.682940</td>\n",
       "      <td>-0.085005</td>\n",
       "      <td>-0.177393</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.059734</td>\n",
       "      <td>0.362989</td>\n",
       "      <td>-0.102501</td>\n",
       "      <td>-0.255750</td>\n",
       "      <td>-0.059734</td>\n",
       "      <td>-0.376505</td>\n",
       "      <td>-1.087539</td>\n",
       "      <td>0.919057</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.255410</td>\n",
       "      <td>-2.249190</td>\n",
       "      <td>-1.684902</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>-0.028163</td>\n",
       "      <td>-0.097133</td>\n",
       "      <td>-0.029882</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>-0.478683</td>\n",
       "      <td>-0.359195</td>\n",
       "      <td>-0.720306</td>\n",
       "      <td>-0.461264</td>\n",
       "      <td>-0.287817</td>\n",
       "      <td>-0.588169</td>\n",
       "      <td>-0.164521</td>\n",
       "      <td>0.481054</td>\n",
       "      <td>0.595168</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.816602</td>\n",
       "      <td>-0.837654</td>\n",
       "      <td>0.692779</td>\n",
       "      <td>-0.773825</td>\n",
       "      <td>0.927183</td>\n",
       "      <td>0.546767</td>\n",
       "      <td>-0.530920</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>0.143023</td>\n",
       "      <td>-0.209374</td>\n",
       "      <td>-0.259983</td>\n",
       "      <td>-0.310613</td>\n",
       "      <td>0.365894</td>\n",
       "      <td>-0.321383</td>\n",
       "      <td>-0.207553</td>\n",
       "      <td>0.279414</td>\n",
       "      <td>0.287076</td>\n",
       "      <td>0.199721</td>\n",
       "      <td>0.340404</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>1.221135</td>\n",
       "      <td>-0.858276</td>\n",
       "      <td>0.825696</td>\n",
       "      <td>-0.069761</td>\n",
       "      <td>-0.769992</td>\n",
       "      <td>-0.277606</td>\n",
       "      <td>-0.484911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.244816</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>-0.748249</td>\n",
       "      <td>0.611602</td>\n",
       "      <td>0.650969</td>\n",
       "      <td>0.500071</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>1.006502</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>1.114170</td>\n",
       "      <td>1.102461</td>\n",
       "      <td>1.080489</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>1.592639</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>3.539206</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>1.624238</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>2.596364</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>4.338693</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>2.343080</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>1.173888</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>0.536020</td>\n",
       "      <td>2.557913</td>\n",
       "      <td>2.838443</td>\n",
       "      <td>-1.098276</td>\n",
       "      <td>-1.276505</td>\n",
       "      <td>-1.128656</td>\n",
       "      <td>0.218215</td>\n",
       "      <td>0.759387</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>2.660205</td>\n",
       "      <td>2.706995</td>\n",
       "      <td>-0.208041</td>\n",
       "      <td>-0.119966</td>\n",
       "      <td>-1.406242</td>\n",
       "      <td>-1.427448</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-1.177326</td>\n",
       "      <td>-0.427663</td>\n",
       "      <td>-1.362771</td>\n",
       "      <td>-1.326675</td>\n",
       "      <td>-0.395687</td>\n",
       "      <td>-0.055366</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>5.441713</td>\n",
       "      <td>-0.008488</td>\n",
       "      <td>-0.596905</td>\n",
       "      <td>-0.019592</td>\n",
       "      <td>-0.534887</td>\n",
       "      <td>-0.008488</td>\n",
       "      <td>0.515575</td>\n",
       "      <td>-0.367904</td>\n",
       "      <td>1.572604</td>\n",
       "      <td>1.602810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>0.416436</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>2.243765</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.035506</td>\n",
       "      <td>-0.531851</td>\n",
       "      <td>-0.027948</td>\n",
       "      <td>-0.467364</td>\n",
       "      <td>-0.036345</td>\n",
       "      <td>-1.372834</td>\n",
       "      <td>-1.005197</td>\n",
       "      <td>1.745834</td>\n",
       "      <td>0.793267</td>\n",
       "      <td>2.129627</td>\n",
       "      <td>-2.505875</td>\n",
       "      <td>-0.500508</td>\n",
       "      <td>3.846728</td>\n",
       "      <td>-2.324506</td>\n",
       "      <td>-1.664559</td>\n",
       "      <td>0.082980</td>\n",
       "      <td>-0.721331</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>-1.145801</td>\n",
       "      <td>-0.632472</td>\n",
       "      <td>-0.310951</td>\n",
       "      <td>-0.749462</td>\n",
       "      <td>-0.168996</td>\n",
       "      <td>0.849216</td>\n",
       "      <td>1.269346</td>\n",
       "      <td>-2.242878</td>\n",
       "      <td>-1.783231</td>\n",
       "      <td>0.264788</td>\n",
       "      <td>-0.371487</td>\n",
       "      <td>-0.148661</td>\n",
       "      <td>-0.183347</td>\n",
       "      <td>2.056465</td>\n",
       "      <td>-2.754610</td>\n",
       "      <td>-0.957024</td>\n",
       "      <td>-0.073996</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.776668</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>-0.748249</td>\n",
       "      <td>1.919519</td>\n",
       "      <td>1.979899</td>\n",
       "      <td>-0.745153</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>2.585044</td>\n",
       "      <td>2.561717</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>3.377219</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>3.705574</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>4.487376</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>4.754944</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>2.166983</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>0.329884</td>\n",
       "      <td>-0.590192</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-1.098276</td>\n",
       "      <td>-1.276505</td>\n",
       "      <td>-1.128656</td>\n",
       "      <td>0.952564</td>\n",
       "      <td>1.311059</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>0.018132</td>\n",
       "      <td>-0.038700</td>\n",
       "      <td>-0.058123</td>\n",
       "      <td>-0.015543</td>\n",
       "      <td>-1.406242</td>\n",
       "      <td>-1.427448</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-1.177326</td>\n",
       "      <td>-0.427663</td>\n",
       "      <td>-1.362771</td>\n",
       "      <td>-1.326675</td>\n",
       "      <td>-0.395687</td>\n",
       "      <td>-0.116379</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>5.441713</td>\n",
       "      <td>0.034919</td>\n",
       "      <td>0.130780</td>\n",
       "      <td>0.317829</td>\n",
       "      <td>-0.110854</td>\n",
       "      <td>0.034919</td>\n",
       "      <td>0.294298</td>\n",
       "      <td>-0.128026</td>\n",
       "      <td>1.648222</td>\n",
       "      <td>1.602810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>1.188723</td>\n",
       "      <td>-2.317619</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>2.243765</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.015286</td>\n",
       "      <td>-0.531851</td>\n",
       "      <td>-0.013275</td>\n",
       "      <td>-0.467364</td>\n",
       "      <td>-0.021695</td>\n",
       "      <td>-2.212239</td>\n",
       "      <td>-0.490848</td>\n",
       "      <td>1.564761</td>\n",
       "      <td>1.829846</td>\n",
       "      <td>1.769630</td>\n",
       "      <td>-1.612474</td>\n",
       "      <td>1.527780</td>\n",
       "      <td>2.363210</td>\n",
       "      <td>-1.708498</td>\n",
       "      <td>-0.445103</td>\n",
       "      <td>0.082980</td>\n",
       "      <td>-0.721331</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>-1.145801</td>\n",
       "      <td>-0.632472</td>\n",
       "      <td>-0.310951</td>\n",
       "      <td>-0.749462</td>\n",
       "      <td>-0.168996</td>\n",
       "      <td>0.849216</td>\n",
       "      <td>1.269346</td>\n",
       "      <td>0.262169</td>\n",
       "      <td>-0.381322</td>\n",
       "      <td>-0.364314</td>\n",
       "      <td>-0.634174</td>\n",
       "      <td>0.386674</td>\n",
       "      <td>-0.453555</td>\n",
       "      <td>-0.110713</td>\n",
       "      <td>0.117676</td>\n",
       "      <td>0.128495</td>\n",
       "      <td>0.334223</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.776668</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>0.709058</td>\n",
       "      <td>1.047574</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>0.915145</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>0.423885</td>\n",
       "      <td>2.051674</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>0.594983</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>2.436123</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>2.531306</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>1.899479</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>0.927730</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>1.057321</td>\n",
       "      <td>1.058441</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>0.933425</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>4.512808</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>4.857426</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>1.329247</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>1.614488</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>1.763145</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>1.602849</td>\n",
       "      <td>2.069672</td>\n",
       "      <td>2.016624</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.112664</td>\n",
       "      <td>-0.409112</td>\n",
       "      <td>2.381850</td>\n",
       "      <td>0.500783</td>\n",
       "      <td>0.212400</td>\n",
       "      <td>0.775386</td>\n",
       "      <td>0.162182</td>\n",
       "      <td>2.066703</td>\n",
       "      <td>-0.287796</td>\n",
       "      <td>0.952564</td>\n",
       "      <td>1.311059</td>\n",
       "      <td>1.448955</td>\n",
       "      <td>1.868033</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.562402</td>\n",
       "      <td>0.241712</td>\n",
       "      <td>0.132134</td>\n",
       "      <td>0.441076</td>\n",
       "      <td>0.239328</td>\n",
       "      <td>0.443594</td>\n",
       "      <td>0.307552</td>\n",
       "      <td>2.053338</td>\n",
       "      <td>-0.426781</td>\n",
       "      <td>1.453246</td>\n",
       "      <td>2.138906</td>\n",
       "      <td>-0.282712</td>\n",
       "      <td>-0.116379</td>\n",
       "      <td>-0.082397</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.048424</td>\n",
       "      <td>-0.184284</td>\n",
       "      <td>-0.038873</td>\n",
       "      <td>0.284990</td>\n",
       "      <td>-0.048424</td>\n",
       "      <td>-0.187630</td>\n",
       "      <td>0.351730</td>\n",
       "      <td>-0.506866</td>\n",
       "      <td>1.602810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>1.152803</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.034697</td>\n",
       "      <td>-0.531851</td>\n",
       "      <td>-0.027300</td>\n",
       "      <td>-0.156847</td>\n",
       "      <td>-0.030097</td>\n",
       "      <td>-1.934675</td>\n",
       "      <td>1.818096</td>\n",
       "      <td>1.256643</td>\n",
       "      <td>0.534822</td>\n",
       "      <td>2.993217</td>\n",
       "      <td>-0.618761</td>\n",
       "      <td>-1.192867</td>\n",
       "      <td>2.241272</td>\n",
       "      <td>-1.691256</td>\n",
       "      <td>-2.296444</td>\n",
       "      <td>-1.005272</td>\n",
       "      <td>-1.395498</td>\n",
       "      <td>2.132432</td>\n",
       "      <td>1.108192</td>\n",
       "      <td>0.796068</td>\n",
       "      <td>-0.421541</td>\n",
       "      <td>1.363867</td>\n",
       "      <td>-0.084468</td>\n",
       "      <td>-0.369064</td>\n",
       "      <td>1.575669</td>\n",
       "      <td>0.014068</td>\n",
       "      <td>0.085375</td>\n",
       "      <td>-0.520774</td>\n",
       "      <td>-0.660333</td>\n",
       "      <td>0.812923</td>\n",
       "      <td>-0.677726</td>\n",
       "      <td>-0.565583</td>\n",
       "      <td>0.141846</td>\n",
       "      <td>-0.286970</td>\n",
       "      <td>0.509814</td>\n",
       "      <td>-1.273979</td>\n",
       "      <td>0.663287</td>\n",
       "      <td>0.906776</td>\n",
       "      <td>0.546649</td>\n",
       "      <td>-0.660916</td>\n",
       "      <td>0.506244</td>\n",
       "      <td>0.113732</td>\n",
       "      <td>0.506675</td>\n",
       "      <td>-0.582343</td>\n",
       "      <td>0.034164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.033771</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>0.707072</td>\n",
       "      <td>0.841541</td>\n",
       "      <td>-0.220710</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>0.160464</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>4.697934</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>0.730702</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>1.229269</td>\n",
       "      <td>1.873048</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>0.790326</td>\n",
       "      <td>0.256806</td>\n",
       "      <td>0.828549</td>\n",
       "      <td>0.823534</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>0.343309</td>\n",
       "      <td>1.025532</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>0.318855</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>0.381794</td>\n",
       "      <td>0.380923</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>0.707220</td>\n",
       "      <td>0.710511</td>\n",
       "      <td>1.784980</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>0.744188</td>\n",
       "      <td>1.517994</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>0.707440</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>0.789501</td>\n",
       "      <td>0.789379</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>6.519026</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>3.085846</td>\n",
       "      <td>1.833372</td>\n",
       "      <td>3.373289</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>0.513363</td>\n",
       "      <td>0.790230</td>\n",
       "      <td>0.445463</td>\n",
       "      <td>4.446349</td>\n",
       "      <td>-0.335372</td>\n",
       "      <td>0.477376</td>\n",
       "      <td>1.540043</td>\n",
       "      <td>1.656151</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>1.405220</td>\n",
       "      <td>1.617335</td>\n",
       "      <td>-0.178057</td>\n",
       "      <td>-0.015543</td>\n",
       "      <td>0.609014</td>\n",
       "      <td>0.313443</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-0.780280</td>\n",
       "      <td>0.016122</td>\n",
       "      <td>-1.165264</td>\n",
       "      <td>-0.973500</td>\n",
       "      <td>-0.367444</td>\n",
       "      <td>0.127675</td>\n",
       "      <td>1.450448</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.057275</td>\n",
       "      <td>-0.494245</td>\n",
       "      <td>-0.100573</td>\n",
       "      <td>-0.472242</td>\n",
       "      <td>-0.057275</td>\n",
       "      <td>-0.191321</td>\n",
       "      <td>0.231791</td>\n",
       "      <td>-0.417746</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>1.170763</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.024183</td>\n",
       "      <td>-0.348673</td>\n",
       "      <td>-0.022122</td>\n",
       "      <td>-0.407649</td>\n",
       "      <td>-0.029451</td>\n",
       "      <td>-1.078564</td>\n",
       "      <td>1.886311</td>\n",
       "      <td>1.606272</td>\n",
       "      <td>-0.098455</td>\n",
       "      <td>0.241667</td>\n",
       "      <td>1.466633</td>\n",
       "      <td>1.015162</td>\n",
       "      <td>1.470123</td>\n",
       "      <td>2.465563</td>\n",
       "      <td>-2.000625</td>\n",
       "      <td>0.187270</td>\n",
       "      <td>-0.761071</td>\n",
       "      <td>-0.468909</td>\n",
       "      <td>-1.226953</td>\n",
       "      <td>-0.536228</td>\n",
       "      <td>-0.383315</td>\n",
       "      <td>-0.711690</td>\n",
       "      <td>-0.181658</td>\n",
       "      <td>0.818366</td>\n",
       "      <td>1.377542</td>\n",
       "      <td>0.143183</td>\n",
       "      <td>-0.246317</td>\n",
       "      <td>-0.283340</td>\n",
       "      <td>-0.314630</td>\n",
       "      <td>0.351092</td>\n",
       "      <td>-0.330400</td>\n",
       "      <td>-0.184736</td>\n",
       "      <td>0.248969</td>\n",
       "      <td>0.271609</td>\n",
       "      <td>0.181822</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10495</th>\n",
       "      <td>0.173765</td>\n",
       "      <td>0.129584</td>\n",
       "      <td>1.125997</td>\n",
       "      <td>-0.748249</td>\n",
       "      <td>-0.696316</td>\n",
       "      <td>0.558533</td>\n",
       "      <td>0.007379</td>\n",
       "      <td>0.243446</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>1.047400</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>0.697046</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>0.504089</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>0.506451</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>1.187559</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>0.804974</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>1.312833</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>1.457153</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>1.645343</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>1.486305</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>2.185909</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>1.208920</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>1.570405</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>1.686359</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>0.205753</td>\n",
       "      <td>0.533642</td>\n",
       "      <td>-0.009644</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>-0.420462</td>\n",
       "      <td>-0.042913</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.682066</td>\n",
       "      <td>-1.276505</td>\n",
       "      <td>-1.128656</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>-0.222502</td>\n",
       "      <td>-0.070062</td>\n",
       "      <td>-0.343006</td>\n",
       "      <td>-0.352615</td>\n",
       "      <td>-0.086157</td>\n",
       "      <td>-0.131175</td>\n",
       "      <td>-1.093728</td>\n",
       "      <td>-1.096808</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-0.994096</td>\n",
       "      <td>-0.277745</td>\n",
       "      <td>-1.255131</td>\n",
       "      <td>-1.160672</td>\n",
       "      <td>-0.336559</td>\n",
       "      <td>-0.158957</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.061537</td>\n",
       "      <td>-0.436599</td>\n",
       "      <td>-0.104429</td>\n",
       "      <td>-0.564141</td>\n",
       "      <td>-0.061537</td>\n",
       "      <td>-0.343897</td>\n",
       "      <td>-1.087539</td>\n",
       "      <td>-0.768308</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.255410</td>\n",
       "      <td>-2.249190</td>\n",
       "      <td>-1.002416</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>-0.312037</td>\n",
       "      <td>-0.028163</td>\n",
       "      <td>-0.395706</td>\n",
       "      <td>-0.035268</td>\n",
       "      <td>0.452254</td>\n",
       "      <td>0.155950</td>\n",
       "      <td>-0.494702</td>\n",
       "      <td>-0.064133</td>\n",
       "      <td>-0.521379</td>\n",
       "      <td>0.282709</td>\n",
       "      <td>-0.088644</td>\n",
       "      <td>-0.327741</td>\n",
       "      <td>0.210624</td>\n",
       "      <td>0.030111</td>\n",
       "      <td>0.648356</td>\n",
       "      <td>0.351889</td>\n",
       "      <td>-0.731034</td>\n",
       "      <td>0.137220</td>\n",
       "      <td>-0.731113</td>\n",
       "      <td>0.553059</td>\n",
       "      <td>0.155089</td>\n",
       "      <td>-0.421558</td>\n",
       "      <td>0.333415</td>\n",
       "      <td>0.161684</td>\n",
       "      <td>0.173824</td>\n",
       "      <td>-0.285664</td>\n",
       "      <td>-0.322497</td>\n",
       "      <td>-0.432703</td>\n",
       "      <td>0.367820</td>\n",
       "      <td>-0.410416</td>\n",
       "      <td>-0.186829</td>\n",
       "      <td>0.228706</td>\n",
       "      <td>0.217499</td>\n",
       "      <td>0.407109</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10496</th>\n",
       "      <td>-0.572911</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>1.219693</td>\n",
       "      <td>1.919519</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>-0.745153</td>\n",
       "      <td>-0.484571</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>1.273151</td>\n",
       "      <td>2.051674</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>0.085573</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>3.250440</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>3.705574</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>0.130563</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>3.502816</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>3.823981</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>-0.595482</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.856891</td>\n",
       "      <td>-0.902396</td>\n",
       "      <td>-0.788854</td>\n",
       "      <td>-0.595259</td>\n",
       "      <td>-0.590192</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.356939</td>\n",
       "      <td>1.382975</td>\n",
       "      <td>1.148335</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>-0.812864</td>\n",
       "      <td>-0.970815</td>\n",
       "      <td>-0.642387</td>\n",
       "      <td>-0.639803</td>\n",
       "      <td>-0.357959</td>\n",
       "      <td>-0.372066</td>\n",
       "      <td>1.079773</td>\n",
       "      <td>1.266504</td>\n",
       "      <td>-0.452891</td>\n",
       "      <td>-0.515439</td>\n",
       "      <td>1.368659</td>\n",
       "      <td>2.363703</td>\n",
       "      <td>-0.072110</td>\n",
       "      <td>0.940669</td>\n",
       "      <td>-0.339200</td>\n",
       "      <td>-0.177393</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.061477</td>\n",
       "      <td>-0.103412</td>\n",
       "      <td>-0.104429</td>\n",
       "      <td>-0.478378</td>\n",
       "      <td>-0.061477</td>\n",
       "      <td>-0.362047</td>\n",
       "      <td>-0.008087</td>\n",
       "      <td>1.454091</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.255410</td>\n",
       "      <td>-2.249190</td>\n",
       "      <td>0.847480</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.028163</td>\n",
       "      <td>-0.431535</td>\n",
       "      <td>-0.035914</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>-0.478683</td>\n",
       "      <td>-0.359195</td>\n",
       "      <td>-0.720306</td>\n",
       "      <td>-0.461264</td>\n",
       "      <td>-0.287817</td>\n",
       "      <td>-0.588169</td>\n",
       "      <td>-0.164521</td>\n",
       "      <td>0.481054</td>\n",
       "      <td>0.595168</td>\n",
       "      <td>0.708071</td>\n",
       "      <td>0.819755</td>\n",
       "      <td>-0.768479</td>\n",
       "      <td>0.379914</td>\n",
       "      <td>-0.426533</td>\n",
       "      <td>0.788068</td>\n",
       "      <td>0.474102</td>\n",
       "      <td>-0.456845</td>\n",
       "      <td>0.121370</td>\n",
       "      <td>-0.450539</td>\n",
       "      <td>0.143023</td>\n",
       "      <td>-0.209374</td>\n",
       "      <td>-0.259983</td>\n",
       "      <td>-0.310613</td>\n",
       "      <td>0.365894</td>\n",
       "      <td>-0.321383</td>\n",
       "      <td>-0.207553</td>\n",
       "      <td>0.279414</td>\n",
       "      <td>0.287076</td>\n",
       "      <td>0.199721</td>\n",
       "      <td>-0.040167</td>\n",
       "      <td>-0.462847</td>\n",
       "      <td>-0.169006</td>\n",
       "      <td>-0.701523</td>\n",
       "      <td>0.743151</td>\n",
       "      <td>-0.461014</td>\n",
       "      <td>-0.370841</td>\n",
       "      <td>0.391895</td>\n",
       "      <td>0.263321</td>\n",
       "      <td>0.296133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10497</th>\n",
       "      <td>-0.295720</td>\n",
       "      <td>1.121284</td>\n",
       "      <td>-0.670089</td>\n",
       "      <td>-0.661723</td>\n",
       "      <td>0.794159</td>\n",
       "      <td>-0.651661</td>\n",
       "      <td>0.570121</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>0.276782</td>\n",
       "      <td>1.354072</td>\n",
       "      <td>-0.224800</td>\n",
       "      <td>-0.267811</td>\n",
       "      <td>1.319473</td>\n",
       "      <td>-0.327915</td>\n",
       "      <td>1.162105</td>\n",
       "      <td>1.615314</td>\n",
       "      <td>1.514788</td>\n",
       "      <td>-0.371538</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.113214</td>\n",
       "      <td>-0.035939</td>\n",
       "      <td>-0.180349</td>\n",
       "      <td>1.791801</td>\n",
       "      <td>-0.186986</td>\n",
       "      <td>-0.362747</td>\n",
       "      <td>-0.334612</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.385179</td>\n",
       "      <td>0.335403</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.369987</td>\n",
       "      <td>-0.384719</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.392116</td>\n",
       "      <td>0.208058</td>\n",
       "      <td>2.304445</td>\n",
       "      <td>2.411157</td>\n",
       "      <td>-0.129679</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>0.016750</td>\n",
       "      <td>0.159666</td>\n",
       "      <td>-0.035582</td>\n",
       "      <td>2.881006</td>\n",
       "      <td>0.028667</td>\n",
       "      <td>-0.167205</td>\n",
       "      <td>-0.136356</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.198578</td>\n",
       "      <td>0.641279</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.184337</td>\n",
       "      <td>-0.177162</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.179750</td>\n",
       "      <td>0.584400</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>1.210552</td>\n",
       "      <td>0.015140</td>\n",
       "      <td>0.455754</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>0.251040</td>\n",
       "      <td>-0.258457</td>\n",
       "      <td>-0.386105</td>\n",
       "      <td>-1.098276</td>\n",
       "      <td>-1.276505</td>\n",
       "      <td>-1.128656</td>\n",
       "      <td>0.265634</td>\n",
       "      <td>0.643401</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>1.145637</td>\n",
       "      <td>1.078520</td>\n",
       "      <td>0.556694</td>\n",
       "      <td>0.552823</td>\n",
       "      <td>-1.406242</td>\n",
       "      <td>-1.427448</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-1.177326</td>\n",
       "      <td>-0.427663</td>\n",
       "      <td>-1.362771</td>\n",
       "      <td>-1.326675</td>\n",
       "      <td>-0.395687</td>\n",
       "      <td>0.306374</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.058058</td>\n",
       "      <td>-0.478407</td>\n",
       "      <td>-0.000311</td>\n",
       "      <td>-0.536316</td>\n",
       "      <td>-0.058058</td>\n",
       "      <td>-0.254279</td>\n",
       "      <td>-0.367904</td>\n",
       "      <td>0.892150</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>-0.140329</td>\n",
       "      <td>-2.317619</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.034697</td>\n",
       "      <td>-0.495215</td>\n",
       "      <td>-0.027300</td>\n",
       "      <td>-0.455421</td>\n",
       "      <td>-0.035268</td>\n",
       "      <td>1.355682</td>\n",
       "      <td>1.349137</td>\n",
       "      <td>-0.176327</td>\n",
       "      <td>1.045013</td>\n",
       "      <td>-0.026277</td>\n",
       "      <td>1.514871</td>\n",
       "      <td>0.654782</td>\n",
       "      <td>-0.850713</td>\n",
       "      <td>-0.143843</td>\n",
       "      <td>-1.367446</td>\n",
       "      <td>0.082980</td>\n",
       "      <td>-0.721331</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>-1.145801</td>\n",
       "      <td>-0.632472</td>\n",
       "      <td>-0.310951</td>\n",
       "      <td>-0.749462</td>\n",
       "      <td>-0.168996</td>\n",
       "      <td>0.849216</td>\n",
       "      <td>1.269346</td>\n",
       "      <td>-0.226737</td>\n",
       "      <td>-0.115695</td>\n",
       "      <td>0.110056</td>\n",
       "      <td>-0.110959</td>\n",
       "      <td>-0.514640</td>\n",
       "      <td>0.826881</td>\n",
       "      <td>0.762258</td>\n",
       "      <td>-1.371569</td>\n",
       "      <td>-0.680232</td>\n",
       "      <td>0.320252</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10498</th>\n",
       "      <td>1.726894</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>-0.748249</td>\n",
       "      <td>1.047574</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>-0.476484</td>\n",
       "      <td>1.757766</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>1.169588</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.371433</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>1.826698</td>\n",
       "      <td>2.254329</td>\n",
       "      <td>0.338555</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>0.042308</td>\n",
       "      <td>1.469178</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>0.021879</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>0.028437</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>1.841526</td>\n",
       "      <td>2.293568</td>\n",
       "      <td>0.344060</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>0.088583</td>\n",
       "      <td>1.499125</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>0.093623</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>0.108113</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>-0.595482</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.378457</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>-0.595259</td>\n",
       "      <td>-0.590192</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.501843</td>\n",
       "      <td>0.299554</td>\n",
       "      <td>0.693615</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>-0.245624</td>\n",
       "      <td>-0.103658</td>\n",
       "      <td>-0.642387</td>\n",
       "      <td>-0.639803</td>\n",
       "      <td>-0.357959</td>\n",
       "      <td>-0.372066</td>\n",
       "      <td>-0.401870</td>\n",
       "      <td>-0.327503</td>\n",
       "      <td>-0.380188</td>\n",
       "      <td>-0.392349</td>\n",
       "      <td>0.488936</td>\n",
       "      <td>0.028410</td>\n",
       "      <td>0.394760</td>\n",
       "      <td>0.607865</td>\n",
       "      <td>-0.283534</td>\n",
       "      <td>-0.177393</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.054727</td>\n",
       "      <td>-0.467901</td>\n",
       "      <td>-0.090932</td>\n",
       "      <td>-0.484339</td>\n",
       "      <td>-0.054727</td>\n",
       "      <td>-0.212752</td>\n",
       "      <td>0.231791</td>\n",
       "      <td>-0.946580</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.255410</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>1.116882</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>-0.458580</td>\n",
       "      <td>-0.028163</td>\n",
       "      <td>-0.371821</td>\n",
       "      <td>-0.034837</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>-0.478683</td>\n",
       "      <td>-0.359195</td>\n",
       "      <td>-0.720306</td>\n",
       "      <td>-0.461264</td>\n",
       "      <td>-0.287817</td>\n",
       "      <td>-0.588169</td>\n",
       "      <td>-0.164521</td>\n",
       "      <td>0.481054</td>\n",
       "      <td>0.595168</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.816602</td>\n",
       "      <td>-0.837654</td>\n",
       "      <td>0.692779</td>\n",
       "      <td>-0.773825</td>\n",
       "      <td>0.927183</td>\n",
       "      <td>0.546767</td>\n",
       "      <td>-0.530920</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>0.143023</td>\n",
       "      <td>-0.209374</td>\n",
       "      <td>-0.259983</td>\n",
       "      <td>-0.310613</td>\n",
       "      <td>0.365894</td>\n",
       "      <td>-0.321383</td>\n",
       "      <td>-0.207553</td>\n",
       "      <td>0.279414</td>\n",
       "      <td>0.287076</td>\n",
       "      <td>0.199721</td>\n",
       "      <td>0.340404</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>1.221135</td>\n",
       "      <td>-0.858276</td>\n",
       "      <td>0.825696</td>\n",
       "      <td>-0.069761</td>\n",
       "      <td>-0.769992</td>\n",
       "      <td>-0.277606</td>\n",
       "      <td>-0.484911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10499</th>\n",
       "      <td>0.049869</td>\n",
       "      <td>0.137163</td>\n",
       "      <td>0.149275</td>\n",
       "      <td>-0.029934</td>\n",
       "      <td>0.099808</td>\n",
       "      <td>-0.275330</td>\n",
       "      <td>-0.052159</td>\n",
       "      <td>0.165623</td>\n",
       "      <td>0.324727</td>\n",
       "      <td>0.211726</td>\n",
       "      <td>-0.130521</td>\n",
       "      <td>0.258378</td>\n",
       "      <td>-0.151199</td>\n",
       "      <td>0.030762</td>\n",
       "      <td>0.252313</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>0.103405</td>\n",
       "      <td>1.215267</td>\n",
       "      <td>-0.023485</td>\n",
       "      <td>-0.026209</td>\n",
       "      <td>0.287021</td>\n",
       "      <td>0.715386</td>\n",
       "      <td>-0.100735</td>\n",
       "      <td>-0.002891</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.066069</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.007959</td>\n",
       "      <td>0.235306</td>\n",
       "      <td>-0.078371</td>\n",
       "      <td>0.107234</td>\n",
       "      <td>-0.138229</td>\n",
       "      <td>-0.195542</td>\n",
       "      <td>0.218319</td>\n",
       "      <td>0.181620</td>\n",
       "      <td>0.076264</td>\n",
       "      <td>-0.308481</td>\n",
       "      <td>-0.110422</td>\n",
       "      <td>0.313692</td>\n",
       "      <td>0.587327</td>\n",
       "      <td>-0.092379</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>0.073463</td>\n",
       "      <td>0.832647</td>\n",
       "      <td>0.298305</td>\n",
       "      <td>0.341073</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.103240</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>0.873890</td>\n",
       "      <td>0.255457</td>\n",
       "      <td>0.794241</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>0.919118</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>0.352471</td>\n",
       "      <td>0.817414</td>\n",
       "      <td>-0.064760</td>\n",
       "      <td>0.037566</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.042318</td>\n",
       "      <td>0.020636</td>\n",
       "      <td>0.023250</td>\n",
       "      <td>0.256680</td>\n",
       "      <td>-0.053252</td>\n",
       "      <td>0.134300</td>\n",
       "      <td>-0.023721</td>\n",
       "      <td>-0.084820</td>\n",
       "      <td>0.124557</td>\n",
       "      <td>0.211987</td>\n",
       "      <td>0.004776</td>\n",
       "      <td>-0.246291</td>\n",
       "      <td>-0.255487</td>\n",
       "      <td>0.501091</td>\n",
       "      <td>0.829536</td>\n",
       "      <td>0.021022</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>0.133192</td>\n",
       "      <td>0.357582</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.378457</td>\n",
       "      <td>0.513624</td>\n",
       "      <td>0.741906</td>\n",
       "      <td>-0.202106</td>\n",
       "      <td>0.488481</td>\n",
       "      <td>0.808327</td>\n",
       "      <td>-0.501843</td>\n",
       "      <td>-0.169525</td>\n",
       "      <td>0.112567</td>\n",
       "      <td>0.648405</td>\n",
       "      <td>0.810206</td>\n",
       "      <td>0.537818</td>\n",
       "      <td>0.750178</td>\n",
       "      <td>-0.006431</td>\n",
       "      <td>-0.027009</td>\n",
       "      <td>-0.323996</td>\n",
       "      <td>-0.273145</td>\n",
       "      <td>0.250234</td>\n",
       "      <td>0.567978</td>\n",
       "      <td>-0.180873</td>\n",
       "      <td>-0.166812</td>\n",
       "      <td>0.292211</td>\n",
       "      <td>-0.421086</td>\n",
       "      <td>1.311243</td>\n",
       "      <td>0.551022</td>\n",
       "      <td>0.636731</td>\n",
       "      <td>0.071408</td>\n",
       "      <td>-0.131741</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.039342</td>\n",
       "      <td>0.559842</td>\n",
       "      <td>0.032467</td>\n",
       "      <td>1.444172</td>\n",
       "      <td>-0.039342</td>\n",
       "      <td>-0.057714</td>\n",
       "      <td>0.231791</td>\n",
       "      <td>0.201772</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.255410</td>\n",
       "      <td>-2.249190</td>\n",
       "      <td>-0.804854</td>\n",
       "      <td>-2.317619</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.033079</td>\n",
       "      <td>1.593012</td>\n",
       "      <td>-0.026221</td>\n",
       "      <td>0.762760</td>\n",
       "      <td>-0.012216</td>\n",
       "      <td>0.918478</td>\n",
       "      <td>0.948730</td>\n",
       "      <td>-0.663978</td>\n",
       "      <td>0.755554</td>\n",
       "      <td>-0.596474</td>\n",
       "      <td>0.995407</td>\n",
       "      <td>0.535360</td>\n",
       "      <td>-0.531634</td>\n",
       "      <td>-0.127197</td>\n",
       "      <td>-0.675755</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.816602</td>\n",
       "      <td>-0.837654</td>\n",
       "      <td>0.692779</td>\n",
       "      <td>-0.773825</td>\n",
       "      <td>0.927183</td>\n",
       "      <td>0.546767</td>\n",
       "      <td>-0.530920</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>-1.108263</td>\n",
       "      <td>-0.199351</td>\n",
       "      <td>2.316649</td>\n",
       "      <td>0.503525</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>1.235178</td>\n",
       "      <td>1.085443</td>\n",
       "      <td>-0.749917</td>\n",
       "      <td>-1.510394</td>\n",
       "      <td>1.413701</td>\n",
       "      <td>0.340404</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>1.221135</td>\n",
       "      <td>-0.858276</td>\n",
       "      <td>0.825696</td>\n",
       "      <td>-0.069761</td>\n",
       "      <td>-0.769992</td>\n",
       "      <td>-0.277606</td>\n",
       "      <td>-0.484911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10500 rows  187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       daily_rt_tw_0  daily_rt_tw_1  daily_rt_tw_2  daily_rt_tw_3  \\\n",
       "0          -0.033771      -0.288698       0.707072       0.444093   \n",
       "1           0.244816      -0.920621      -0.792514      -0.748249   \n",
       "2          -0.776668      -0.920621      -0.792514      -0.748249   \n",
       "3          -0.776668      -0.920621      -0.792514       0.709058   \n",
       "4          -0.033771      -0.920621       0.707072       0.841541   \n",
       "...              ...            ...            ...            ...   \n",
       "10495       0.173765       0.129584       1.125997      -0.748249   \n",
       "10496      -0.572911      -0.920621      -0.792514       1.219693   \n",
       "10497      -0.295720       1.121284      -0.670089      -0.661723   \n",
       "10498       1.726894      -0.920621      -0.792514      -0.748249   \n",
       "10499       0.049869       0.137163       0.149275      -0.029934   \n",
       "\n",
       "       daily_rt_tw_4  daily_rt_tw_5  daily_rt_tw_6  daily_rt_0  daily_rt_1  \\\n",
       "0           0.254897      -0.677962      -0.292344    0.041097   -0.181523   \n",
       "1           0.611602       0.650969       0.500071   -0.683237   -0.713969   \n",
       "2           1.919519       1.979899      -0.745153   -0.683237   -0.713969   \n",
       "3           1.047574      -0.677962       0.915145   -0.683237   -0.713969   \n",
       "4          -0.220710      -0.677962       0.160464   -0.683237   -0.713969   \n",
       "...              ...            ...            ...         ...         ...   \n",
       "10495      -0.696316       0.558533       0.007379    0.243446   -0.713969   \n",
       "10496       1.919519      -0.677962      -0.745153   -0.484571   -0.713969   \n",
       "10497       0.794159      -0.651661       0.570121   -0.683237   -0.713969   \n",
       "10498       1.047574      -0.677962      -0.476484    1.757766   -0.713969   \n",
       "10499       0.099808      -0.275330      -0.052159    0.165623    0.324727   \n",
       "\n",
       "       daily_rt_2  daily_rt_3  daily_rt_4  daily_rt_5  daily_rt_6  daily_tw_0  \\\n",
       "0        0.683446    0.520329    0.367691   -0.575360   -0.189205   -0.372495   \n",
       "1       -0.619365   -0.636998   -0.594585   -0.575360   -0.637312    1.006502   \n",
       "2       -0.619365   -0.636998   -0.594585   -0.575360   -0.637312   -0.372495   \n",
       "3       -0.619365    0.423885    2.051674   -0.575360    0.594983   -0.372495   \n",
       "4       -0.619365   -0.636998    4.697934   -0.575360   -0.637312    0.730702   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "10495    1.047400   -0.636998   -0.594585    0.697046   -0.637312   -0.372495   \n",
       "10496   -0.619365    1.273151    2.051674   -0.575360   -0.637312   -0.372495   \n",
       "10497   -0.619365   -0.636998   -0.594585   -0.575360   -0.637312    0.276782   \n",
       "10498   -0.619365   -0.636998    1.169588   -0.575360   -0.371433   -0.372495   \n",
       "10499    0.211726   -0.130521    0.258378   -0.151199    0.030762    0.252313   \n",
       "\n",
       "       daily_tw_1  daily_tw_2  daily_tw_3  daily_tw_4  daily_tw_5  daily_tw_6  \\\n",
       "0       -0.396039   -0.341369   -0.379251   -0.356703   -0.356795   -0.370328   \n",
       "1       -0.396039   -0.341369   -0.379251    1.114170    1.102461    1.080489   \n",
       "2       -0.396039   -0.341369   -0.379251    2.585044    2.561717   -0.370328   \n",
       "3       -0.396039   -0.341369    2.436123   -0.356703   -0.356795    2.531306   \n",
       "4       -0.396039    1.229269    1.873048   -0.356703   -0.356795    0.790326   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "10495    0.504089   -0.341369   -0.379251   -0.356703   -0.356795    0.506451   \n",
       "10496   -0.396039   -0.341369   -0.379251   -0.356703   -0.356795   -0.370328   \n",
       "10497    1.354072   -0.224800   -0.267811    1.319473   -0.327915    1.162105   \n",
       "10498   -0.396039   -0.341369   -0.379251   -0.356703   -0.356795   -0.370328   \n",
       "10499   -0.396039    0.103405    1.215267   -0.023485   -0.026209    0.287021   \n",
       "\n",
       "       hour_rt_tw_0  hour_rt_tw_1  hour_rt_tw_2  hour_rt_tw_3  hour_rt_tw_4  \\\n",
       "0         -0.436619      0.185990      0.823534      0.325808     -0.333863   \n",
       "1         -0.436619     -0.456568     -0.440325      1.592639     -0.333863   \n",
       "2          3.377219     -0.456568     -0.440325     -0.398096     -0.333863   \n",
       "3         -0.436619      1.899479     -0.440325     -0.398096     -0.333863   \n",
       "4          0.256806      0.828549      0.823534     -0.398096     -0.333863   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "10495     -0.436619      1.187559     -0.440325      0.804974     -0.333863   \n",
       "10496     -0.436619     -0.456568     -0.440325     -0.398096     -0.333863   \n",
       "10497      1.615314      1.514788     -0.371538     -0.398096     -0.333863   \n",
       "10498     -0.436619     -0.456568     -0.440325      1.826698      2.254329   \n",
       "10499      0.715386     -0.100735     -0.002891     -0.398096     -0.333863   \n",
       "\n",
       "       hour_rt_tw_5  hour_rt_tw_6  hour_rt_tw_7  hour_rt_tw_8  hour_rt_tw_9  \\\n",
       "0         -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "1         -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "2         -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "3         -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "4         -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "10495     -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "10496     -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "10497     -0.274391     -0.229903     -0.185057     -0.113214     -0.035939   \n",
       "10498      0.338555     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "10499     -0.274391     -0.066069     -0.000142      0.007959      0.235306   \n",
       "\n",
       "       hour_rt_tw_10  hour_rt_tw_11  hour_rt_tw_12  hour_rt_tw_13  \\\n",
       "0           0.933970       0.668776      -0.366339       0.347962   \n",
       "1          -0.244488      -0.304238      -0.366339      -0.403655   \n",
       "2          -0.244488      -0.304238      -0.366339      -0.403655   \n",
       "3          -0.244488      -0.304238      -0.366339      -0.403655   \n",
       "4          -0.244488      -0.304238      -0.366339      -0.403655   \n",
       "...              ...            ...            ...            ...   \n",
       "10495      -0.244488       1.312833      -0.366339      -0.403655   \n",
       "10496      -0.244488      -0.304238       0.085573      -0.403655   \n",
       "10497      -0.180349       1.791801      -0.186986      -0.362747   \n",
       "10498      -0.244488      -0.304238      -0.366339       0.042308   \n",
       "10499      -0.078371       0.107234      -0.138229      -0.195542   \n",
       "\n",
       "       hour_rt_tw_14  hour_rt_tw_15  hour_rt_tw_16  hour_rt_tw_17  \\\n",
       "0           0.343309      -0.427253       0.296254       0.318855   \n",
       "1          -0.417419      -0.427253       3.539206      -0.411422   \n",
       "2          -0.417419      -0.427253      -0.424402      -0.411422   \n",
       "3          -0.417419      -0.427253      -0.424402      -0.411422   \n",
       "4           0.343309       1.025532      -0.424402       0.318855   \n",
       "...              ...            ...            ...            ...   \n",
       "10495      -0.417419      -0.427253      -0.424402       1.457153   \n",
       "10496      -0.417419      -0.427253      -0.424402      -0.411422   \n",
       "10497      -0.334612      -0.427253      -0.385179       0.335403   \n",
       "10498       1.469178      -0.427253      -0.424402       0.021879   \n",
       "10499       0.218319       0.181620       0.076264      -0.308481   \n",
       "\n",
       "       hour_rt_tw_18  hour_rt_tw_19  hour_rt_tw_20  hour_rt_tw_21  \\\n",
       "0           0.316542      -0.410225      -0.428838      -0.432100   \n",
       "1          -0.416884      -0.410225      -0.428838      -0.432100   \n",
       "2          -0.416884      -0.410225      -0.428838      -0.432100   \n",
       "3           0.927730      -0.410225       1.057321       1.058441   \n",
       "4          -0.416884      -0.410225       0.381794       0.380923   \n",
       "...              ...            ...            ...            ...   \n",
       "10495      -0.416884      -0.410225       1.645343      -0.432100   \n",
       "10496      -0.416884       3.250440      -0.428838      -0.432100   \n",
       "10497      -0.416884      -0.369987      -0.384719      -0.432100   \n",
       "10498      -0.416884       0.028437      -0.428838      -0.432100   \n",
       "10499      -0.110422       0.313692       0.587327      -0.092379   \n",
       "\n",
       "       hour_rt_tw_22  hour_rt_tw_23  hour_tw_0  hour_tw_1  hour_tw_2  \\\n",
       "0          -0.433070      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "1          -0.433070       1.624238  -0.237819  -0.240063  -0.229344   \n",
       "2           3.705574      -0.448203   4.487376  -0.240063  -0.229344   \n",
       "3          -0.433070       0.933425  -0.237819   4.512808  -0.229344   \n",
       "4          -0.433070      -0.448203   0.707220   0.710511   1.784980   \n",
       "...              ...            ...        ...        ...        ...   \n",
       "10495      -0.433070      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "10496       3.705574      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "10497      -0.392116       0.208058   2.304445   2.411157  -0.129679   \n",
       "10498      -0.433070      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "10499      -0.433070       0.073463   0.832647   0.298305   0.341073   \n",
       "\n",
       "       hour_tw_3  hour_tw_4  hour_tw_5  hour_tw_6  hour_tw_7  hour_tw_8  \\\n",
       "0      -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "1       2.596364  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "2      -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "3      -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "4      -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10495   1.486305  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "10496  -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "10497  -0.209185  -0.171581  -0.139658  -0.123363  -0.099588   0.016750   \n",
       "10498  -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "10499  -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "\n",
       "       hour_tw_9  hour_tw_10  hour_tw_11  hour_tw_12  hour_tw_13  hour_tw_14  \\\n",
       "0      -0.103240   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "1      -0.103240   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "2      -0.103240   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "3      -0.103240   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "4      -0.103240   -0.129052   -0.160847   -0.201605   -0.218649    0.744188   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "10495  -0.103240   -0.129052    2.185909   -0.201605   -0.218649   -0.233061   \n",
       "10496  -0.103240   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "10497   0.159666   -0.035582    2.881006    0.028667   -0.167205   -0.136356   \n",
       "10498  -0.103240   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "10499  -0.103240   -0.129052   -0.160847   -0.201605   -0.218649    0.873890   \n",
       "\n",
       "       hour_tw_15  hour_tw_16  hour_tw_17  hour_tw_18  hour_tw_19  hour_tw_20  \\\n",
       "0       -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "1       -0.243309    4.338693   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "2       -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "3       -0.243309   -0.243926   -0.233566   -0.235228   -0.230935    4.857426   \n",
       "4        1.517994   -0.243926    0.707440   -0.235228   -0.230935    0.789501   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "10495   -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "10496   -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "10497   -0.243309   -0.198578    0.641279   -0.235228   -0.184337   -0.177162   \n",
       "10498   -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "10499    0.255457    0.794241   -0.233566    0.919118   -0.230935   -0.227481   \n",
       "\n",
       "       hour_tw_21  hour_tw_22  hour_tw_23  hour_rt_0  hour_rt_1  hour_rt_2  \\\n",
       "0       -0.225439   -0.229070   -0.230576  -0.376853   0.228385   0.811238   \n",
       "1       -0.225439   -0.229070    2.343080  -0.376853  -0.400679  -0.390113   \n",
       "2       -0.225439    4.754944   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "3       -0.225439   -0.229070   -0.230576  -0.376853   1.329247  -0.390113   \n",
       "4        0.789379   -0.229070   -0.230576  -0.376853   6.519026  -0.390113   \n",
       "...           ...         ...         ...        ...        ...        ...   \n",
       "10495   -0.225439   -0.229070   -0.230576  -0.376853   1.208920  -0.390113   \n",
       "10496   -0.225439   -0.229070   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "10497   -0.225439   -0.179750    0.584400  -0.376853  -0.400679  -0.390113   \n",
       "10498   -0.225439   -0.229070   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "10499   -0.225439   -0.229070    0.352471   0.817414  -0.064760   0.037566   \n",
       "\n",
       "       hour_rt_3  hour_rt_4  hour_rt_5  hour_rt_6  hour_rt_7  hour_rt_8  \\\n",
       "0       0.359363  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "1      -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "2      -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "3      -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "4      -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10495  -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "10496  -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "10497  -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "10498   1.841526   2.293568   0.344060  -0.204794  -0.171377  -0.174216   \n",
       "10499  -0.355508  -0.297520  -0.249225  -0.042318   0.020636   0.023250   \n",
       "\n",
       "       hour_rt_9  hour_rt_10  hour_rt_11  hour_rt_12  hour_rt_13  hour_rt_14  \\\n",
       "0      -0.193251    0.926646    0.682208   -0.308974    0.385695    0.394744   \n",
       "1      -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "2      -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "3      -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "4      -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "10495  -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "10496  -0.193251   -0.214046   -0.267182    0.130563   -0.344916   -0.351465   \n",
       "10497  -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "10498  -0.193251   -0.214046   -0.267182   -0.308974    0.088583    1.499125   \n",
       "10499   0.256680   -0.053252    0.134300   -0.023721   -0.084820    0.124557   \n",
       "\n",
       "       hour_rt_15  hour_rt_16  hour_rt_17  hour_rt_18  hour_rt_19  hour_rt_20  \\\n",
       "0       -0.361243    0.370076    0.399188    0.360211   -0.354061   -0.366727   \n",
       "1       -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "2       -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "3       -0.361243   -0.356107   -0.352209    1.614488   -0.354061   -0.366727   \n",
       "4       -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "10495   -0.361243   -0.356107    1.570405   -0.356518   -0.354061    1.686359   \n",
       "10496   -0.361243   -0.356107   -0.352209   -0.356518    3.502816   -0.366727   \n",
       "10497   -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "10498   -0.361243   -0.356107    0.093623   -0.356518    0.108113   -0.366727   \n",
       "10499    0.211987    0.004776   -0.246291   -0.255487    0.501091    0.829536   \n",
       "\n",
       "       hour_rt_21  hour_rt_22  hour_rt_23  mst_fr_ment_tw_1  mst_fr_ment_tw_2  \\\n",
       "0       -0.363244   -0.377549   -0.380247         -0.595482         -0.546279   \n",
       "1       -0.363244   -0.377549   -0.380247          1.173888         -0.546279   \n",
       "2       -0.363244   -0.377549   -0.380247          2.166983         -0.546279   \n",
       "3        1.763145   -0.377549    1.602849          2.069672          2.016624   \n",
       "4       -0.363244   -0.377549   -0.380247          3.085846          1.833372   \n",
       "...           ...         ...         ...               ...               ...   \n",
       "10495   -0.363244   -0.377549   -0.380247          0.205753          0.533642   \n",
       "10496   -0.363244    3.823981   -0.380247         -0.595482         -0.546279   \n",
       "10497   -0.363244   -0.377549   -0.380247          1.210552          0.015140   \n",
       "10498   -0.363244   -0.377549   -0.380247         -0.595482         -0.546279   \n",
       "10499    0.021022   -0.377549    0.133192          0.357582         -0.546279   \n",
       "\n",
       "       mst_fr_ment_tw_3  mst_fr_ment_rt_1  mst_fr_ment_rt_2  mst_fr_ment_rt_3  \\\n",
       "0             -0.502808         -0.378457          0.946685          1.737489   \n",
       "1             -0.502808         -0.993747         -0.984585         -0.844317   \n",
       "2             -0.502808         -0.993747         -0.984585         -0.844317   \n",
       "3             -0.502808         -0.112664         -0.409112          2.381850   \n",
       "4              3.373289         -0.993747         -0.984585         -0.844317   \n",
       "...                 ...               ...               ...               ...   \n",
       "10495         -0.009644         -0.993747         -0.984585         -0.844317   \n",
       "10496         -0.502808         -0.856891         -0.902396         -0.788854   \n",
       "10497          0.455754         -0.993747         -0.984585         -0.844317   \n",
       "10498         -0.502808         -0.378457         -0.984585         -0.844317   \n",
       "10499         -0.502808         -0.378457          0.513624          0.741906   \n",
       "\n",
       "       mst_fr_hs_tw_1  mst_fr_hs_tw_2  mst_fr_hs_tw_3  mst_fr_hs_rt_1  \\\n",
       "0           -0.595259       -0.590192       -0.543541       -0.501843   \n",
       "1            0.536020        2.557913        2.838443       -1.098276   \n",
       "2            0.329884       -0.590192       -0.543541       -1.098276   \n",
       "3            0.500783        0.212400        0.775386        0.162182   \n",
       "4            0.513363        0.790230        0.445463        4.446349   \n",
       "...               ...             ...             ...             ...   \n",
       "10495       -0.420462       -0.042913       -0.543541       -0.682066   \n",
       "10496       -0.595259       -0.590192       -0.543541       -0.356939   \n",
       "10497        0.251040       -0.258457       -0.386105       -1.098276   \n",
       "10498       -0.595259       -0.590192       -0.543541       -0.501843   \n",
       "10499       -0.202106        0.488481        0.808327       -0.501843   \n",
       "\n",
       "       mst_fr_hs_rt_2  mst_fr_hs_rt_3  tw_urls_avg  tw_urls_std  rt_urls_avg  \\\n",
       "0           -0.133677       -0.043415    -0.516134    -0.572468     0.788158   \n",
       "1           -1.276505       -1.128656     0.218215     0.759387    -0.973966   \n",
       "2           -1.276505       -1.128656     0.952564     1.311059    -0.973966   \n",
       "3            2.066703       -0.287796     0.952564     1.311059     1.448955   \n",
       "4           -0.335372        0.477376     1.540043     1.656151    -0.973966   \n",
       "...               ...             ...          ...          ...          ...   \n",
       "10495       -1.276505       -1.128656    -0.516134    -0.572468    -0.222502   \n",
       "10496        1.382975        1.148335    -0.516134    -0.572468    -0.812864   \n",
       "10497       -1.276505       -1.128656     0.265634     0.643401    -0.973966   \n",
       "10498        0.299554        0.693615    -0.516134    -0.572468    -0.245624   \n",
       "10499       -0.169525        0.112567     0.648405     0.810206     0.537818   \n",
       "\n",
       "       rt_urls_std  tw_hash_avg  tw_hash_std  tw_ment_avg  tw_ment_std  \\\n",
       "0         0.839594    -0.642387    -0.639803    -0.357959    -0.372066   \n",
       "1        -1.161941     2.660205     2.706995    -0.208041    -0.119966   \n",
       "2        -1.161941     0.018132    -0.038700    -0.058123    -0.015543   \n",
       "3         1.868033     0.678650     0.562402     0.241712     0.132134   \n",
       "4        -1.161941     1.405220     1.617335    -0.178057    -0.015543   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10495    -0.070062    -0.343006    -0.352615    -0.086157    -0.131175   \n",
       "10496    -0.970815    -0.642387    -0.639803    -0.357959    -0.372066   \n",
       "10497    -1.161941     1.145637     1.078520     0.556694     0.552823   \n",
       "10498    -0.103658    -0.642387    -0.639803    -0.357959    -0.372066   \n",
       "10499     0.750178    -0.006431    -0.027009    -0.323996    -0.273145   \n",
       "\n",
       "       rt_hash_avg  rt_hash_std  rt_ment_avg  rt_ment_std  rt_time_avg  \\\n",
       "0        -0.245943    -0.038697    -0.282074    -0.168967     0.515613   \n",
       "1        -1.406242    -1.427448    -0.536694    -0.599163    -1.177326   \n",
       "2        -1.406242    -1.427448    -0.536694    -0.599163    -1.177326   \n",
       "3         0.441076     0.239328     0.443594     0.307552     2.053338   \n",
       "4         0.609014     0.313443    -0.536694    -0.599163    -0.780280   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10495    -1.093728    -1.096808    -0.536694    -0.599163    -0.994096   \n",
       "10496     1.079773     1.266504    -0.452891    -0.515439     1.368659   \n",
       "10497    -1.406242    -1.427448    -0.536694    -0.599163    -1.177326   \n",
       "10498    -0.401870    -0.327503    -0.380188    -0.392349     0.488936   \n",
       "10499     0.250234     0.567978    -0.180873    -0.166812     0.292211   \n",
       "\n",
       "       rt_time_min  rt_time_max  rt_time_std    rt_avg    tw_avg  \\\n",
       "0        -0.306350     1.209928     0.682940 -0.085005 -0.177393   \n",
       "1        -0.427663    -1.362771    -1.326675 -0.395687 -0.055366   \n",
       "2        -0.427663    -1.362771    -1.326675 -0.395687 -0.116379   \n",
       "3        -0.426781     1.453246     2.138906 -0.282712 -0.116379   \n",
       "4         0.016122    -1.165264    -0.973500 -0.367444  0.127675   \n",
       "...            ...          ...          ...       ...       ...   \n",
       "10495    -0.277745    -1.255131    -1.160672 -0.336559 -0.158957   \n",
       "10496     2.363703    -0.072110     0.940669 -0.339200 -0.177393   \n",
       "10497    -0.427663    -1.362771    -1.326675 -0.395687  0.306374   \n",
       "10498     0.028410     0.394760     0.607865 -0.283534 -0.177393   \n",
       "10499    -0.421086     1.311243     0.551022  0.636731  0.071408   \n",
       "\n",
       "       tw_rt_ration  verified  followers  favourites    listed  statuses  \\\n",
       "0         -0.163073 -0.183766  -0.059734    0.362989 -0.102501 -0.255750   \n",
       "1         -0.163073  5.441713  -0.008488   -0.596905 -0.019592 -0.534887   \n",
       "2         -0.163073  5.441713   0.034919    0.130780  0.317829 -0.110854   \n",
       "3         -0.082397 -0.183766  -0.048424   -0.184284 -0.038873  0.284990   \n",
       "4          1.450448 -0.183766  -0.057275   -0.494245 -0.100573 -0.472242   \n",
       "...             ...       ...        ...         ...       ...       ...   \n",
       "10495     -0.163073 -0.183766  -0.061537   -0.436599 -0.104429 -0.564141   \n",
       "10496     -0.163073 -0.183766  -0.061477   -0.103412 -0.104429 -0.478378   \n",
       "10497     -0.163073 -0.183766  -0.058058   -0.478407 -0.000311 -0.536316   \n",
       "10498     -0.163073 -0.183766  -0.054727   -0.467901 -0.090932 -0.484339   \n",
       "10499     -0.131741 -0.183766  -0.039342    0.559842  0.032467  1.444172   \n",
       "\n",
       "       followers.1  friends_count  name_len  name_screen_sim       geo  \\\n",
       "0        -0.059734      -0.376505 -1.087539         0.919057 -0.623904   \n",
       "1        -0.008488       0.515575 -0.367904         1.572604  1.602810   \n",
       "2         0.034919       0.294298 -0.128026         1.648222  1.602810   \n",
       "3        -0.048424      -0.187630  0.351730        -0.506866  1.602810   \n",
       "4        -0.057275      -0.191321  0.231791        -0.417746 -0.623904   \n",
       "...            ...            ...       ...              ...       ...   \n",
       "10495    -0.061537      -0.343897 -1.087539        -0.768308 -0.623904   \n",
       "10496    -0.061477      -0.362047 -0.008087         1.454091 -0.623904   \n",
       "10497    -0.058058      -0.254279 -0.367904         0.892150 -0.623904   \n",
       "10498    -0.054727      -0.212752  0.231791        -0.946580 -0.623904   \n",
       "10499    -0.039342      -0.057714  0.231791         0.201772 -0.623904   \n",
       "\n",
       "       protected  location  description  description_len  bckg_img  \\\n",
       "0            0.0 -1.255410    -2.249190        -1.684902  0.431477   \n",
       "1            0.0  0.796552     0.444604         0.416436  0.431477   \n",
       "2            0.0  0.796552     0.444604         1.188723 -2.317619   \n",
       "3            0.0  0.796552     0.444604         1.152803  0.431477   \n",
       "4            0.0  0.796552     0.444604         1.170763  0.431477   \n",
       "...          ...       ...          ...              ...       ...   \n",
       "10495        0.0 -1.255410    -2.249190        -1.002416  0.431477   \n",
       "10496        0.0 -1.255410    -2.249190         0.847480  0.431477   \n",
       "10497        0.0  0.796552     0.444604        -0.140329 -2.317619   \n",
       "10498        0.0 -1.255410     0.444604         1.116882  0.431477   \n",
       "10499        0.0 -1.255410    -2.249190        -0.804854 -2.317619   \n",
       "\n",
       "       default_prof  entities   rt_self  in_degree  out_degree  w_in_degree  \\\n",
       "0          0.749070 -0.445679 -0.256252  -0.036315    0.017683    -0.028163   \n",
       "1         -1.334988  2.243765 -0.256252  -0.035506   -0.531851    -0.027948   \n",
       "2         -1.334988  2.243765 -0.256252  -0.015286   -0.531851    -0.013275   \n",
       "3         -1.334988 -0.445679 -0.256252  -0.034697   -0.531851    -0.027300   \n",
       "4         -1.334988 -0.445679 -0.256252  -0.024183   -0.348673    -0.022122   \n",
       "...             ...       ...       ...        ...         ...          ...   \n",
       "10495      0.749070 -0.445679 -0.256252  -0.036315   -0.312037    -0.028163   \n",
       "10496      0.749070 -0.445679 -0.256252  -0.036315   -0.421944    -0.028163   \n",
       "10497     -1.334988 -0.445679 -0.256252  -0.034697   -0.495215    -0.027300   \n",
       "10498     -1.334988 -0.445679 -0.256252  -0.036315   -0.458580    -0.028163   \n",
       "10499     -1.334988 -0.445679 -0.256252  -0.033079    1.593012    -0.026221   \n",
       "\n",
       "       w_out_degree  w_degree  hs_tw_v_0  hs_tw_v_1  hs_tw_v_2  hs_tw_v_3  \\\n",
       "0         -0.097133 -0.029882   0.079034  -0.478683  -0.359195  -0.720306   \n",
       "1         -0.467364 -0.036345  -1.372834  -1.005197   1.745834   0.793267   \n",
       "2         -0.467364 -0.021695  -2.212239  -0.490848   1.564761   1.829846   \n",
       "3         -0.156847 -0.030097  -1.934675   1.818096   1.256643   0.534822   \n",
       "4         -0.407649 -0.029451  -1.078564   1.886311   1.606272  -0.098455   \n",
       "...             ...       ...        ...        ...        ...        ...   \n",
       "10495     -0.395706 -0.035268   0.452254   0.155950  -0.494702  -0.064133   \n",
       "10496     -0.431535 -0.035914   0.079034  -0.478683  -0.359195  -0.720306   \n",
       "10497     -0.455421 -0.035268   1.355682   1.349137  -0.176327   1.045013   \n",
       "10498     -0.371821 -0.034837   0.079034  -0.478683  -0.359195  -0.720306   \n",
       "10499      0.762760 -0.012216   0.918478   0.948730  -0.663978   0.755554   \n",
       "\n",
       "       hs_tw_v_4  hs_tw_v_5  hs_tw_v_6  hs_tw_v_7  hs_tw_v_8  hs_tw_v_9  \\\n",
       "0      -0.461264  -0.287817  -0.588169  -0.164521   0.481054   0.595168   \n",
       "1       2.129627  -2.505875  -0.500508   3.846728  -2.324506  -1.664559   \n",
       "2       1.769630  -1.612474   1.527780   2.363210  -1.708498  -0.445103   \n",
       "3       2.993217  -0.618761  -1.192867   2.241272  -1.691256  -2.296444   \n",
       "4       0.241667   1.466633   1.015162   1.470123   2.465563  -2.000625   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10495  -0.521379   0.282709  -0.088644  -0.327741   0.210624   0.030111   \n",
       "10496  -0.461264  -0.287817  -0.588169  -0.164521   0.481054   0.595168   \n",
       "10497  -0.026277   1.514871   0.654782  -0.850713  -0.143843  -1.367446   \n",
       "10498  -0.461264  -0.287817  -0.588169  -0.164521   0.481054   0.595168   \n",
       "10499  -0.596474   0.995407   0.535360  -0.531634  -0.127197  -0.675755   \n",
       "\n",
       "       hs_rt_v_0  hs_rt_v_1  hs_rt_v_2  hs_rt_v_3  hs_rt_v_4  hs_rt_v_5  \\\n",
       "0       0.893168   0.816602  -0.837654   0.692779  -0.773825   0.927183   \n",
       "1       0.082980  -0.721331  -0.484803  -1.145801  -0.632472  -0.310951   \n",
       "2       0.082980  -0.721331  -0.484803  -1.145801  -0.632472  -0.310951   \n",
       "3      -1.005272  -1.395498   2.132432   1.108192   0.796068  -0.421541   \n",
       "4       0.187270  -0.761071  -0.468909  -1.226953  -0.536228  -0.383315   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "10495   0.648356   0.351889  -0.731034   0.137220  -0.731113   0.553059   \n",
       "10496   0.708071   0.819755  -0.768479   0.379914  -0.426533   0.788068   \n",
       "10497   0.082980  -0.721331  -0.484803  -1.145801  -0.632472  -0.310951   \n",
       "10498   0.893168   0.816602  -0.837654   0.692779  -0.773825   0.927183   \n",
       "10499   0.893168   0.816602  -0.837654   0.692779  -0.773825   0.927183   \n",
       "\n",
       "       hs_rt_v_6  hs_rt_v_7  hs_rt_v_8  hs_rt_v_9  ment_tw_v_0  ment_tw_v_1  \\\n",
       "0       0.546767  -0.530920   0.110069  -0.317943     0.143023    -0.209374   \n",
       "1      -0.749462  -0.168996   0.849216   1.269346    -2.242878    -1.783231   \n",
       "2      -0.749462  -0.168996   0.849216   1.269346     0.262169    -0.381322   \n",
       "3       1.363867  -0.084468  -0.369064   1.575669     0.014068     0.085375   \n",
       "4      -0.711690  -0.181658   0.818366   1.377542     0.143183    -0.246317   \n",
       "...          ...        ...        ...        ...          ...          ...   \n",
       "10495   0.155089  -0.421558   0.333415   0.161684     0.173824    -0.285664   \n",
       "10496   0.474102  -0.456845   0.121370  -0.450539     0.143023    -0.209374   \n",
       "10497  -0.749462  -0.168996   0.849216   1.269346    -0.226737    -0.115695   \n",
       "10498   0.546767  -0.530920   0.110069  -0.317943     0.143023    -0.209374   \n",
       "10499   0.546767  -0.530920   0.110069  -0.317943    -1.108263    -0.199351   \n",
       "\n",
       "       ment_tw_v_2  ment_tw_v_3  ment_tw_v_4  ment_tw_v_5  ment_tw_v_6  \\\n",
       "0        -0.259983    -0.310613     0.365894    -0.321383    -0.207553   \n",
       "1         0.264788    -0.371487    -0.148661    -0.183347     2.056465   \n",
       "2        -0.364314    -0.634174     0.386674    -0.453555    -0.110713   \n",
       "3        -0.520774    -0.660333     0.812923    -0.677726    -0.565583   \n",
       "4        -0.283340    -0.314630     0.351092    -0.330400    -0.184736   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10495    -0.322497    -0.432703     0.367820    -0.410416    -0.186829   \n",
       "10496    -0.259983    -0.310613     0.365894    -0.321383    -0.207553   \n",
       "10497     0.110056    -0.110959    -0.514640     0.826881     0.762258   \n",
       "10498    -0.259983    -0.310613     0.365894    -0.321383    -0.207553   \n",
       "10499     2.316649     0.503525     0.104478     1.235178     1.085443   \n",
       "\n",
       "       ment_tw_v_7  ment_tw_v_8  ment_tw_v_9  ment_rt_v_0  ment_rt_v_1  \\\n",
       "0         0.279414     0.287076     0.199721     0.340404     0.903730   \n",
       "1        -2.754610    -0.957024    -0.073996     0.266555    -0.299594   \n",
       "2         0.117676     0.128495     0.334223     0.266555    -0.299594   \n",
       "3         0.141846    -0.286970     0.509814    -1.273979     0.663287   \n",
       "4         0.248969     0.271609     0.181822     0.266555    -0.299594   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10495     0.228706     0.217499     0.407109     0.266555    -0.299594   \n",
       "10496     0.279414     0.287076     0.199721    -0.040167    -0.462847   \n",
       "10497    -1.371569    -0.680232     0.320252     0.266555    -0.299594   \n",
       "10498     0.279414     0.287076     0.199721     0.340404     0.903730   \n",
       "10499    -0.749917    -1.510394     1.413701     0.340404     0.903730   \n",
       "\n",
       "       ment_rt_v_2  ment_rt_v_3  ment_rt_v_4  ment_rt_v_5  ment_rt_v_6  \\\n",
       "0         0.541693     1.221135    -0.858276     0.825696    -0.069761   \n",
       "1        -0.534590    -0.591333     0.734788    -0.203802    -0.230186   \n",
       "2        -0.534590    -0.591333     0.734788    -0.203802    -0.230186   \n",
       "3         0.906776     0.546649    -0.660916     0.506244     0.113732   \n",
       "4        -0.534590    -0.591333     0.734788    -0.203802    -0.230186   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "10495    -0.534590    -0.591333     0.734788    -0.203802    -0.230186   \n",
       "10496    -0.169006    -0.701523     0.743151    -0.461014    -0.370841   \n",
       "10497    -0.534590    -0.591333     0.734788    -0.203802    -0.230186   \n",
       "10498     0.541693     1.221135    -0.858276     0.825696    -0.069761   \n",
       "10499     0.541693     1.221135    -0.858276     0.825696    -0.069761   \n",
       "\n",
       "       ment_rt_v_7  ment_rt_v_8  ment_rt_v_9  \n",
       "0        -0.769992    -0.277606    -0.484911  \n",
       "1         0.290932    -0.079112     0.530592  \n",
       "2         0.290932    -0.079112     0.530592  \n",
       "3         0.506675    -0.582343     0.034164  \n",
       "4         0.290932    -0.079112     0.530592  \n",
       "...            ...          ...          ...  \n",
       "10495     0.290932    -0.079112     0.530592  \n",
       "10496     0.391895     0.263321     0.296133  \n",
       "10497     0.290932    -0.079112     0.530592  \n",
       "10498    -0.769992    -0.277606    -0.484911  \n",
       "10499    -0.769992    -0.277606    -0.484911  \n",
       "\n",
       "[10500 rows x 187 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10500,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>daily_rt_tw_0</th>\n",
       "      <th>daily_rt_tw_1</th>\n",
       "      <th>daily_rt_tw_2</th>\n",
       "      <th>daily_rt_tw_3</th>\n",
       "      <th>daily_rt_tw_4</th>\n",
       "      <th>daily_rt_tw_5</th>\n",
       "      <th>daily_rt_tw_6</th>\n",
       "      <th>daily_rt_0</th>\n",
       "      <th>daily_rt_1</th>\n",
       "      <th>daily_rt_2</th>\n",
       "      <th>daily_rt_3</th>\n",
       "      <th>daily_rt_4</th>\n",
       "      <th>daily_rt_5</th>\n",
       "      <th>daily_rt_6</th>\n",
       "      <th>daily_tw_0</th>\n",
       "      <th>daily_tw_1</th>\n",
       "      <th>daily_tw_2</th>\n",
       "      <th>daily_tw_3</th>\n",
       "      <th>daily_tw_4</th>\n",
       "      <th>daily_tw_5</th>\n",
       "      <th>daily_tw_6</th>\n",
       "      <th>hour_rt_tw_0</th>\n",
       "      <th>hour_rt_tw_1</th>\n",
       "      <th>hour_rt_tw_2</th>\n",
       "      <th>hour_rt_tw_3</th>\n",
       "      <th>hour_rt_tw_4</th>\n",
       "      <th>hour_rt_tw_5</th>\n",
       "      <th>hour_rt_tw_6</th>\n",
       "      <th>hour_rt_tw_7</th>\n",
       "      <th>hour_rt_tw_8</th>\n",
       "      <th>hour_rt_tw_9</th>\n",
       "      <th>hour_rt_tw_10</th>\n",
       "      <th>hour_rt_tw_11</th>\n",
       "      <th>hour_rt_tw_12</th>\n",
       "      <th>hour_rt_tw_13</th>\n",
       "      <th>hour_rt_tw_14</th>\n",
       "      <th>hour_rt_tw_15</th>\n",
       "      <th>hour_rt_tw_16</th>\n",
       "      <th>hour_rt_tw_17</th>\n",
       "      <th>hour_rt_tw_18</th>\n",
       "      <th>hour_rt_tw_19</th>\n",
       "      <th>hour_rt_tw_20</th>\n",
       "      <th>hour_rt_tw_21</th>\n",
       "      <th>hour_rt_tw_22</th>\n",
       "      <th>hour_rt_tw_23</th>\n",
       "      <th>hour_tw_0</th>\n",
       "      <th>hour_tw_1</th>\n",
       "      <th>hour_tw_2</th>\n",
       "      <th>hour_tw_3</th>\n",
       "      <th>hour_tw_4</th>\n",
       "      <th>hour_tw_5</th>\n",
       "      <th>hour_tw_6</th>\n",
       "      <th>hour_tw_7</th>\n",
       "      <th>hour_tw_8</th>\n",
       "      <th>hour_tw_9</th>\n",
       "      <th>hour_tw_10</th>\n",
       "      <th>hour_tw_11</th>\n",
       "      <th>hour_tw_12</th>\n",
       "      <th>hour_tw_13</th>\n",
       "      <th>hour_tw_14</th>\n",
       "      <th>hour_tw_15</th>\n",
       "      <th>hour_tw_16</th>\n",
       "      <th>hour_tw_17</th>\n",
       "      <th>hour_tw_18</th>\n",
       "      <th>hour_tw_19</th>\n",
       "      <th>hour_tw_20</th>\n",
       "      <th>hour_tw_21</th>\n",
       "      <th>hour_tw_22</th>\n",
       "      <th>hour_tw_23</th>\n",
       "      <th>hour_rt_0</th>\n",
       "      <th>hour_rt_1</th>\n",
       "      <th>hour_rt_2</th>\n",
       "      <th>hour_rt_3</th>\n",
       "      <th>hour_rt_4</th>\n",
       "      <th>hour_rt_5</th>\n",
       "      <th>hour_rt_6</th>\n",
       "      <th>hour_rt_7</th>\n",
       "      <th>hour_rt_8</th>\n",
       "      <th>hour_rt_9</th>\n",
       "      <th>hour_rt_10</th>\n",
       "      <th>hour_rt_11</th>\n",
       "      <th>hour_rt_12</th>\n",
       "      <th>hour_rt_13</th>\n",
       "      <th>hour_rt_14</th>\n",
       "      <th>hour_rt_15</th>\n",
       "      <th>hour_rt_16</th>\n",
       "      <th>hour_rt_17</th>\n",
       "      <th>hour_rt_18</th>\n",
       "      <th>hour_rt_19</th>\n",
       "      <th>hour_rt_20</th>\n",
       "      <th>hour_rt_21</th>\n",
       "      <th>hour_rt_22</th>\n",
       "      <th>hour_rt_23</th>\n",
       "      <th>mst_fr_ment_tw_1</th>\n",
       "      <th>mst_fr_ment_tw_2</th>\n",
       "      <th>mst_fr_ment_tw_3</th>\n",
       "      <th>mst_fr_ment_rt_1</th>\n",
       "      <th>mst_fr_ment_rt_2</th>\n",
       "      <th>mst_fr_ment_rt_3</th>\n",
       "      <th>mst_fr_hs_tw_1</th>\n",
       "      <th>mst_fr_hs_tw_2</th>\n",
       "      <th>mst_fr_hs_tw_3</th>\n",
       "      <th>mst_fr_hs_rt_1</th>\n",
       "      <th>mst_fr_hs_rt_2</th>\n",
       "      <th>mst_fr_hs_rt_3</th>\n",
       "      <th>tw_urls_avg</th>\n",
       "      <th>tw_urls_std</th>\n",
       "      <th>rt_urls_avg</th>\n",
       "      <th>rt_urls_std</th>\n",
       "      <th>tw_hash_avg</th>\n",
       "      <th>tw_hash_std</th>\n",
       "      <th>tw_ment_avg</th>\n",
       "      <th>tw_ment_std</th>\n",
       "      <th>rt_hash_avg</th>\n",
       "      <th>rt_hash_std</th>\n",
       "      <th>rt_ment_avg</th>\n",
       "      <th>rt_ment_std</th>\n",
       "      <th>rt_time_avg</th>\n",
       "      <th>rt_time_min</th>\n",
       "      <th>rt_time_max</th>\n",
       "      <th>rt_time_std</th>\n",
       "      <th>rt_avg</th>\n",
       "      <th>tw_avg</th>\n",
       "      <th>tw_rt_ration</th>\n",
       "      <th>verified</th>\n",
       "      <th>followers</th>\n",
       "      <th>favourites</th>\n",
       "      <th>listed</th>\n",
       "      <th>statuses</th>\n",
       "      <th>followers.1</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>name_len</th>\n",
       "      <th>name_screen_sim</th>\n",
       "      <th>geo</th>\n",
       "      <th>protected</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>description_len</th>\n",
       "      <th>bckg_img</th>\n",
       "      <th>default_prof</th>\n",
       "      <th>entities</th>\n",
       "      <th>rt_self</th>\n",
       "      <th>in_degree</th>\n",
       "      <th>out_degree</th>\n",
       "      <th>w_in_degree</th>\n",
       "      <th>w_out_degree</th>\n",
       "      <th>w_degree</th>\n",
       "      <th>hs_tw_v_0</th>\n",
       "      <th>hs_tw_v_1</th>\n",
       "      <th>hs_tw_v_2</th>\n",
       "      <th>hs_tw_v_3</th>\n",
       "      <th>hs_tw_v_4</th>\n",
       "      <th>hs_tw_v_5</th>\n",
       "      <th>hs_tw_v_6</th>\n",
       "      <th>hs_tw_v_7</th>\n",
       "      <th>hs_tw_v_8</th>\n",
       "      <th>hs_tw_v_9</th>\n",
       "      <th>hs_rt_v_0</th>\n",
       "      <th>hs_rt_v_1</th>\n",
       "      <th>hs_rt_v_2</th>\n",
       "      <th>hs_rt_v_3</th>\n",
       "      <th>hs_rt_v_4</th>\n",
       "      <th>hs_rt_v_5</th>\n",
       "      <th>hs_rt_v_6</th>\n",
       "      <th>hs_rt_v_7</th>\n",
       "      <th>hs_rt_v_8</th>\n",
       "      <th>hs_rt_v_9</th>\n",
       "      <th>ment_tw_v_0</th>\n",
       "      <th>ment_tw_v_1</th>\n",
       "      <th>ment_tw_v_2</th>\n",
       "      <th>ment_tw_v_3</th>\n",
       "      <th>ment_tw_v_4</th>\n",
       "      <th>ment_tw_v_5</th>\n",
       "      <th>ment_tw_v_6</th>\n",
       "      <th>ment_tw_v_7</th>\n",
       "      <th>ment_tw_v_8</th>\n",
       "      <th>ment_tw_v_9</th>\n",
       "      <th>ment_rt_v_0</th>\n",
       "      <th>ment_rt_v_1</th>\n",
       "      <th>ment_rt_v_2</th>\n",
       "      <th>ment_rt_v_3</th>\n",
       "      <th>ment_rt_v_4</th>\n",
       "      <th>ment_rt_v_5</th>\n",
       "      <th>ment_rt_v_6</th>\n",
       "      <th>ment_rt_v_7</th>\n",
       "      <th>ment_rt_v_8</th>\n",
       "      <th>ment_rt_v_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.131317</td>\n",
       "      <td>1.396429</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>-0.748249</td>\n",
       "      <td>0.466278</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>0.361713</td>\n",
       "      <td>0.312722</td>\n",
       "      <td>0.750257</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>0.728545</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>0.594983</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>5.561772</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>2.352271</td>\n",
       "      <td>2.371916</td>\n",
       "      <td>1.348373</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>0.561934</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>10.178808</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>1.664264</td>\n",
       "      <td>2.726645</td>\n",
       "      <td>1.662660</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>0.736556</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>1.115374</td>\n",
       "      <td>1.552816</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.378457</td>\n",
       "      <td>0.677762</td>\n",
       "      <td>0.910509</td>\n",
       "      <td>2.193264</td>\n",
       "      <td>-0.179787</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.501843</td>\n",
       "      <td>-0.324570</td>\n",
       "      <td>1.137781</td>\n",
       "      <td>2.421262</td>\n",
       "      <td>2.091241</td>\n",
       "      <td>0.237495</td>\n",
       "      <td>0.497644</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.562402</td>\n",
       "      <td>0.841382</td>\n",
       "      <td>0.636334</td>\n",
       "      <td>-0.146707</td>\n",
       "      <td>0.066183</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>-0.039526</td>\n",
       "      <td>0.574022</td>\n",
       "      <td>-0.346494</td>\n",
       "      <td>1.355147</td>\n",
       "      <td>1.119481</td>\n",
       "      <td>-0.169737</td>\n",
       "      <td>-0.146886</td>\n",
       "      <td>-0.142904</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>0.262932</td>\n",
       "      <td>-0.636011</td>\n",
       "      <td>0.053677</td>\n",
       "      <td>0.536171</td>\n",
       "      <td>0.262932</td>\n",
       "      <td>3.854206</td>\n",
       "      <td>0.471669</td>\n",
       "      <td>-1.754548</td>\n",
       "      <td>1.602810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>-0.768934</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>2.243765</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.035506</td>\n",
       "      <td>0.090954</td>\n",
       "      <td>-0.027948</td>\n",
       "      <td>-0.228505</td>\n",
       "      <td>-0.032036</td>\n",
       "      <td>-1.020578</td>\n",
       "      <td>0.164965</td>\n",
       "      <td>-0.303133</td>\n",
       "      <td>-0.513429</td>\n",
       "      <td>1.146065</td>\n",
       "      <td>0.418321</td>\n",
       "      <td>3.143337</td>\n",
       "      <td>-1.040450</td>\n",
       "      <td>0.539162</td>\n",
       "      <td>-2.144940</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.816602</td>\n",
       "      <td>-0.837654</td>\n",
       "      <td>0.692779</td>\n",
       "      <td>-0.773825</td>\n",
       "      <td>0.927183</td>\n",
       "      <td>0.546767</td>\n",
       "      <td>-0.530920</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>-0.893175</td>\n",
       "      <td>0.135816</td>\n",
       "      <td>-0.360947</td>\n",
       "      <td>-0.356595</td>\n",
       "      <td>-1.335503</td>\n",
       "      <td>1.849421</td>\n",
       "      <td>1.772424</td>\n",
       "      <td>0.192387</td>\n",
       "      <td>-4.195685</td>\n",
       "      <td>-0.766355</td>\n",
       "      <td>0.340404</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>1.221135</td>\n",
       "      <td>-0.858276</td>\n",
       "      <td>0.825696</td>\n",
       "      <td>-0.069761</td>\n",
       "      <td>-0.769992</td>\n",
       "      <td>-0.277606</td>\n",
       "      <td>-0.484911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.244816</td>\n",
       "      <td>-0.486174</td>\n",
       "      <td>0.753934</td>\n",
       "      <td>-0.201759</td>\n",
       "      <td>0.284622</td>\n",
       "      <td>-0.013497</td>\n",
       "      <td>-0.433847</td>\n",
       "      <td>0.542558</td>\n",
       "      <td>-0.263438</td>\n",
       "      <td>1.034203</td>\n",
       "      <td>0.015853</td>\n",
       "      <td>0.219649</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.258144</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>1.604461</td>\n",
       "      <td>3.534555</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>0.426950</td>\n",
       "      <td>0.428578</td>\n",
       "      <td>0.597272</td>\n",
       "      <td>0.245112</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>0.773264</td>\n",
       "      <td>0.870949</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>0.766434</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>0.098052</td>\n",
       "      <td>1.243091</td>\n",
       "      <td>0.126853</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>3.127862</td>\n",
       "      <td>3.531547</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>2.908361</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>0.663891</td>\n",
       "      <td>0.118151</td>\n",
       "      <td>0.249383</td>\n",
       "      <td>0.415863</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>1.011120</td>\n",
       "      <td>1.157159</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>1.047028</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>1.670104</td>\n",
       "      <td>0.291030</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>0.130067</td>\n",
       "      <td>2.491469</td>\n",
       "      <td>1.538238</td>\n",
       "      <td>-0.378457</td>\n",
       "      <td>1.068948</td>\n",
       "      <td>0.952742</td>\n",
       "      <td>-0.016780</td>\n",
       "      <td>0.516923</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.501843</td>\n",
       "      <td>-0.482117</td>\n",
       "      <td>1.910032</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>0.020053</td>\n",
       "      <td>0.341347</td>\n",
       "      <td>0.018132</td>\n",
       "      <td>-0.038700</td>\n",
       "      <td>0.441602</td>\n",
       "      <td>0.340980</td>\n",
       "      <td>-0.166084</td>\n",
       "      <td>-0.100258</td>\n",
       "      <td>-0.278156</td>\n",
       "      <td>-0.288732</td>\n",
       "      <td>-0.524762</td>\n",
       "      <td>-0.426560</td>\n",
       "      <td>-0.049721</td>\n",
       "      <td>-0.445877</td>\n",
       "      <td>-0.028518</td>\n",
       "      <td>-0.085873</td>\n",
       "      <td>-0.125837</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.062997</td>\n",
       "      <td>-0.592291</td>\n",
       "      <td>-0.104429</td>\n",
       "      <td>-0.587068</td>\n",
       "      <td>-0.062997</td>\n",
       "      <td>-0.385118</td>\n",
       "      <td>-0.487843</td>\n",
       "      <td>-0.314915</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>-2.249190</td>\n",
       "      <td>-1.684902</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>0.164225</td>\n",
       "      <td>-0.028163</td>\n",
       "      <td>-0.204619</td>\n",
       "      <td>-0.031821</td>\n",
       "      <td>1.314177</td>\n",
       "      <td>1.621587</td>\n",
       "      <td>-0.807647</td>\n",
       "      <td>1.451248</td>\n",
       "      <td>-0.660210</td>\n",
       "      <td>1.600296</td>\n",
       "      <td>1.064972</td>\n",
       "      <td>-0.704685</td>\n",
       "      <td>-0.413916</td>\n",
       "      <td>-1.274846</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.816602</td>\n",
       "      <td>-0.837654</td>\n",
       "      <td>0.692779</td>\n",
       "      <td>-0.773825</td>\n",
       "      <td>0.927183</td>\n",
       "      <td>0.546767</td>\n",
       "      <td>-0.530920</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>-2.860970</td>\n",
       "      <td>1.520002</td>\n",
       "      <td>2.554650</td>\n",
       "      <td>1.722672</td>\n",
       "      <td>-1.462427</td>\n",
       "      <td>1.361398</td>\n",
       "      <td>0.575711</td>\n",
       "      <td>0.603942</td>\n",
       "      <td>-1.186472</td>\n",
       "      <td>-0.763926</td>\n",
       "      <td>0.340404</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>1.221135</td>\n",
       "      <td>-0.858276</td>\n",
       "      <td>0.825696</td>\n",
       "      <td>-0.069761</td>\n",
       "      <td>-0.769992</td>\n",
       "      <td>-0.277606</td>\n",
       "      <td>-0.484911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.776668</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>3.331348</td>\n",
       "      <td>1.437712</td>\n",
       "      <td>-0.696316</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>-0.745153</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>2.963367</td>\n",
       "      <td>1.484767</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>4.297942</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>3.655987</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>4.339466</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>3.930101</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>-0.595482</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>0.783619</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>-0.595259</td>\n",
       "      <td>-0.590192</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>2.754944</td>\n",
       "      <td>-0.133677</td>\n",
       "      <td>0.322746</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>2.256595</td>\n",
       "      <td>1.548150</td>\n",
       "      <td>-0.642387</td>\n",
       "      <td>-0.639803</td>\n",
       "      <td>-0.357959</td>\n",
       "      <td>-0.372066</td>\n",
       "      <td>0.944890</td>\n",
       "      <td>1.444875</td>\n",
       "      <td>-0.256612</td>\n",
       "      <td>-0.319345</td>\n",
       "      <td>1.395214</td>\n",
       "      <td>1.030740</td>\n",
       "      <td>0.547546</td>\n",
       "      <td>1.224387</td>\n",
       "      <td>-0.339200</td>\n",
       "      <td>-0.177393</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.057186</td>\n",
       "      <td>-0.093927</td>\n",
       "      <td>-0.102501</td>\n",
       "      <td>-0.212169</td>\n",
       "      <td>-0.057186</td>\n",
       "      <td>-0.248947</td>\n",
       "      <td>1.071364</td>\n",
       "      <td>0.990353</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>-0.840775</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>-0.028163</td>\n",
       "      <td>-0.419592</td>\n",
       "      <td>-0.035699</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>-0.478683</td>\n",
       "      <td>-0.359195</td>\n",
       "      <td>-0.720306</td>\n",
       "      <td>-0.461264</td>\n",
       "      <td>-0.287817</td>\n",
       "      <td>-0.588169</td>\n",
       "      <td>-0.164521</td>\n",
       "      <td>0.481054</td>\n",
       "      <td>0.595168</td>\n",
       "      <td>1.267491</td>\n",
       "      <td>0.179343</td>\n",
       "      <td>-0.991301</td>\n",
       "      <td>-1.377781</td>\n",
       "      <td>0.451393</td>\n",
       "      <td>0.456394</td>\n",
       "      <td>-0.368657</td>\n",
       "      <td>-1.405098</td>\n",
       "      <td>0.968096</td>\n",
       "      <td>1.481373</td>\n",
       "      <td>0.143023</td>\n",
       "      <td>-0.209374</td>\n",
       "      <td>-0.259983</td>\n",
       "      <td>-0.310613</td>\n",
       "      <td>0.365894</td>\n",
       "      <td>-0.321383</td>\n",
       "      <td>-0.207553</td>\n",
       "      <td>0.279414</td>\n",
       "      <td>0.287076</td>\n",
       "      <td>0.199721</td>\n",
       "      <td>0.889230</td>\n",
       "      <td>2.418624</td>\n",
       "      <td>-1.927980</td>\n",
       "      <td>0.585707</td>\n",
       "      <td>-0.624025</td>\n",
       "      <td>0.765078</td>\n",
       "      <td>-0.980186</td>\n",
       "      <td>0.586810</td>\n",
       "      <td>-2.248494</td>\n",
       "      <td>-1.570992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040519</td>\n",
       "      <td>0.469609</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>-0.748249</td>\n",
       "      <td>0.350018</td>\n",
       "      <td>0.385182</td>\n",
       "      <td>0.251026</td>\n",
       "      <td>3.300598</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>1.093414</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>1.114170</td>\n",
       "      <td>1.102461</td>\n",
       "      <td>1.080489</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>4.940725</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>1.249901</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>1.161041</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>1.222387</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>9.316448</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>2.380715</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>2.262937</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>7.631907</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>1.858983</td>\n",
       "      <td>1.790716</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>0.627256</td>\n",
       "      <td>0.066156</td>\n",
       "      <td>0.775386</td>\n",
       "      <td>2.249118</td>\n",
       "      <td>-0.335372</td>\n",
       "      <td>1.703084</td>\n",
       "      <td>1.686913</td>\n",
       "      <td>1.734372</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>1.339168</td>\n",
       "      <td>1.163505</td>\n",
       "      <td>-0.058123</td>\n",
       "      <td>0.132134</td>\n",
       "      <td>1.952519</td>\n",
       "      <td>1.474037</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-1.074249</td>\n",
       "      <td>-0.312452</td>\n",
       "      <td>-1.311497</td>\n",
       "      <td>-1.234988</td>\n",
       "      <td>-0.367444</td>\n",
       "      <td>-0.055366</td>\n",
       "      <td>0.482336</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.052268</td>\n",
       "      <td>-0.615962</td>\n",
       "      <td>-0.104429</td>\n",
       "      <td>-0.570583</td>\n",
       "      <td>-0.052268</td>\n",
       "      <td>-0.188245</td>\n",
       "      <td>-1.327417</td>\n",
       "      <td>-0.506866</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>0.344596</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.027418</td>\n",
       "      <td>-0.202131</td>\n",
       "      <td>-0.023848</td>\n",
       "      <td>-0.359878</td>\n",
       "      <td>-0.030313</td>\n",
       "      <td>-1.580022</td>\n",
       "      <td>-1.399357</td>\n",
       "      <td>2.967140</td>\n",
       "      <td>1.941894</td>\n",
       "      <td>1.549330</td>\n",
       "      <td>-0.456462</td>\n",
       "      <td>2.107056</td>\n",
       "      <td>-0.038365</td>\n",
       "      <td>-0.994057</td>\n",
       "      <td>0.956052</td>\n",
       "      <td>-0.187652</td>\n",
       "      <td>-0.173377</td>\n",
       "      <td>-0.766297</td>\n",
       "      <td>-1.163422</td>\n",
       "      <td>0.845101</td>\n",
       "      <td>-0.594682</td>\n",
       "      <td>-1.848032</td>\n",
       "      <td>-0.759520</td>\n",
       "      <td>-1.339933</td>\n",
       "      <td>0.082435</td>\n",
       "      <td>1.172375</td>\n",
       "      <td>-3.153759</td>\n",
       "      <td>0.074357</td>\n",
       "      <td>-0.874539</td>\n",
       "      <td>0.622143</td>\n",
       "      <td>-2.765501</td>\n",
       "      <td>0.222963</td>\n",
       "      <td>1.065027</td>\n",
       "      <td>0.281387</td>\n",
       "      <td>2.828504</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.274002</td>\n",
       "      <td>1.700984</td>\n",
       "      <td>0.370149</td>\n",
       "      <td>-0.452982</td>\n",
       "      <td>-0.307098</td>\n",
       "      <td>-0.281814</td>\n",
       "      <td>-0.127353</td>\n",
       "      <td>-0.169141</td>\n",
       "      <td>1.546938</td>\n",
       "      <td>0.448557</td>\n",
       "      <td>-0.348421</td>\n",
       "      <td>-0.193725</td>\n",
       "      <td>-0.178784</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>0.272932</td>\n",
       "      <td>-0.127817</td>\n",
       "      <td>-0.278668</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>0.055158</td>\n",
       "      <td>0.749989</td>\n",
       "      <td>1.334024</td>\n",
       "      <td>-0.055610</td>\n",
       "      <td>0.497367</td>\n",
       "      <td>-0.037792</td>\n",
       "      <td>0.145978</td>\n",
       "      <td>-0.221098</td>\n",
       "      <td>-0.014096</td>\n",
       "      <td>-0.016135</td>\n",
       "      <td>-0.048080</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>0.309690</td>\n",
       "      <td>-0.078832</td>\n",
       "      <td>-0.236451</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>0.100980</td>\n",
       "      <td>0.776487</td>\n",
       "      <td>1.366550</td>\n",
       "      <td>0.015297</td>\n",
       "      <td>0.572731</td>\n",
       "      <td>0.032227</td>\n",
       "      <td>0.193530</td>\n",
       "      <td>-0.154798</td>\n",
       "      <td>0.043797</td>\n",
       "      <td>0.032364</td>\n",
       "      <td>0.013291</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>-0.595482</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.378457</td>\n",
       "      <td>0.181001</td>\n",
       "      <td>1.342464</td>\n",
       "      <td>-0.595259</td>\n",
       "      <td>-0.590192</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>-0.501843</td>\n",
       "      <td>-0.324570</td>\n",
       "      <td>0.547271</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>0.528621</td>\n",
       "      <td>0.686326</td>\n",
       "      <td>-0.642387</td>\n",
       "      <td>-0.639803</td>\n",
       "      <td>-0.357959</td>\n",
       "      <td>-0.372066</td>\n",
       "      <td>0.687125</td>\n",
       "      <td>1.075553</td>\n",
       "      <td>0.049525</td>\n",
       "      <td>0.738041</td>\n",
       "      <td>1.034698</td>\n",
       "      <td>-0.379064</td>\n",
       "      <td>1.426610</td>\n",
       "      <td>1.117477</td>\n",
       "      <td>0.818797</td>\n",
       "      <td>-0.177393</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.062401</td>\n",
       "      <td>-0.614510</td>\n",
       "      <td>-0.104429</td>\n",
       "      <td>-0.557654</td>\n",
       "      <td>-0.062401</td>\n",
       "      <td>-0.357227</td>\n",
       "      <td>0.351730</td>\n",
       "      <td>-0.116965</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>-2.249190</td>\n",
       "      <td>-1.684902</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.036315</td>\n",
       "      <td>0.787030</td>\n",
       "      <td>-0.028163</td>\n",
       "      <td>0.046183</td>\n",
       "      <td>-0.027297</td>\n",
       "      <td>0.079034</td>\n",
       "      <td>-0.478683</td>\n",
       "      <td>-0.359195</td>\n",
       "      <td>-0.720306</td>\n",
       "      <td>-0.461264</td>\n",
       "      <td>-0.287817</td>\n",
       "      <td>-0.588169</td>\n",
       "      <td>-0.164521</td>\n",
       "      <td>0.481054</td>\n",
       "      <td>0.595168</td>\n",
       "      <td>0.893168</td>\n",
       "      <td>0.816602</td>\n",
       "      <td>-0.837654</td>\n",
       "      <td>0.692779</td>\n",
       "      <td>-0.773825</td>\n",
       "      <td>0.927183</td>\n",
       "      <td>0.546767</td>\n",
       "      <td>-0.530920</td>\n",
       "      <td>0.110069</td>\n",
       "      <td>-0.317943</td>\n",
       "      <td>0.143023</td>\n",
       "      <td>-0.209374</td>\n",
       "      <td>-0.259983</td>\n",
       "      <td>-0.310613</td>\n",
       "      <td>0.365894</td>\n",
       "      <td>-0.321383</td>\n",
       "      <td>-0.207553</td>\n",
       "      <td>0.279414</td>\n",
       "      <td>0.287076</td>\n",
       "      <td>0.199721</td>\n",
       "      <td>0.340404</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>1.221135</td>\n",
       "      <td>-0.858276</td>\n",
       "      <td>0.825696</td>\n",
       "      <td>-0.069761</td>\n",
       "      <td>-0.769992</td>\n",
       "      <td>-0.277606</td>\n",
       "      <td>-0.484911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2363</th>\n",
       "      <td>0.420934</td>\n",
       "      <td>0.517548</td>\n",
       "      <td>-0.650312</td>\n",
       "      <td>0.231664</td>\n",
       "      <td>-0.064907</td>\n",
       "      <td>-0.403011</td>\n",
       "      <td>-0.315765</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>1.244260</td>\n",
       "      <td>0.836612</td>\n",
       "      <td>-0.205969</td>\n",
       "      <td>0.882813</td>\n",
       "      <td>0.353374</td>\n",
       "      <td>-0.054880</td>\n",
       "      <td>0.129954</td>\n",
       "      <td>0.220939</td>\n",
       "      <td>0.030890</td>\n",
       "      <td>-0.200627</td>\n",
       "      <td>0.013781</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>0.064836</td>\n",
       "      <td>-0.210094</td>\n",
       "      <td>0.309085</td>\n",
       "      <td>0.159685</td>\n",
       "      <td>-0.013961</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.431061</td>\n",
       "      <td>0.032383</td>\n",
       "      <td>0.338870</td>\n",
       "      <td>-0.004935</td>\n",
       "      <td>-0.305276</td>\n",
       "      <td>0.576870</td>\n",
       "      <td>0.415505</td>\n",
       "      <td>0.117953</td>\n",
       "      <td>0.371273</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>0.374768</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>0.677683</td>\n",
       "      <td>0.440904</td>\n",
       "      <td>0.212200</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>0.253161</td>\n",
       "      <td>1.170414</td>\n",
       "      <td>0.743330</td>\n",
       "      <td>0.298544</td>\n",
       "      <td>0.649404</td>\n",
       "      <td>0.286517</td>\n",
       "      <td>-0.053083</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>0.130067</td>\n",
       "      <td>-0.031107</td>\n",
       "      <td>0.903478</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>1.646087</td>\n",
       "      <td>2.109451</td>\n",
       "      <td>0.805963</td>\n",
       "      <td>-1.098276</td>\n",
       "      <td>-1.276505</td>\n",
       "      <td>-1.128656</td>\n",
       "      <td>0.395472</td>\n",
       "      <td>0.911447</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>4.527877</td>\n",
       "      <td>4.729665</td>\n",
       "      <td>0.386460</td>\n",
       "      <td>0.322295</td>\n",
       "      <td>-1.406242</td>\n",
       "      <td>-1.427448</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-1.177326</td>\n",
       "      <td>-0.427663</td>\n",
       "      <td>-1.362771</td>\n",
       "      <td>-1.326675</td>\n",
       "      <td>-0.395687</td>\n",
       "      <td>1.591999</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.063005</td>\n",
       "      <td>-0.626741</td>\n",
       "      <td>-0.104429</td>\n",
       "      <td>-0.595536</td>\n",
       "      <td>-0.063005</td>\n",
       "      <td>-0.383169</td>\n",
       "      <td>-0.367904</td>\n",
       "      <td>-0.602842</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>0.254795</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.028227</td>\n",
       "      <td>-0.531851</td>\n",
       "      <td>-0.022122</td>\n",
       "      <td>-0.467364</td>\n",
       "      <td>-0.030528</td>\n",
       "      <td>1.140238</td>\n",
       "      <td>1.365056</td>\n",
       "      <td>-0.593881</td>\n",
       "      <td>1.001638</td>\n",
       "      <td>0.737407</td>\n",
       "      <td>2.664951</td>\n",
       "      <td>3.323410</td>\n",
       "      <td>-0.571578</td>\n",
       "      <td>-2.168086</td>\n",
       "      <td>-3.116967</td>\n",
       "      <td>0.082980</td>\n",
       "      <td>-0.721331</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>-1.145801</td>\n",
       "      <td>-0.632472</td>\n",
       "      <td>-0.310951</td>\n",
       "      <td>-0.749462</td>\n",
       "      <td>-0.168996</td>\n",
       "      <td>0.849216</td>\n",
       "      <td>1.269346</td>\n",
       "      <td>-2.860970</td>\n",
       "      <td>1.520002</td>\n",
       "      <td>2.554650</td>\n",
       "      <td>1.722672</td>\n",
       "      <td>-1.462427</td>\n",
       "      <td>1.361398</td>\n",
       "      <td>0.575711</td>\n",
       "      <td>0.603942</td>\n",
       "      <td>-1.186472</td>\n",
       "      <td>-0.763926</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2364</th>\n",
       "      <td>0.040519</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>0.307183</td>\n",
       "      <td>-0.165326</td>\n",
       "      <td>-0.696316</td>\n",
       "      <td>0.739564</td>\n",
       "      <td>0.583086</td>\n",
       "      <td>-0.070339</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>0.483014</td>\n",
       "      <td>0.015853</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>0.686972</td>\n",
       "      <td>0.879359</td>\n",
       "      <td>2.385499</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>2.561717</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>0.580404</td>\n",
       "      <td>0.014641</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>0.901285</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>0.409305</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>0.147531</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>0.105435</td>\n",
       "      <td>0.632560</td>\n",
       "      <td>0.659651</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>0.674099</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>0.164117</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>4.338693</td>\n",
       "      <td>4.471468</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>0.758583</td>\n",
       "      <td>0.131606</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>1.129245</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>0.536148</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>0.273293</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>0.261496</td>\n",
       "      <td>0.258355</td>\n",
       "      <td>0.283588</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>0.964143</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>0.291030</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>1.116224</td>\n",
       "      <td>-0.546279</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.378457</td>\n",
       "      <td>1.418377</td>\n",
       "      <td>0.764842</td>\n",
       "      <td>0.627256</td>\n",
       "      <td>1.320063</td>\n",
       "      <td>1.988060</td>\n",
       "      <td>0.068114</td>\n",
       "      <td>-0.226774</td>\n",
       "      <td>0.396817</td>\n",
       "      <td>0.952564</td>\n",
       "      <td>1.311059</td>\n",
       "      <td>0.517062</td>\n",
       "      <td>0.679203</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.704303</td>\n",
       "      <td>-0.058123</td>\n",
       "      <td>-0.015543</td>\n",
       "      <td>0.919054</td>\n",
       "      <td>0.876940</td>\n",
       "      <td>-0.148888</td>\n",
       "      <td>-0.203440</td>\n",
       "      <td>0.107926</td>\n",
       "      <td>-0.379873</td>\n",
       "      <td>0.984411</td>\n",
       "      <td>0.496029</td>\n",
       "      <td>-0.028518</td>\n",
       "      <td>-0.116379</td>\n",
       "      <td>-0.138249</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.056933</td>\n",
       "      <td>-0.551058</td>\n",
       "      <td>-0.100573</td>\n",
       "      <td>-0.548137</td>\n",
       "      <td>-0.056933</td>\n",
       "      <td>-0.274274</td>\n",
       "      <td>0.111852</td>\n",
       "      <td>1.307945</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>1.116882</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>3.902403</td>\n",
       "      <td>-0.034697</td>\n",
       "      <td>0.677123</td>\n",
       "      <td>-0.027732</td>\n",
       "      <td>0.010354</td>\n",
       "      <td>-0.027512</td>\n",
       "      <td>-1.580022</td>\n",
       "      <td>-1.399357</td>\n",
       "      <td>2.967140</td>\n",
       "      <td>1.941894</td>\n",
       "      <td>1.549330</td>\n",
       "      <td>-0.456462</td>\n",
       "      <td>2.107056</td>\n",
       "      <td>-0.038365</td>\n",
       "      <td>-0.994057</td>\n",
       "      <td>0.956052</td>\n",
       "      <td>-0.869367</td>\n",
       "      <td>-1.106873</td>\n",
       "      <td>1.171481</td>\n",
       "      <td>0.135689</td>\n",
       "      <td>1.208372</td>\n",
       "      <td>-1.765448</td>\n",
       "      <td>-0.680727</td>\n",
       "      <td>2.518644</td>\n",
       "      <td>-1.467869</td>\n",
       "      <td>-0.648737</td>\n",
       "      <td>-3.144598</td>\n",
       "      <td>0.185877</td>\n",
       "      <td>1.982330</td>\n",
       "      <td>0.767973</td>\n",
       "      <td>0.343544</td>\n",
       "      <td>-0.085505</td>\n",
       "      <td>0.442667</td>\n",
       "      <td>-0.534290</td>\n",
       "      <td>-2.112344</td>\n",
       "      <td>2.732012</td>\n",
       "      <td>0.340404</td>\n",
       "      <td>0.903730</td>\n",
       "      <td>0.541693</td>\n",
       "      <td>1.221135</td>\n",
       "      <td>-0.858276</td>\n",
       "      <td>0.825696</td>\n",
       "      <td>-0.069761</td>\n",
       "      <td>-0.769992</td>\n",
       "      <td>-0.277606</td>\n",
       "      <td>-0.484911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2365</th>\n",
       "      <td>1.266300</td>\n",
       "      <td>-0.920621</td>\n",
       "      <td>-0.792514</td>\n",
       "      <td>1.437712</td>\n",
       "      <td>-0.696316</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>-0.745153</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>3.606533</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>5.143494</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>5.047338</td>\n",
       "      <td>-0.366339</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>-0.411422</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>3.705574</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>15.371983</td>\n",
       "      <td>-0.201605</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>-0.233566</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>8.025511</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>2.699585</td>\n",
       "      <td>2.858496</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>3.274720</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>3.617986</td>\n",
       "      <td>-0.179787</td>\n",
       "      <td>-0.543541</td>\n",
       "      <td>0.510671</td>\n",
       "      <td>-1.276505</td>\n",
       "      <td>-1.128656</td>\n",
       "      <td>-0.516134</td>\n",
       "      <td>-0.572468</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>0.678650</td>\n",
       "      <td>0.562402</td>\n",
       "      <td>0.841382</td>\n",
       "      <td>0.636334</td>\n",
       "      <td>-0.734490</td>\n",
       "      <td>-0.847151</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>-0.203440</td>\n",
       "      <td>-1.171274</td>\n",
       "      <td>-0.420899</td>\n",
       "      <td>-1.359761</td>\n",
       "      <td>-1.321292</td>\n",
       "      <td>-0.367444</td>\n",
       "      <td>-0.146886</td>\n",
       "      <td>-0.001721</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.049504</td>\n",
       "      <td>-0.502408</td>\n",
       "      <td>-0.073579</td>\n",
       "      <td>-0.466892</td>\n",
       "      <td>-0.049504</td>\n",
       "      <td>-0.211419</td>\n",
       "      <td>-0.967600</td>\n",
       "      <td>-1.414271</td>\n",
       "      <td>1.602810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>0.182954</td>\n",
       "      <td>0.431477</td>\n",
       "      <td>0.749070</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.019330</td>\n",
       "      <td>-0.531851</td>\n",
       "      <td>-0.022337</td>\n",
       "      <td>-0.455421</td>\n",
       "      <td>-0.030528</td>\n",
       "      <td>2.344255</td>\n",
       "      <td>-0.900076</td>\n",
       "      <td>-0.191358</td>\n",
       "      <td>-1.102807</td>\n",
       "      <td>-0.091543</td>\n",
       "      <td>0.388300</td>\n",
       "      <td>-0.333088</td>\n",
       "      <td>-1.018076</td>\n",
       "      <td>0.018001</td>\n",
       "      <td>0.386748</td>\n",
       "      <td>0.982342</td>\n",
       "      <td>1.159793</td>\n",
       "      <td>-1.593922</td>\n",
       "      <td>1.154222</td>\n",
       "      <td>-1.122946</td>\n",
       "      <td>0.767545</td>\n",
       "      <td>1.896557</td>\n",
       "      <td>1.124013</td>\n",
       "      <td>-0.401684</td>\n",
       "      <td>-0.247797</td>\n",
       "      <td>0.291675</td>\n",
       "      <td>-0.199923</td>\n",
       "      <td>-0.364259</td>\n",
       "      <td>-0.480214</td>\n",
       "      <td>0.409697</td>\n",
       "      <td>-0.455343</td>\n",
       "      <td>-0.122241</td>\n",
       "      <td>0.133906</td>\n",
       "      <td>0.335568</td>\n",
       "      <td>0.320101</td>\n",
       "      <td>0.393819</td>\n",
       "      <td>-0.307904</td>\n",
       "      <td>-0.594488</td>\n",
       "      <td>-0.669309</td>\n",
       "      <td>0.730428</td>\n",
       "      <td>-0.267469</td>\n",
       "      <td>-0.211902</td>\n",
       "      <td>0.229083</td>\n",
       "      <td>-0.068271</td>\n",
       "      <td>0.587567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2366</th>\n",
       "      <td>-0.776668</td>\n",
       "      <td>0.237904</td>\n",
       "      <td>1.956727</td>\n",
       "      <td>-0.019596</td>\n",
       "      <td>0.175629</td>\n",
       "      <td>-0.677962</td>\n",
       "      <td>0.084996</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>-0.713969</td>\n",
       "      <td>-0.619365</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>-0.372495</td>\n",
       "      <td>0.596930</td>\n",
       "      <td>2.276361</td>\n",
       "      <td>0.559207</td>\n",
       "      <td>0.623879</td>\n",
       "      <td>-0.356795</td>\n",
       "      <td>0.596883</td>\n",
       "      <td>0.834660</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>-0.440325</td>\n",
       "      <td>-0.398096</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>2.219940</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>-0.244488</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>2.654388</td>\n",
       "      <td>0.974308</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>0.927419</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>1.337246</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>2.891531</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>3.676722</td>\n",
       "      <td>1.514260</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>1.334778</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>-0.390113</td>\n",
       "      <td>-0.355508</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>-0.214046</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>1.274847</td>\n",
       "      <td>1.423859</td>\n",
       "      <td>-0.502808</td>\n",
       "      <td>-0.993747</td>\n",
       "      <td>-0.984585</td>\n",
       "      <td>-0.844317</td>\n",
       "      <td>0.500783</td>\n",
       "      <td>1.219523</td>\n",
       "      <td>0.238414</td>\n",
       "      <td>-1.098276</td>\n",
       "      <td>-1.276505</td>\n",
       "      <td>-1.128656</td>\n",
       "      <td>2.421262</td>\n",
       "      <td>2.091241</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>1.339168</td>\n",
       "      <td>1.413353</td>\n",
       "      <td>-0.158068</td>\n",
       "      <td>-0.080966</td>\n",
       "      <td>-1.406242</td>\n",
       "      <td>-1.427448</td>\n",
       "      <td>-0.536694</td>\n",
       "      <td>-0.599163</td>\n",
       "      <td>-1.177326</td>\n",
       "      <td>-0.427663</td>\n",
       "      <td>-1.362771</td>\n",
       "      <td>-1.326675</td>\n",
       "      <td>-0.395687</td>\n",
       "      <td>0.005648</td>\n",
       "      <td>-0.163073</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>-0.008298</td>\n",
       "      <td>2.645066</td>\n",
       "      <td>-0.292247</td>\n",
       "      <td>-0.006700</td>\n",
       "      <td>0.232468</td>\n",
       "      <td>-0.367904</td>\n",
       "      <td>-0.417746</td>\n",
       "      <td>1.602810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>0.182954</td>\n",
       "      <td>-2.317619</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>2.243765</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.034697</td>\n",
       "      <td>-0.458580</td>\n",
       "      <td>-0.027516</td>\n",
       "      <td>-0.443478</td>\n",
       "      <td>-0.035483</td>\n",
       "      <td>-1.934675</td>\n",
       "      <td>1.818096</td>\n",
       "      <td>1.256643</td>\n",
       "      <td>0.534822</td>\n",
       "      <td>2.993217</td>\n",
       "      <td>-0.618761</td>\n",
       "      <td>-1.192867</td>\n",
       "      <td>2.241272</td>\n",
       "      <td>-1.691256</td>\n",
       "      <td>-2.296444</td>\n",
       "      <td>0.082980</td>\n",
       "      <td>-0.721331</td>\n",
       "      <td>-0.484803</td>\n",
       "      <td>-1.145801</td>\n",
       "      <td>-0.632472</td>\n",
       "      <td>-0.310951</td>\n",
       "      <td>-0.749462</td>\n",
       "      <td>-0.168996</td>\n",
       "      <td>0.849216</td>\n",
       "      <td>1.269346</td>\n",
       "      <td>-1.255051</td>\n",
       "      <td>0.655050</td>\n",
       "      <td>0.211937</td>\n",
       "      <td>-0.906292</td>\n",
       "      <td>2.234151</td>\n",
       "      <td>-0.274406</td>\n",
       "      <td>0.563785</td>\n",
       "      <td>-2.031098</td>\n",
       "      <td>0.798303</td>\n",
       "      <td>-1.211480</td>\n",
       "      <td>0.266555</td>\n",
       "      <td>-0.299594</td>\n",
       "      <td>-0.534590</td>\n",
       "      <td>-0.591333</td>\n",
       "      <td>0.734788</td>\n",
       "      <td>-0.203802</td>\n",
       "      <td>-0.230186</td>\n",
       "      <td>0.290932</td>\n",
       "      <td>-0.079112</td>\n",
       "      <td>0.530592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>0.040519</td>\n",
       "      <td>0.469609</td>\n",
       "      <td>2.506576</td>\n",
       "      <td>-0.748249</td>\n",
       "      <td>-0.696316</td>\n",
       "      <td>0.385182</td>\n",
       "      <td>-0.745153</td>\n",
       "      <td>-0.683237</td>\n",
       "      <td>1.238333</td>\n",
       "      <td>4.157611</td>\n",
       "      <td>-0.636998</td>\n",
       "      <td>-0.594585</td>\n",
       "      <td>-0.575360</td>\n",
       "      <td>-0.637312</td>\n",
       "      <td>2.385499</td>\n",
       "      <td>-0.396039</td>\n",
       "      <td>-0.341369</td>\n",
       "      <td>-0.379251</td>\n",
       "      <td>-0.356703</td>\n",
       "      <td>2.561717</td>\n",
       "      <td>-0.370328</td>\n",
       "      <td>-0.436619</td>\n",
       "      <td>-0.456568</td>\n",
       "      <td>0.949920</td>\n",
       "      <td>1.194492</td>\n",
       "      <td>-0.333863</td>\n",
       "      <td>-0.274391</td>\n",
       "      <td>-0.229903</td>\n",
       "      <td>-0.185057</td>\n",
       "      <td>-0.189426</td>\n",
       "      <td>-0.206538</td>\n",
       "      <td>2.348119</td>\n",
       "      <td>-0.304238</td>\n",
       "      <td>1.446097</td>\n",
       "      <td>-0.403655</td>\n",
       "      <td>-0.417419</td>\n",
       "      <td>-0.427253</td>\n",
       "      <td>-0.424402</td>\n",
       "      <td>1.195188</td>\n",
       "      <td>-0.416884</td>\n",
       "      <td>-0.410225</td>\n",
       "      <td>-0.428838</td>\n",
       "      <td>-0.432100</td>\n",
       "      <td>-0.433070</td>\n",
       "      <td>-0.448203</td>\n",
       "      <td>-0.237819</td>\n",
       "      <td>-0.240063</td>\n",
       "      <td>-0.229344</td>\n",
       "      <td>-0.209185</td>\n",
       "      <td>-0.171581</td>\n",
       "      <td>-0.139658</td>\n",
       "      <td>-0.123363</td>\n",
       "      <td>-0.099588</td>\n",
       "      <td>-0.095332</td>\n",
       "      <td>-0.10324</td>\n",
       "      <td>-0.129052</td>\n",
       "      <td>-0.160847</td>\n",
       "      <td>5.615886</td>\n",
       "      <td>-0.218649</td>\n",
       "      <td>-0.233061</td>\n",
       "      <td>-0.243309</td>\n",
       "      <td>-0.243926</td>\n",
       "      <td>4.471468</td>\n",
       "      <td>-0.235228</td>\n",
       "      <td>-0.230935</td>\n",
       "      <td>-0.227481</td>\n",
       "      <td>-0.225439</td>\n",
       "      <td>-0.229070</td>\n",
       "      <td>-0.230576</td>\n",
       "      <td>-0.376853</td>\n",
       "      <td>-0.400679</td>\n",
       "      <td>1.812364</td>\n",
       "      <td>2.265686</td>\n",
       "      <td>-0.297520</td>\n",
       "      <td>-0.249225</td>\n",
       "      <td>-0.204794</td>\n",
       "      <td>-0.171377</td>\n",
       "      <td>-0.174216</td>\n",
       "      <td>-0.193251</td>\n",
       "      <td>3.968493</td>\n",
       "      <td>-0.267182</td>\n",
       "      <td>-0.308974</td>\n",
       "      <td>-0.344916</td>\n",
       "      <td>-0.351465</td>\n",
       "      <td>-0.361243</td>\n",
       "      <td>-0.356107</td>\n",
       "      <td>-0.352209</td>\n",
       "      <td>-0.356518</td>\n",
       "      <td>-0.354061</td>\n",
       "      <td>-0.366727</td>\n",
       "      <td>-0.363244</td>\n",
       "      <td>-0.377549</td>\n",
       "      <td>-0.380247</td>\n",
       "      <td>2.919492</td>\n",
       "      <td>1.796934</td>\n",
       "      <td>1.998019</td>\n",
       "      <td>0.877599</td>\n",
       "      <td>0.714784</td>\n",
       "      <td>-0.048006</td>\n",
       "      <td>0.329884</td>\n",
       "      <td>0.974670</td>\n",
       "      <td>1.258518</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>-0.052800</td>\n",
       "      <td>0.858992</td>\n",
       "      <td>0.952564</td>\n",
       "      <td>1.311059</td>\n",
       "      <td>-0.973966</td>\n",
       "      <td>-1.161941</td>\n",
       "      <td>1.999686</td>\n",
       "      <td>2.048410</td>\n",
       "      <td>0.541547</td>\n",
       "      <td>0.697503</td>\n",
       "      <td>3.743858</td>\n",
       "      <td>4.442833</td>\n",
       "      <td>0.023470</td>\n",
       "      <td>-0.088287</td>\n",
       "      <td>-0.456075</td>\n",
       "      <td>-0.406635</td>\n",
       "      <td>-0.372682</td>\n",
       "      <td>-0.301387</td>\n",
       "      <td>-0.310956</td>\n",
       "      <td>-0.116379</td>\n",
       "      <td>-0.055505</td>\n",
       "      <td>-0.183766</td>\n",
       "      <td>-0.049065</td>\n",
       "      <td>-0.197592</td>\n",
       "      <td>-0.090932</td>\n",
       "      <td>-0.433660</td>\n",
       "      <td>-0.049065</td>\n",
       "      <td>-0.113085</td>\n",
       "      <td>-0.727721</td>\n",
       "      <td>1.156710</td>\n",
       "      <td>-0.623904</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.796552</td>\n",
       "      <td>0.444604</td>\n",
       "      <td>1.045042</td>\n",
       "      <td>-2.317619</td>\n",
       "      <td>-1.334988</td>\n",
       "      <td>-0.445679</td>\n",
       "      <td>-0.256252</td>\n",
       "      <td>-0.032271</td>\n",
       "      <td>-0.531851</td>\n",
       "      <td>-0.026869</td>\n",
       "      <td>-0.312106</td>\n",
       "      <td>-0.032467</td>\n",
       "      <td>-2.212239</td>\n",
       "      <td>-0.490848</td>\n",
       "      <td>1.564761</td>\n",
       "      <td>1.829846</td>\n",
       "      <td>1.769630</td>\n",
       "      <td>-1.612474</td>\n",
       "      <td>1.527780</td>\n",
       "      <td>2.363210</td>\n",
       "      <td>-1.708498</td>\n",
       "      <td>-0.445103</td>\n",
       "      <td>-1.188104</td>\n",
       "      <td>-1.722972</td>\n",
       "      <td>1.633795</td>\n",
       "      <td>0.481317</td>\n",
       "      <td>0.136679</td>\n",
       "      <td>-1.436053</td>\n",
       "      <td>-0.955202</td>\n",
       "      <td>0.799254</td>\n",
       "      <td>-1.463128</td>\n",
       "      <td>-0.037419</td>\n",
       "      <td>0.208442</td>\n",
       "      <td>-0.241314</td>\n",
       "      <td>-0.287642</td>\n",
       "      <td>-0.387337</td>\n",
       "      <td>0.377204</td>\n",
       "      <td>-0.357503</td>\n",
       "      <td>-0.154902</td>\n",
       "      <td>0.212228</td>\n",
       "      <td>0.298041</td>\n",
       "      <td>0.228439</td>\n",
       "      <td>-2.847608</td>\n",
       "      <td>-0.406829</td>\n",
       "      <td>-2.242649</td>\n",
       "      <td>1.613714</td>\n",
       "      <td>-4.524545</td>\n",
       "      <td>-2.375662</td>\n",
       "      <td>-0.945000</td>\n",
       "      <td>-0.100657</td>\n",
       "      <td>-0.053684</td>\n",
       "      <td>5.142790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2368 rows  187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      daily_rt_tw_0  daily_rt_tw_1  daily_rt_tw_2  daily_rt_tw_3  \\\n",
       "0          0.131317       1.396429      -0.792514      -0.748249   \n",
       "1          0.244816      -0.486174       0.753934      -0.201759   \n",
       "2         -0.776668      -0.920621       3.331348       1.437712   \n",
       "3          0.040519       0.469609      -0.792514      -0.748249   \n",
       "4         -0.206538      -0.274002       1.700984       0.370149   \n",
       "...             ...            ...            ...            ...   \n",
       "2363       0.420934       0.517548      -0.650312       0.231664   \n",
       "2364       0.040519      -0.920621       0.307183      -0.165326   \n",
       "2365       1.266300      -0.920621      -0.792514       1.437712   \n",
       "2366      -0.776668       0.237904       1.956727      -0.019596   \n",
       "2367       0.040519       0.469609       2.506576      -0.748249   \n",
       "\n",
       "      daily_rt_tw_4  daily_rt_tw_5  daily_rt_tw_6  daily_rt_0  daily_rt_1  \\\n",
       "0          0.466278      -0.677962       0.361713    0.312722    0.750257   \n",
       "1          0.284622      -0.013497      -0.433847    0.542558   -0.263438   \n",
       "2         -0.696316      -0.677962      -0.745153   -0.683237   -0.713969   \n",
       "3          0.350018       0.385182       0.251026    3.300598   -0.713969   \n",
       "4         -0.452982      -0.307098      -0.281814   -0.127353   -0.169141   \n",
       "...             ...            ...            ...         ...         ...   \n",
       "2363      -0.064907      -0.403011      -0.315765   -0.683237   -0.713969   \n",
       "2364      -0.696316       0.739564       0.583086   -0.070339   -0.713969   \n",
       "2365      -0.696316      -0.677962      -0.745153   -0.683237   -0.713969   \n",
       "2366       0.175629      -0.677962       0.084996   -0.683237   -0.713969   \n",
       "2367      -0.696316       0.385182      -0.745153   -0.683237    1.238333   \n",
       "\n",
       "      daily_rt_2  daily_rt_3  daily_rt_4  daily_rt_5  daily_rt_6  daily_tw_0  \\\n",
       "0      -0.619365   -0.636998    0.728545   -0.575360    0.594983   -0.372495   \n",
       "1       1.034203    0.015853    0.219649   -0.575360   -0.258144   -0.372495   \n",
       "2       2.963367    1.484767   -0.594585   -0.575360   -0.637312   -0.372495   \n",
       "3      -0.619365   -0.636998   -0.594585   -0.575360   -0.637312   -0.372495   \n",
       "4       1.546938    0.448557   -0.348421   -0.193725   -0.178784   -0.372495   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2363   -0.619365   -0.636998   -0.594585   -0.575360   -0.637312    1.244260   \n",
       "2364    0.483014    0.015853   -0.594585    0.686972    0.879359    2.385499   \n",
       "2365   -0.619365    3.606533   -0.594585   -0.575360   -0.637312    5.143494   \n",
       "2366   -0.619365   -0.636998   -0.594585   -0.575360   -0.637312   -0.372495   \n",
       "2367    4.157611   -0.636998   -0.594585   -0.575360   -0.637312    2.385499   \n",
       "\n",
       "      daily_tw_1  daily_tw_2  daily_tw_3  daily_tw_4  daily_tw_5  daily_tw_6  \\\n",
       "0       5.561772   -0.341369   -0.379251   -0.356703   -0.356795   -0.370328   \n",
       "1      -0.396039   -0.341369   -0.379251    1.604461    3.534555   -0.370328   \n",
       "2      -0.396039   -0.341369   -0.379251   -0.356703   -0.356795   -0.370328   \n",
       "3       1.093414   -0.341369   -0.379251    1.114170    1.102461    1.080489   \n",
       "4      -0.396039   -0.341369   -0.379251   -0.356703   -0.356795   -0.370328   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2363    0.836612   -0.205969    0.882813    0.353374   -0.054880    0.129954   \n",
       "2364   -0.396039   -0.341369   -0.379251   -0.356703    2.561717   -0.370328   \n",
       "2365   -0.396039   -0.341369   -0.379251   -0.356703   -0.356795   -0.370328   \n",
       "2366    0.596930    2.276361    0.559207    0.623879   -0.356795    0.596883   \n",
       "2367   -0.396039   -0.341369   -0.379251   -0.356703    2.561717   -0.370328   \n",
       "\n",
       "      hour_rt_tw_0  hour_rt_tw_1  hour_rt_tw_2  hour_rt_tw_3  hour_rt_tw_4  \\\n",
       "0        -0.436619     -0.456568     -0.440325     -0.398096     -0.333863   \n",
       "1        -0.436619      0.426950      0.428578      0.597272      0.245112   \n",
       "2        -0.436619     -0.456568     -0.440325     -0.398096      4.297942   \n",
       "3        -0.436619     -0.456568     -0.440325     -0.398096     -0.333863   \n",
       "4         0.272932     -0.127817     -0.278668     -0.398096     -0.333863   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2363      0.220939      0.030890     -0.200627      0.013781     -0.333863   \n",
       "2364      0.580404      0.014641     -0.440325     -0.398096      0.901285   \n",
       "2365     -0.436619     -0.456568     -0.440325     -0.398096     -0.333863   \n",
       "2366      0.834660     -0.456568     -0.440325     -0.398096     -0.333863   \n",
       "2367     -0.436619     -0.456568      0.949920      1.194492     -0.333863   \n",
       "\n",
       "      hour_rt_tw_5  hour_rt_tw_6  hour_rt_tw_7  hour_rt_tw_8  hour_rt_tw_9  \\\n",
       "0        -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "1        -0.274391     -0.229903     -0.185057      0.773264      0.870949   \n",
       "2        -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "3        -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "4        -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "2363     -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "2364     -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "2365     -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "2366     -0.274391     -0.229903      2.219940     -0.189426     -0.206538   \n",
       "2367     -0.274391     -0.229903     -0.185057     -0.189426     -0.206538   \n",
       "\n",
       "      hour_rt_tw_10  hour_rt_tw_11  hour_rt_tw_12  hour_rt_tw_13  \\\n",
       "0         -0.244488      -0.304238      -0.366339       2.352271   \n",
       "1         -0.244488      -0.304238       0.766434      -0.403655   \n",
       "2         -0.244488      -0.304238      -0.366339      -0.403655   \n",
       "3          4.940725      -0.304238      -0.366339       1.249901   \n",
       "4         -0.244488      -0.304238       0.055158       0.749989   \n",
       "...             ...            ...            ...            ...   \n",
       "2363      -0.244488       0.064836      -0.210094       0.309085   \n",
       "2364      -0.244488       0.409305      -0.366339       0.147531   \n",
       "2365      -0.244488       5.047338      -0.366339      -0.403655   \n",
       "2366      -0.244488      -0.304238       2.654388       0.974308   \n",
       "2367       2.348119      -0.304238       1.446097      -0.403655   \n",
       "\n",
       "      hour_rt_tw_14  hour_rt_tw_15  hour_rt_tw_16  hour_rt_tw_17  \\\n",
       "0          2.371916       1.348373      -0.424402      -0.411422   \n",
       "1         -0.417419      -0.427253      -0.424402      -0.411422   \n",
       "2         -0.417419      -0.427253      -0.424402      -0.411422   \n",
       "3         -0.417419      -0.427253       1.161041      -0.411422   \n",
       "4          1.334024      -0.055610       0.497367      -0.037792   \n",
       "...             ...            ...            ...            ...   \n",
       "2363       0.159685      -0.013961      -0.424402       0.004081   \n",
       "2364      -0.417419       0.105435       0.632560       0.659651   \n",
       "2365      -0.417419      -0.427253      -0.424402      -0.411422   \n",
       "2366      -0.417419      -0.427253      -0.424402       0.927419   \n",
       "2367      -0.417419      -0.427253      -0.424402       1.195188   \n",
       "\n",
       "      hour_rt_tw_18  hour_rt_tw_19  hour_rt_tw_20  hour_rt_tw_21  \\\n",
       "0         -0.416884      -0.410225       0.561934      -0.432100   \n",
       "1         -0.416884       0.098052       1.243091       0.126853   \n",
       "2         -0.416884       3.655987      -0.428838      -0.432100   \n",
       "3         -0.416884      -0.410225      -0.428838      -0.432100   \n",
       "4          0.145978      -0.221098      -0.014096      -0.016135   \n",
       "...             ...            ...            ...            ...   \n",
       "2363       0.695900       0.431061       0.032383       0.338870   \n",
       "2364      -0.416884       0.674099      -0.428838       0.164117   \n",
       "2365      -0.416884      -0.410225      -0.428838      -0.432100   \n",
       "2366      -0.416884      -0.410225      -0.428838      -0.432100   \n",
       "2367      -0.416884      -0.410225      -0.428838      -0.432100   \n",
       "\n",
       "      hour_rt_tw_22  hour_rt_tw_23  hour_tw_0  hour_tw_1  hour_tw_2  \\\n",
       "0         -0.433070      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "1         -0.433070      -0.448203  -0.237819  -0.240063   3.127862   \n",
       "2         -0.433070      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "3          1.222387      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "4         -0.048080      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "...             ...            ...        ...        ...        ...   \n",
       "2363      -0.004935      -0.305276   0.576870   0.415505   0.117953   \n",
       "2364      -0.433070      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "2365       3.705574      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "2366      -0.433070      -0.448203   1.337246  -0.240063  -0.229344   \n",
       "2367      -0.433070      -0.448203  -0.237819  -0.240063  -0.229344   \n",
       "\n",
       "      hour_tw_3  hour_tw_4  hour_tw_5  hour_tw_6  hour_tw_7  hour_tw_8  \\\n",
       "0     -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "1      3.531547  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "2     -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "3     -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "4     -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2363   0.371273  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "2364  -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "2365  -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "2366  -0.209185  -0.171581  -0.139658  -0.123363   2.891531  -0.095332   \n",
       "2367  -0.209185  -0.171581  -0.139658  -0.123363  -0.099588  -0.095332   \n",
       "\n",
       "      hour_tw_9  hour_tw_10  hour_tw_11  hour_tw_12  hour_tw_13  hour_tw_14  \\\n",
       "0      -0.10324   -0.129052   -0.160847   -0.201605   10.178808   -0.233061   \n",
       "1      -0.10324   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "2      -0.10324   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "3      -0.10324    9.316448   -0.160847   -0.201605    2.380715   -0.233061   \n",
       "4      -0.10324   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2363   -0.10324   -0.129052    0.374768   -0.001002    0.677683    0.440904   \n",
       "2364   -0.10324   -0.129052   -0.160847   -0.201605   -0.218649   -0.233061   \n",
       "2365   -0.10324   -0.129052   15.371983   -0.201605   -0.218649   -0.233061   \n",
       "2366   -0.10324   -0.129052   -0.160847    3.676722    1.514260   -0.233061   \n",
       "2367   -0.10324   -0.129052   -0.160847    5.615886   -0.218649   -0.233061   \n",
       "\n",
       "      hour_tw_15  hour_tw_16  hour_tw_17  hour_tw_18  hour_tw_19  hour_tw_20  \\\n",
       "0      -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "1      -0.243309   -0.243926   -0.233566   -0.235228    2.908361   -0.227481   \n",
       "2      -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "3      -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "4      -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2363    0.212200   -0.243926    0.253161    1.170414    0.743330    0.298544   \n",
       "2364   -0.243309    4.338693    4.471468   -0.235228   -0.230935   -0.227481   \n",
       "2365   -0.243309   -0.243926   -0.233566   -0.235228   -0.230935   -0.227481   \n",
       "2366   -0.243309   -0.243926    1.334778   -0.235228   -0.230935   -0.227481   \n",
       "2367   -0.243309   -0.243926    4.471468   -0.235228   -0.230935   -0.227481   \n",
       "\n",
       "      hour_tw_21  hour_tw_22  hour_tw_23  hour_rt_0  hour_rt_1  hour_rt_2  \\\n",
       "0      -0.225439   -0.229070   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "1      -0.225439   -0.229070   -0.230576  -0.376853   0.663891   0.118151   \n",
       "2      -0.225439   -0.229070   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "3      -0.225439    2.262937   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "4      -0.225439   -0.229070   -0.230576   0.309690  -0.078832  -0.236451   \n",
       "...          ...         ...         ...        ...        ...        ...   \n",
       "2363    0.649404    0.286517   -0.053083  -0.376853  -0.400679  -0.390113   \n",
       "2364   -0.225439   -0.229070   -0.230576   0.758583   0.131606  -0.390113   \n",
       "2365   -0.225439   -0.229070   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "2366   -0.225439   -0.229070   -0.230576  -0.376853  -0.400679  -0.390113   \n",
       "2367   -0.225439   -0.229070   -0.230576  -0.376853  -0.400679   1.812364   \n",
       "\n",
       "      hour_rt_3  hour_rt_4  hour_rt_5  hour_rt_6  hour_rt_7  hour_rt_8  \\\n",
       "0     -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "1      0.249383   0.415863  -0.249225  -0.204794  -0.171377   1.011120   \n",
       "2     -0.355508   4.339466  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "3     -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "4     -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2363  -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "2364  -0.355508   1.129245  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "2365  -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "2366  -0.355508  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "2367   2.265686  -0.297520  -0.249225  -0.204794  -0.171377  -0.174216   \n",
       "\n",
       "      hour_rt_9  hour_rt_10  hour_rt_11  hour_rt_12  hour_rt_13  hour_rt_14  \\\n",
       "0     -0.193251   -0.214046   -0.267182   -0.308974    1.664264    2.726645   \n",
       "1      1.157159   -0.214046   -0.267182    1.047028   -0.344916   -0.351465   \n",
       "2     -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "3     -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "4     -0.193251   -0.214046   -0.267182    0.100980    0.776487    1.366550   \n",
       "...         ...         ...         ...         ...         ...         ...   \n",
       "2363  -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "2364  -0.193251   -0.214046    0.536148   -0.308974    0.273293   -0.351465   \n",
       "2365  -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "2366  -0.193251   -0.214046   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "2367  -0.193251    3.968493   -0.267182   -0.308974   -0.344916   -0.351465   \n",
       "\n",
       "      hour_rt_15  hour_rt_16  hour_rt_17  hour_rt_18  hour_rt_19  hour_rt_20  \\\n",
       "0       1.662660   -0.356107   -0.352209   -0.356518   -0.354061    0.736556   \n",
       "1      -0.361243   -0.356107   -0.352209   -0.356518   -0.354061    1.670104   \n",
       "2      -0.361243   -0.356107   -0.352209   -0.356518    3.930101   -0.366727   \n",
       "3      -0.361243    7.631907   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "4       0.015297    0.572731    0.032227    0.193530   -0.154798    0.043797   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "2363   -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "2364    0.261496    0.258355    0.283588   -0.356518    0.964143   -0.366727   \n",
       "2365   -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "2366   -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "2367   -0.361243   -0.356107   -0.352209   -0.356518   -0.354061   -0.366727   \n",
       "\n",
       "      hour_rt_21  hour_rt_22  hour_rt_23  mst_fr_ment_tw_1  mst_fr_ment_tw_2  \\\n",
       "0      -0.363244   -0.377549   -0.380247          1.115374          1.552816   \n",
       "1       0.291030   -0.377549   -0.380247          0.130067          2.491469   \n",
       "2      -0.363244   -0.377549   -0.380247         -0.595482         -0.546279   \n",
       "3      -0.363244   -0.377549   -0.380247          1.858983          1.790716   \n",
       "4       0.032364    0.013291   -0.380247         -0.595482         -0.546279   \n",
       "...          ...         ...         ...               ...               ...   \n",
       "2363   -0.363244   -0.377549   -0.380247          0.130067         -0.031107   \n",
       "2364    0.291030   -0.377549   -0.380247          1.116224         -0.546279   \n",
       "2365   -0.363244    8.025511   -0.380247          2.699585          2.858496   \n",
       "2366   -0.363244   -0.377549   -0.380247          1.274847          1.423859   \n",
       "2367   -0.363244   -0.377549   -0.380247          2.919492          1.796934   \n",
       "\n",
       "      mst_fr_ment_tw_3  mst_fr_ment_rt_1  mst_fr_ment_rt_2  mst_fr_ment_rt_3  \\\n",
       "0            -0.502808         -0.378457          0.677762          0.910509   \n",
       "1             1.538238         -0.378457          1.068948          0.952742   \n",
       "2            -0.502808          0.783619         -0.984585         -0.844317   \n",
       "3            -0.502808         -0.993747         -0.984585         -0.844317   \n",
       "4            -0.502808         -0.378457          0.181001          1.342464   \n",
       "...                ...               ...               ...               ...   \n",
       "2363          0.903478         -0.993747         -0.984585         -0.844317   \n",
       "2364         -0.502808         -0.378457          1.418377          0.764842   \n",
       "2365         -0.502808          3.274720         -0.984585         -0.844317   \n",
       "2366         -0.502808         -0.993747         -0.984585         -0.844317   \n",
       "2367          1.998019          0.877599          0.714784         -0.048006   \n",
       "\n",
       "      mst_fr_hs_tw_1  mst_fr_hs_tw_2  mst_fr_hs_tw_3  mst_fr_hs_rt_1  \\\n",
       "0           2.193264       -0.179787       -0.543541       -0.501843   \n",
       "1          -0.016780        0.516923       -0.543541       -0.501843   \n",
       "2          -0.595259       -0.590192       -0.543541        2.754944   \n",
       "3           0.627256        0.066156        0.775386        2.249118   \n",
       "4          -0.595259       -0.590192       -0.543541       -0.501843   \n",
       "...              ...             ...             ...             ...   \n",
       "2363        1.646087        2.109451        0.805963       -1.098276   \n",
       "2364        0.627256        1.320063        1.988060        0.068114   \n",
       "2365        3.617986       -0.179787       -0.543541        0.510671   \n",
       "2366        0.500783        1.219523        0.238414       -1.098276   \n",
       "2367        0.329884        0.974670        1.258518        0.000121   \n",
       "\n",
       "      mst_fr_hs_rt_2  mst_fr_hs_rt_3  tw_urls_avg  tw_urls_std  rt_urls_avg  \\\n",
       "0          -0.324570        1.137781     2.421262     2.091241     0.237495   \n",
       "1          -0.482117        1.910032    -0.516134    -0.572468     0.020053   \n",
       "2          -0.133677        0.322746    -0.516134    -0.572468     2.256595   \n",
       "3          -0.335372        1.703084     1.686913     1.734372    -0.973966   \n",
       "4          -0.324570        0.547271    -0.516134    -0.572468     0.528621   \n",
       "...              ...             ...          ...          ...          ...   \n",
       "2363       -1.276505       -1.128656     0.395472     0.911447    -0.973966   \n",
       "2364       -0.226774        0.396817     0.952564     1.311059     0.517062   \n",
       "2365       -1.276505       -1.128656    -0.516134    -0.572468    -0.973966   \n",
       "2366       -1.276505       -1.128656     2.421262     2.091241    -0.973966   \n",
       "2367       -0.052800        0.858992     0.952564     1.311059    -0.973966   \n",
       "\n",
       "      rt_urls_std  tw_hash_avg  tw_hash_std  tw_ment_avg  tw_ment_std  \\\n",
       "0        0.497644     0.678650     0.562402     0.841382     0.636334   \n",
       "1        0.341347     0.018132    -0.038700     0.441602     0.340980   \n",
       "2        1.548150    -0.642387    -0.639803    -0.357959    -0.372066   \n",
       "3       -1.161941     1.339168     1.163505    -0.058123     0.132134   \n",
       "4        0.686326    -0.642387    -0.639803    -0.357959    -0.372066   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2363    -1.161941     4.527877     4.729665     0.386460     0.322295   \n",
       "2364     0.679203     0.678650     0.704303    -0.058123    -0.015543   \n",
       "2365    -1.161941     0.678650     0.562402     0.841382     0.636334   \n",
       "2366    -1.161941     1.339168     1.413353    -0.158068    -0.080966   \n",
       "2367    -1.161941     1.999686     2.048410     0.541547     0.697503   \n",
       "\n",
       "      rt_hash_avg  rt_hash_std  rt_ment_avg  rt_ment_std  rt_time_avg  \\\n",
       "0       -0.146707     0.066183     0.023470    -0.039526     0.574022   \n",
       "1       -0.166084    -0.100258    -0.278156    -0.288732    -0.524762   \n",
       "2        0.944890     1.444875    -0.256612    -0.319345     1.395214   \n",
       "3        1.952519     1.474037    -0.536694    -0.599163    -1.074249   \n",
       "4        0.687125     1.075553     0.049525     0.738041     1.034698   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2363    -1.406242    -1.427448    -0.536694    -0.599163    -1.177326   \n",
       "2364     0.919054     0.876940    -0.148888    -0.203440     0.107926   \n",
       "2365    -0.734490    -0.847151     0.023470    -0.203440    -1.171274   \n",
       "2366    -1.406242    -1.427448    -0.536694    -0.599163    -1.177326   \n",
       "2367     3.743858     4.442833     0.023470    -0.088287    -0.456075   \n",
       "\n",
       "      rt_time_min  rt_time_max  rt_time_std    rt_avg    tw_avg  tw_rt_ration  \\\n",
       "0       -0.346494     1.355147     1.119481 -0.169737 -0.146886     -0.142904   \n",
       "1       -0.426560    -0.049721    -0.445877 -0.028518 -0.085873     -0.125837   \n",
       "2        1.030740     0.547546     1.224387 -0.339200 -0.177393     -0.163073   \n",
       "3       -0.312452    -1.311497    -1.234988 -0.367444 -0.055366      0.482336   \n",
       "4       -0.379064     1.426610     1.117477  0.818797 -0.177393     -0.163073   \n",
       "...           ...          ...          ...       ...       ...           ...   \n",
       "2363    -0.427663    -1.362771    -1.326675 -0.395687  1.591999     -0.163073   \n",
       "2364    -0.379873     0.984411     0.496029 -0.028518 -0.116379     -0.138249   \n",
       "2365    -0.420899    -1.359761    -1.321292 -0.367444 -0.146886     -0.001721   \n",
       "2366    -0.427663    -1.362771    -1.326675 -0.395687  0.005648     -0.163073   \n",
       "2367    -0.406635    -0.372682    -0.301387 -0.310956 -0.116379     -0.055505   \n",
       "\n",
       "      verified  followers  favourites    listed  statuses  followers.1  \\\n",
       "0    -0.183766   0.262932   -0.636011  0.053677  0.536171     0.262932   \n",
       "1    -0.183766  -0.062997   -0.592291 -0.104429 -0.587068    -0.062997   \n",
       "2    -0.183766  -0.057186   -0.093927 -0.102501 -0.212169    -0.057186   \n",
       "3    -0.183766  -0.052268   -0.615962 -0.104429 -0.570583    -0.052268   \n",
       "4    -0.183766  -0.062401   -0.614510 -0.104429 -0.557654    -0.062401   \n",
       "...        ...        ...         ...       ...       ...          ...   \n",
       "2363 -0.183766  -0.063005   -0.626741 -0.104429 -0.595536    -0.063005   \n",
       "2364 -0.183766  -0.056933   -0.551058 -0.100573 -0.548137    -0.056933   \n",
       "2365 -0.183766  -0.049504   -0.502408 -0.073579 -0.466892    -0.049504   \n",
       "2366 -0.183766  -0.006700   -0.008298  2.645066 -0.292247    -0.006700   \n",
       "2367 -0.183766  -0.049065   -0.197592 -0.090932 -0.433660    -0.049065   \n",
       "\n",
       "      friends_count  name_len  name_screen_sim       geo  protected  location  \\\n",
       "0          3.854206  0.471669        -1.754548  1.602810        0.0  0.796552   \n",
       "1         -0.385118 -0.487843        -0.314915 -0.623904        0.0  0.796552   \n",
       "2         -0.248947  1.071364         0.990353 -0.623904        0.0  0.796552   \n",
       "3         -0.188245 -1.327417        -0.506866 -0.623904        0.0  0.796552   \n",
       "4         -0.357227  0.351730        -0.116965 -0.623904        0.0  0.796552   \n",
       "...             ...       ...              ...       ...        ...       ...   \n",
       "2363      -0.383169 -0.367904        -0.602842 -0.623904        0.0  0.796552   \n",
       "2364      -0.274274  0.111852         1.307945 -0.623904        0.0  0.796552   \n",
       "2365      -0.211419 -0.967600        -1.414271  1.602810        0.0  0.796552   \n",
       "2366       0.232468 -0.367904        -0.417746  1.602810        0.0  0.796552   \n",
       "2367      -0.113085 -0.727721         1.156710 -0.623904        0.0  0.796552   \n",
       "\n",
       "      description  description_len  bckg_img  default_prof  entities  \\\n",
       "0        0.444604        -0.768934  0.431477     -1.334988  2.243765   \n",
       "1       -2.249190        -1.684902  0.431477      0.749070 -0.445679   \n",
       "2        0.444604        -0.840775  0.431477      0.749070 -0.445679   \n",
       "3        0.444604         0.344596  0.431477      0.749070 -0.445679   \n",
       "4       -2.249190        -1.684902  0.431477      0.749070 -0.445679   \n",
       "...           ...              ...       ...           ...       ...   \n",
       "2363     0.444604         0.254795  0.431477      0.749070 -0.445679   \n",
       "2364     0.444604         1.116882  0.431477      0.749070 -0.445679   \n",
       "2365     0.444604         0.182954  0.431477      0.749070 -0.445679   \n",
       "2366     0.444604         0.182954 -2.317619     -1.334988  2.243765   \n",
       "2367     0.444604         1.045042 -2.317619     -1.334988 -0.445679   \n",
       "\n",
       "       rt_self  in_degree  out_degree  w_in_degree  w_out_degree  w_degree  \\\n",
       "0    -0.256252  -0.035506    0.090954    -0.027948     -0.228505 -0.032036   \n",
       "1    -0.256252  -0.036315    0.164225    -0.028163     -0.204619 -0.031821   \n",
       "2    -0.256252  -0.036315   -0.421944    -0.028163     -0.419592 -0.035699   \n",
       "3    -0.256252  -0.027418   -0.202131    -0.023848     -0.359878 -0.030313   \n",
       "4    -0.256252  -0.036315    0.787030    -0.028163      0.046183 -0.027297   \n",
       "...        ...        ...         ...          ...           ...       ...   \n",
       "2363 -0.256252  -0.028227   -0.531851    -0.022122     -0.467364 -0.030528   \n",
       "2364  3.902403  -0.034697    0.677123    -0.027732      0.010354 -0.027512   \n",
       "2365 -0.256252  -0.019330   -0.531851    -0.022337     -0.455421 -0.030528   \n",
       "2366 -0.256252  -0.034697   -0.458580    -0.027516     -0.443478 -0.035483   \n",
       "2367 -0.256252  -0.032271   -0.531851    -0.026869     -0.312106 -0.032467   \n",
       "\n",
       "      hs_tw_v_0  hs_tw_v_1  hs_tw_v_2  hs_tw_v_3  hs_tw_v_4  hs_tw_v_5  \\\n",
       "0     -1.020578   0.164965  -0.303133  -0.513429   1.146065   0.418321   \n",
       "1      1.314177   1.621587  -0.807647   1.451248  -0.660210   1.600296   \n",
       "2      0.079034  -0.478683  -0.359195  -0.720306  -0.461264  -0.287817   \n",
       "3     -1.580022  -1.399357   2.967140   1.941894   1.549330  -0.456462   \n",
       "4      0.079034  -0.478683  -0.359195  -0.720306  -0.461264  -0.287817   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2363   1.140238   1.365056  -0.593881   1.001638   0.737407   2.664951   \n",
       "2364  -1.580022  -1.399357   2.967140   1.941894   1.549330  -0.456462   \n",
       "2365   2.344255  -0.900076  -0.191358  -1.102807  -0.091543   0.388300   \n",
       "2366  -1.934675   1.818096   1.256643   0.534822   2.993217  -0.618761   \n",
       "2367  -2.212239  -0.490848   1.564761   1.829846   1.769630  -1.612474   \n",
       "\n",
       "      hs_tw_v_6  hs_tw_v_7  hs_tw_v_8  hs_tw_v_9  hs_rt_v_0  hs_rt_v_1  \\\n",
       "0      3.143337  -1.040450   0.539162  -2.144940   0.893168   0.816602   \n",
       "1      1.064972  -0.704685  -0.413916  -1.274846   0.893168   0.816602   \n",
       "2     -0.588169  -0.164521   0.481054   0.595168   1.267491   0.179343   \n",
       "3      2.107056  -0.038365  -0.994057   0.956052  -0.187652  -0.173377   \n",
       "4     -0.588169  -0.164521   0.481054   0.595168   0.893168   0.816602   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2363   3.323410  -0.571578  -2.168086  -3.116967   0.082980  -0.721331   \n",
       "2364   2.107056  -0.038365  -0.994057   0.956052  -0.869367  -1.106873   \n",
       "2365  -0.333088  -1.018076   0.018001   0.386748   0.982342   1.159793   \n",
       "2366  -1.192867   2.241272  -1.691256  -2.296444   0.082980  -0.721331   \n",
       "2367   1.527780   2.363210  -1.708498  -0.445103  -1.188104  -1.722972   \n",
       "\n",
       "      hs_rt_v_2  hs_rt_v_3  hs_rt_v_4  hs_rt_v_5  hs_rt_v_6  hs_rt_v_7  \\\n",
       "0     -0.837654   0.692779  -0.773825   0.927183   0.546767  -0.530920   \n",
       "1     -0.837654   0.692779  -0.773825   0.927183   0.546767  -0.530920   \n",
       "2     -0.991301  -1.377781   0.451393   0.456394  -0.368657  -1.405098   \n",
       "3     -0.766297  -1.163422   0.845101  -0.594682  -1.848032  -0.759520   \n",
       "4     -0.837654   0.692779  -0.773825   0.927183   0.546767  -0.530920   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "2363  -0.484803  -1.145801  -0.632472  -0.310951  -0.749462  -0.168996   \n",
       "2364   1.171481   0.135689   1.208372  -1.765448  -0.680727   2.518644   \n",
       "2365  -1.593922   1.154222  -1.122946   0.767545   1.896557   1.124013   \n",
       "2366  -0.484803  -1.145801  -0.632472  -0.310951  -0.749462  -0.168996   \n",
       "2367   1.633795   0.481317   0.136679  -1.436053  -0.955202   0.799254   \n",
       "\n",
       "      hs_rt_v_8  hs_rt_v_9  ment_tw_v_0  ment_tw_v_1  ment_tw_v_2  \\\n",
       "0      0.110069  -0.317943    -0.893175     0.135816    -0.360947   \n",
       "1      0.110069  -0.317943    -2.860970     1.520002     2.554650   \n",
       "2      0.968096   1.481373     0.143023    -0.209374    -0.259983   \n",
       "3     -1.339933   0.082435     1.172375    -3.153759     0.074357   \n",
       "4      0.110069  -0.317943     0.143023    -0.209374    -0.259983   \n",
       "...         ...        ...          ...          ...          ...   \n",
       "2363   0.849216   1.269346    -2.860970     1.520002     2.554650   \n",
       "2364  -1.467869  -0.648737    -3.144598     0.185877     1.982330   \n",
       "2365  -0.401684  -0.247797     0.291675    -0.199923    -0.364259   \n",
       "2366   0.849216   1.269346    -1.255051     0.655050     0.211937   \n",
       "2367  -1.463128  -0.037419     0.208442    -0.241314    -0.287642   \n",
       "\n",
       "      ment_tw_v_3  ment_tw_v_4  ment_tw_v_5  ment_tw_v_6  ment_tw_v_7  \\\n",
       "0       -0.356595    -1.335503     1.849421     1.772424     0.192387   \n",
       "1        1.722672    -1.462427     1.361398     0.575711     0.603942   \n",
       "2       -0.310613     0.365894    -0.321383    -0.207553     0.279414   \n",
       "3       -0.874539     0.622143    -2.765501     0.222963     1.065027   \n",
       "4       -0.310613     0.365894    -0.321383    -0.207553     0.279414   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2363     1.722672    -1.462427     1.361398     0.575711     0.603942   \n",
       "2364     0.767973     0.343544    -0.085505     0.442667    -0.534290   \n",
       "2365    -0.480214     0.409697    -0.455343    -0.122241     0.133906   \n",
       "2366    -0.906292     2.234151    -0.274406     0.563785    -2.031098   \n",
       "2367    -0.387337     0.377204    -0.357503    -0.154902     0.212228   \n",
       "\n",
       "      ment_tw_v_8  ment_tw_v_9  ment_rt_v_0  ment_rt_v_1  ment_rt_v_2  \\\n",
       "0       -4.195685    -0.766355     0.340404     0.903730     0.541693   \n",
       "1       -1.186472    -0.763926     0.340404     0.903730     0.541693   \n",
       "2        0.287076     0.199721     0.889230     2.418624    -1.927980   \n",
       "3        0.281387     2.828504     0.266555    -0.299594    -0.534590   \n",
       "4        0.287076     0.199721     0.340404     0.903730     0.541693   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2363    -1.186472    -0.763926     0.266555    -0.299594    -0.534590   \n",
       "2364    -2.112344     2.732012     0.340404     0.903730     0.541693   \n",
       "2365     0.335568     0.320101     0.393819    -0.307904    -0.594488   \n",
       "2366     0.798303    -1.211480     0.266555    -0.299594    -0.534590   \n",
       "2367     0.298041     0.228439    -2.847608    -0.406829    -2.242649   \n",
       "\n",
       "      ment_rt_v_3  ment_rt_v_4  ment_rt_v_5  ment_rt_v_6  ment_rt_v_7  \\\n",
       "0        1.221135    -0.858276     0.825696    -0.069761    -0.769992   \n",
       "1        1.221135    -0.858276     0.825696    -0.069761    -0.769992   \n",
       "2        0.585707    -0.624025     0.765078    -0.980186     0.586810   \n",
       "3       -0.591333     0.734788    -0.203802    -0.230186     0.290932   \n",
       "4        1.221135    -0.858276     0.825696    -0.069761    -0.769992   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "2363    -0.591333     0.734788    -0.203802    -0.230186     0.290932   \n",
       "2364     1.221135    -0.858276     0.825696    -0.069761    -0.769992   \n",
       "2365    -0.669309     0.730428    -0.267469    -0.211902     0.229083   \n",
       "2366    -0.591333     0.734788    -0.203802    -0.230186     0.290932   \n",
       "2367     1.613714    -4.524545    -2.375662    -0.945000    -0.100657   \n",
       "\n",
       "      ment_rt_v_8  ment_rt_v_9  \n",
       "0       -0.277606    -0.484911  \n",
       "1       -0.277606    -0.484911  \n",
       "2       -2.248494    -1.570992  \n",
       "3       -0.079112     0.530592  \n",
       "4       -0.277606    -0.484911  \n",
       "...           ...          ...  \n",
       "2363    -0.079112     0.530592  \n",
       "2364    -0.277606    -0.484911  \n",
       "2365    -0.068271     0.587567  \n",
       "2366    -0.079112     0.530592  \n",
       "2367    -0.053684     5.142790  \n",
       "\n",
       "[2368 rows x 187 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2368, 187)\n",
      "(2368,)\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "#used for testing of reading functions\n",
    "#Read data from CSV file into dataframe with balancing by target values and one-hot of text features\n",
    "\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "X_train, y_train, X_test, y_test = read_data(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection - Parameter fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmAAAAI/CAYAAADQs2XyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAB0y0lEQVR4nO3dd3yN5//H8deVJWJnmIm9ZxBq02Ur1aKULoq2uqvV/f11aukwiypVqjVa3VupUUVi71ErttiJkXH9/rgPRZUgOXfG+/l4eMg55z7nfM7tlrxzTWOtRURERES8x8ftAkRERESyGwUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES/zc7uAKxEaGmpLlizpdhkiIiIilxUTE3PAWht2sccyVQArWbIk0dHRbpchIiIiclnGmG3/9Zi6IEVERES8TAFMRERExMsUwERERES8LFONARMREZH0lZiYSGxsLCdPnnS7lEwjMDCQ8PBw/P39U/0cBTARERE5KzY2ljx58lCyZEmMMW6Xk+FZa4mLiyM2NpZSpUql+nnqghQREZGzTp48SUhIiMJXKhljCAkJueIWQwUwEREROY/C15W5mvOlACYiIiLiZQpgIiIikqFNmzaNSpUqcf3111/T62zdupXJkydf9fPfeOONa3r/c6UqgBljWhpj1htjNhljBlzk8QLGmBnGmBXGmEXGmKqe+wM9t5cbY1YbY/7vguc97Hnd1caYt9PmI4mIiEhWYa3lww8/ZOTIkcyaNStVz0lKSrro/ZkqgBljfIERQCugMtDVGFP5gsOeA5ZZa6sDdwFDPPefAm6w1tYAIoGWxph6nte9HmgPVLfWVgEGX/vHERERkcxu69atVKpUiQcffBAfHx9+/fVX+vbtS//+/Tl58iT33nsv1apVo2bNmmdD2ccff0ynTp1o164dzZs3v+jrDhgwgLlz5xIZGcl7771HcnIy/fv3p06dOlSvXp3Ro0cDsHv3bpo0aUJkZCRVq1Zl7ty5DBgwgBMnThAZGcmdd955zZ8xNctQ1AU2WWv/BjDGfI4TnNacc0xl4E0Aa+06Y0xJY0wha+1e4LjnGH/PH+u5/QAw0Fp7yvO8fdf6YURERCTt/N+3q1mz62iavmblonl5uV2Vyx63fv16xo8fz8iRI2nWrBmDBw8mKiqKd955B4CVK1eybt06mjdvzoYNGwBYsGABK1asIDg4+KKvOXDgQAYPHsx3330HwJgxY8iXLx+LFy/m1KlTNGzYkObNm/Pll1/SokULnn/+eZKTk0lISKBx48YMHz6cZcuWpcl5SE0AKwbsOOd2LHDdBccsBzoC84wxdYESQDiw19OCFgOUBUZYaxd6nlMeaGyMeR04CTxlrV181Z9EREREsowSJUpQr169f90/b948Hn74YQAqVqxIiRIlzgawm2+++T/D18X88ssvrFixgunTpwNw5MgRNm7cSJ06dbjvvvtITEykQ4cOREZGXvsHukBqAtjF5lbaC24PBIYYY5YBK4GlQBKAtTYZiDTG5AdmGGOqWmtXed67AFAPqANMNcaUttae99rGmN5Ab4DixYun8mOJiIjItUpNS1V6yZUr10XvvyAmpOo5/8Vay7Bhw2jRosW/HpszZw7ff/89PXr0oH///tx1111X9NqXk5pB+LFAxDm3w4Fd5x5grT1qrb3XWhuJMwYsDNhywTGHgdlAy3Ne90vrWASkAKEXvrm1doy1NspaGxUWFpaazyQiIiJZVJMmTfj0008B2LBhA9u3b6dChQqpem6ePHk4duzY2dstWrTggw8+IDEx8ezrxcfHs23bNgoWLMj9999Pz549WbJkCQD+/v5nj71WqQlgi4FyxphSxpgA4A7gm3MPMMbk9zwG0AuYY609aowJ87R8YYzJCdwErPMc9xVwg+ex8kAAcODaPo6IiIhkZQ8++CDJyclUq1aNLl268PHHH5MjR45UPbd69er4+flRo0YN3nvvPXr16kXlypWpVasWVatWpU+fPiQlJTF79mwiIyOpWbMmX3zxBY8++igAvXv3pnr16mkyCN9cqinv7EHGtAbeB3yBcdba140xfQGstaOMMfWBT4BknMH5Pa21h4wx1YEJnuf5AFOtta94XjMAGIczO/I0zhiw3y9VR1RUlI2Ojr6azykiIiKpsHbtWipVquR2GZnOxc6bMSbGWht1seNTtRm3tfYH4IcL7ht1ztcLgHIXed4KoOZ/vOZpoHtq3l9EREQkK0lVABMRERHJLFauXEmPHj3Ouy9HjhwsXLjwP57hfQpgIiIikqVUq1YtzdbrSi/aC1JERETEyxTARERERLxMAUxERDKNdXuOcufYv/hx5W63SxG5JgpgIiKSKfy0ajcdR/7Jn5vjeGjyEqbHxLpdkmQis2fP5s8//7yq527dupXJkyenaT0KYCIikqGlpFje/XUDfSctoULhPMx6shkNyoTy1LTlTFyw1e3yJJNQABMREUml46eS6DMphqEzN9Kpdjif965HydBcjL07ipsqFeLFr1cz+o/NbpcpaSw+Pp42bdpQo0YNqlatyoQJE+jcufPZx2fPnk27du0A+Omnn6hVqxY1atTgxhtvvOjrbd26lVGjRvHee+8RGRnJ3Llz2b9/P7fddht16tShTp06zJ8/H4A//viDyMjIsyvhHzt2jAEDBjB37lwiIyN577330uQzahkKERHJkLYeiOf+T6L5+0A8/2tXmbsblMQYA0Cgvy8fdK/F41OW8eaP64g/nczjN5U7+7hkbj/99BNFixbl+++/B+DIkSO8+OKLxMfHkytXLqZMmUKXLl3Yv38/999/P3PmzKFUqVIcPHjwoq9XsmRJ+vbtS+7cuXnqqacA6NatG48//jiNGjVi+/bttGjRgrVr1zJ48GBGjBhBw4YNOX78OIGBgQwcOJDBgwfz3XffpdlnVAATEZEMZ86G/fSbvARfH8PE++rSoGzov47x9/VhyB01CQrwZejMjSScSuL5NpUUwtLSjwNgz8q0fc3C1aDVwEseUq1aNZ566imeeeYZ2rZtS+PGjWnZsiXffvstt99+O99//z1vv/02s2fPpkmTJpQqVQqA4ODgVJfx22+/sWbNmrO3jx49yrFjx2jYsCFPPPEEd955Jx07diQ8PPzqPudlKICJiEiGYa1l7NwtvPnjWsoXysOHd0URERz0n8f7+hgGdqxOUIAfY+dtISExmdfaV8XHRyEsMytfvjwxMTH88MMPPPvsszRv3pwuXbowYsQIgoODqVOnDnny5MFae9WBOyUlhQULFpAzZ87z7h8wYABt2rThhx9+oF69evz2229p8ZH+RQFMREQyhJOJyTz75UpmLN1J62qFGdypBkEBl/8x5eNjeLldZYICfBk5ezMnTicz6Pbq+PlqmPM1u0xLVXrZtWsXwcHBdO/endy5c/Pxxx/z/PPP07NnTz788EO6dOkCQP369XnooYfYsmXL2S7I/2oFy5MnD0ePHj17u3nz5gwfPpz+/fsDsGzZMiIjI9m8eTPVqlWjWrVqLFiwgHXr1hEREcGxY8fS9DPq6hQREdftPnKCTqMW8NWynTzVvDwjutVKVfg6wxjD0y0r0r9FBWYs3Um/yUs5lZScjhVLelq5ciV169YlMjKS119/nRdeeAFfX1/atm3Ljz/+SNu2bQEICwtjzJgxdOzYkRo1apwNZhfTrl07ZsyYcXYQ/tChQ4mOjqZ69epUrlyZUaNGAfD+++9TtWpVatSoQc6cOWnVqhXVq1fHz8+PGjVqpNkgfGOtTZMX8oaoqCgbHR3tdhkiIpKGorcepO+kJZxMTOb9LpHcVLnQNb3euHlbeOW7NTQtH8boHrUJ9PdNo0qzh7Vr11KpUiW3y8h0LnbejDEx1tqoix2vFjAREXHNZ4u20/XDv8gT6MdXDzW45vAFcF+jUrx1WzXmbNzP3eMWcfxUUhpUKpK2NAZMRES87nRSCq98t5pJf22nafkwhnatSb6c/mn2+l3qFCfQ35cnpi6n+9iFTLi3LvmC0u71JeMaP348Q4YMOe++hg0bMmLECJcqujgFMBER8aoDx0/x4KdLWLTlIH2alubpFhXxTYdZi+0ji5HT35d+k5dyx4d/MbFnXUJz50jz95GM5d577+Xee+91u4zLUhekiIh4zaqdR2g/fD7LdxxmyB2RPNuqUrqErzOaVynM2Luj2HLgOF1GL2DPkZPp9l4iV0IBTEREvOKb5bu4fdSfWGuZ3rcB7SOLeeV9m5QP45P7rmPv0VN0Gv0nOw4meOV9RS5FAUxERNJVcopl4I/reOSzpVQvlp9vHm5EtfB8Xq2hbqlgPu11HcdOJtFp1AI27Tvu1fcXuZACmIiIpJsjJxLpOWExo/7YTPd6xZnU6zrXxmHViMjP573rkZSSQpfRC1i7++jlnySSThTAREQkXWzad5xbR8xn3sYDvH5rVV7rUI0AP3d/7FQsnJepfeoT4OfDHWP+YtmOw67WI9mXApiIiKS5mWv3cuuI+Rw9mchnvetx53Ul3C7prNJhuZnapz75cvpz54d/sfDvOLdLkgvkzp071ccePnyYkSNHXvV7vf/++yQkeH9coAKYiIikGWstI2Ztotcn0ZQIDeKbfo2oU/Lie/O5KSI4iGl961Mkf07uHr+IPzbsd7skuYzk5ItvLaUAJiIi2VrC6ST6fbaUQT+v55YaRZnetwFF8+d0u6z/VChvIFN616N0aG56TVjMT6v2uF2SXGD27Nlcf/31dOvWjWrVql30mAEDBrB582YiIyPPbqw9aNAg6tSpQ/Xq1Xn55ZcBiI+Pp02bNtSoUYOqVasyZcoUhg4dyq5du7j++uu5/vrrvfa5QAuxiohIGthxMIHeE2NYv+coz7WuyP2NS2NM+q3vlVZCcufgs971uGf8Ih6avIR3OtWgQ03vLI+RGby16C3WHVyXpq9ZMbgiz9R9JtXHL1q0iFWrVlGqVKmLPj5w4EBWrVrFsmXLAPjll1/YuHEjixYtwlrLLbfcwpw5c9i/fz9Fixbl+++/B+DIkSPky5ePd999l1mzZhEaGnrNn+1KqAVMRESuyYLNcdwyfB47DyUw/t669G5SJlOErzPy5fRnUs/rqFsymMenLmPywu1ulyTnqFu37n+Gr4v55Zdf+OWXX6hZsya1atVi3bp1bNy4kWrVqvHbb7/xzDPPMHfuXPLl8+5SKBdSC5iIiFwVay2fLNjGK9+toVRoLj68K4pSobncLuuq5Mrhx/h76/DApBiem7GShNNJ9Gpc2u2yXHclLVXpJVeuK7umrLU8++yz9OnT51+PxcTE8MMPP/Dss8/SvHlzXnrppbQq84qpBewcy3Yc5qHJS9h7VFtViIhcyqmkZAZ8sZKXv1nN9RXCmPFgg0wbvs4I9PdldI8oWlcrzGvfr2XYzI1Ya90uSy4jT548HDt27OztFi1aMG7cOI4fdxbb3blzJ/v27WPXrl0EBQXRvXt3nnrqKZYsWXLR53uLWsDOsWnfcX5bs5c/1u/nqebl6VG/ZLruUSYikhntO3qSvpNiWLL9MI/cUJbHbiqPTxb5Xhng58PQO2oS6L+Cd37dQPzpZJ5pWSFTdalmNyEhITRs2JCqVavSqlUrBg0axNq1a6lfvz7gLGkxadIkNm3aRP/+/fHx8cHf358PPvgAgN69e9OqVSuKFCnCrFmzvFa3yUzpPioqykZHR6fre2yLi+fFr1czZ8N+qhXLxxu3VvP6lhkiIhnVsh2H6TMxmqMnknincw1aVyvidknpIiXF8tI3q5j013buql+C/7WrkmVC5uWsXbuWSpUquV1GpnOx82aMibHWRl3seHVBXqBESC4m3FuHYV1rsufoSdqPmMf/vlnNsZOJbpcmIuKqL2Ji6Tx6Af6+Pnz5YIMsG74AfHwMr7avSu8mpflkwTae/mIFySmZp8FCMj51QV6EMYZ2NYrStEIYg39ez4QFW/lx1W5ebleFVlULqylaRLKVpOQU3vhhHePmb6FBmRCGd6tFcK4At8tKd8YYnm1VkVwBfrz32wZOJCbzXudI17dTyq7i4uK48cYb/3X/zJkzCQkJcaGia6MAdgl5A/15pX1VOtYK57kvV/Lgp0u4vkIYr7SvSkRwkNvliYiku0Pxp+n32RLmb4rj3oYleb51Jfx8s08AMcbw6E3lCArw5fUf1nLydDIj7qxFoL+v26VlOyEhIWfX+soKss//omsQGZGfb/o15MW2lVm05SA3v/cHI2dvIjE5xe3SRETSzfo9x2g/Yj6Ltxxi0O3VebldlWwVvs51f5PSvNahKr+v30fPCYuJP5XkdknpKjOND88IruZ8Zc//SVfBz9eHno1K8duTTWlWviBv/7SeNkPnsnjrQbdLExFJcz+t2s2tI+dzMjGZz/vUo1NUhNslua57vRK806kGCzbHcde4RRw5kTXHBgcGBhIXF6cQlkrWWuLi4ggMDLyi52kW5FWauXYvL329mp2HT9AlKoIBrSpSIBuMiRCRrC0lxfL+zI0MnbmRyIj8jO5Rm0J5r+wHS1b348rdPPL5UioUzsMn912X5cbDJSYmEhsby8mTWhMztQIDAwkPD8ff3/+8+y81C1IB7BoknE5iyMyNjJ27hXw5/Xm+dSU61iqmQfoikikdP5XE41OW8euavdxeO5zXOlTVWKf/MGv9PvpOjKF4cBCf9rqOggqpchEKYOls3Z6jPPflSpZsP0y90sG81qEaZQvmdrssEZFU23ognvs/iebvA/G80KYS9zQoqV8mL2PB5jh6TlhMWJ4cfNrrOsILaHKWnE/rgKWzioXzMr1vA97sWI01u47Sasgc3v1lPScTk90uTUTksuZs2M8tw+ex//gpJt5Xl3sbllL4SoX6ZUKY1Os6DsWfpvOoBWw5EO92SZKJKIClER8fQ9e6xfn9qWa0rV6Uob9vosX7c5i7cb/bpYmIXJS1lg/n/M094xdRNH9Ovu3XiAZlQ90uK1OpVbwAn/Wux8mkFDqNWsD6Pd7fU1AyJwWwNBaaOwfvdYnk017X4WMMPT5axCOfLWXfMQ1mFJGM42RiMk9MXc7rP6ylRZXCfPFAA61veJWqFM3H1D718PWBLmMWsDL2iNslSSagMWDp6GRiMqP+2MzIWZvJ4e/D0y0r0q1ucW3wLSKu2n3kBL0/iWHlziM8eXN5+t1QVl2OaWB7XALdxv7FkYRExt9bh6iSwW6XJC7TGDCXBPr78thN5fnpscZUD8/Hi1+touMHf7J6l347EhF3RG89SLth89lyIJ4P74ri4RvLKXylkeIhQUztU5+wPDno8dEi5m084HZJkoEpgHlB6bDcTOp5He93iWTnoQTaDZvHq9+tyfIrKYtIxvLZou10/fAvcufwZcaDDbi5ciG3S8pyiubPyZQ+9SkREsR9Hy/mtzV73S5JMigFMC8xxtChZjFmPtGMO+oW56N5W7jp3T/4efUet0sTkSwuMTmFF79axbNfrqR+mVC+fqgR5QrlcbusLCssTw4+712PSkXy0HdSDN8u3+V2SZIBKYB5Wb4gf964tRpfPNCAfDn96TMxhl4Took9lOB2aSKSBR04foo7xy5k4l/b6NOkNOPvqUO+IP/LP1GuSf6gACb1uo5aJQrw6OdLmRq9w+2SJIPRIHwXJSanMH7+Ft77dSMAj99cjnsblsI/m252KyJpa9XOI/SZGMOB46d4+/bqtI8s5nZJ2c6J08n0nhjN3I0H+L9bqnB3g5JulyRepEH4GZS/rw+9m5Thtyeb0rBsKG/8sI52w+YRs+2Q26WJSCb3zfJd3D7qT1KsZXrfBgpfLskZ4MvYu6NoXrkQL3+zmpGzN7ldkmQQCmAZQLH8ORl7dxSje9TmyIlEbh/1J8/NWMmRhES3SxORTCY5xfLWT+t45LOlVCuWj2/6NaJaeD63y8rWcvj5MuLOWrSPLMrbP61n8M/ryUy9T5I+/NwuQP7RokphGpUN5b1fNzD+z638snoPL7SpTPvIopomLiKXdeREIo9+vpTZ6/fT7bri/K9dFQL89Ht2RuDv68O7nSPJ6e/L8FmbSDidzIttK+l7ezamAJbB5MrhxwttK3NrrWI8N2MVj01ZxvSYWF7tUJVSobncLk9EMqhN+47T+5Noth9M4LUOVeler4TbJckFfH0Mb3asRs4AX8bN30LC6SRev7WaFufOpjQIPwNLTrFMXrSdt39ax6mkFB5sVoYHmpUhh5+v26WJSAYyc+1eHvt8GQF+PnzQvTZ1S2kF9ozMWsu7v25g2O+baB9ZlMGdamjyVRZ1qUH4agHLwHx9DD3qlaBFlUK8+t1a3v9tI98s28VrHapqw1wRwVrLyNmbGfzLeqoUzcvoHlEUy5/T7bLkMowxPNm8AkEBfrz10zpOnE5mWLea+uU6m1HkzgQK5glkWNeafHJfXZKtpdvYhTw+ZRkHjp9yuzQRcUnC6ST6fbaUQT+vp131okzr00DhK5N5oFkZ/u+WKvyyZi+9JkRz4nSy2yWJF6kLMpM5mZjMiFmbGPXHZoIC/BjQqiJdoiLw0RgCkWxjx8EEek+MYd2eowxoWZHeTUprMHcmNjV6BwO+WEFUiWA+uieKPIFaKDeruFQXpAJYJrVp33Gen7GShVsOUrtEAV6/tSoVC+d1uywRSWcLNsfx4KcxJKVYhnWtSbMKBd0uSdLAt8t38fiUZVQpmpcJ99Ulf1CA2yVJGtBCrFlQ2YK5+bx3PQZ3qsGWA/G0GTqPN39YS8JpbfAtkhVZa5nw51a6f7SQ4FwBfP1QQ4WvLKRdjaKM6l6btXuOcceYv9h/TENMsjoFsEzMGMPttcOZ+URTbq8Vzug5f3Pzu3OYuXav26WJSBo6lZTMgC9W8vI3q7m+QhhfPdSQ0mG53S5L0thNlQsx7u46bItLoMvoBew+csLtkiQdKYBlAQVyBfDW7dWZ2qc+QQG+9JwQTZ+J0frPK5IF7Dt6kq5j/mJK9A4evqEsY3pojFBW1qhcKBN71mX/sVN0GrWAbXHxbpck6URjwLKY00kpjJ33N0NnbsTXGJ5oXoG765fAT2vMiGQ6y3Ycps/EaI6eSOKdzjVoXa2I2yWJl6yMPcJd4xYS4OfDp72uo2zBPG6XJFdBY8CykQA/Hx5sVpZfH29KnVLBvPrdGtqPmM+yHYfdLk1ErsAXMbF0Hr0Af18fvniggcJXNlMtPB+f965PioXOo/9i9a4jbpckaUwBLIuKCA5i/D11GHlnLQ4cP8WtI+fz0terOHpSG3ynp8TkFE2EkGuSlJzCK9+u4clpy6ldvADf9GtE5aKa4ZwdVSich6l96hPo50PXMX+xZPsht0uSNKQuyGzg2MlE3vllA58s2EpI7hy81LYybasX0bpBVykxOYXYQyfYGhfP1gOeP3EJbI2LJ/bQCXwM3FSpEJ2iwmlSLkzdv5IqsYcS+CJmJ9NidhB76AT3NCjJ820qaYsaIfZQAt3HLmTfsVN8dHcd6pcJcbskSSWtAyYArIg9zPMzVrFy5xGalA/j1fZVKBGiDb4v5sKQtS0ugS0H4tkWF8+OQydITvnn/03uHH6UDA2iREguSoXkIuF0Ml8v20lc/GkK5slBx1rhdIoKp4xmrckFTiYm8/PqPUyLjmX+5gMANCwTSo/6JWhRpbDL1UlGsu/oSbp/tJBtcQmM6l6b6ytqCZLMQAFMzkpOsUxcsJXBv2wgMTmFh28oS+8mZQjwy36/ZScmp7Dz0Am2xMWzzdOKldqQVSIkiFKhuSgRkovQ3AH/ak1MTE7h93X7mBYdy6z1+0hOsUSVKECnqHDaVC9K7hzahjW7stayIvYIU6N38M3yXRw7mUR4gZx0qh3BbbWLEV4gyO0SJYM6GH+au8YtZP2eYwy9oyatNC4ww1MAk3/Zc+Qkr363hu9X7qZswdy81qEq9UpnvWbtJE9L1rkh60yrVuyhEySdE7JyBfhSMjSX8yckiJIhuS4ZslJr37GTzFiyk6nRO9i8P56c/r60rlaEzlHh1C0VrK7gbOLA8VN8tdS5DjbsPU6gvw+tqhahU1Q49UqFaDsxSZWjJxO5d/xilm4/xOBONehYK9ztkuQSFMDkP81at48Xv15F7KET3F47nOdaVyI4V+baAiPpwjFZVxiynK+vLWSlhrWWpTsOMy16B98u383xU0mUDAni9trh3FY7nCL5tJFyVpOYnMLs9fuZFr2D39ftIynFEhmRn85REbStUYS8Ws9LrkLC6STu/ySa+ZvieK1DVbrXK+F2SfIfFMDkkk6cTmbY7xsZM+dvcgf68VyrSnSKCs9QLTNJySnsPHyCLRcMet8Wl8COgwkXD1khuSgZ6t2QlVonTifz46rdTIuOZcHfcfgYaFQujM5R4dxcuRA5/HzdLlGuwca9x5gWE8uXS3Zy4PgpQnPnoGOtYnSqHU65QlrPSa7dycRkHvp0CTPX7eP51pW4v0lpt0uSi1AAk1TZsPcYz89YyeKth6hbMpjXb63q1R8W54asM4PeLxWySni6CM+OzQp1xmaF5c6RIUJWam2PS2B6zA6mx8Sy68hJ8gf5075GUTpFRVC1WD63y5NUOnoyke+W72Zq9A6W7TiMn4/hhooF6RQVQbMKYZrNKGkuMTmFx6Ys4/sVu3nspnI8emO5TPW9LztQAJNUS0mxTI+J5Y0f13L8ZBK9m5Tm4RvKkTMgbVpkLhaytsU5LVoXhqygAN9zxmEFUTI084as1EhOsfy5+QDTomP5afUeTielUKlIXjpHhdMhshgFMlnXcHaQkmL5a0sc06Jj+XHVbk4mplC+UG46R0XQoWYxQnPncLtEyeKSUyzPfLGC6TGx9G5SmmdbVcxy3xszMwUwuWJxx0/xxg/r+GJJLBHBOXmlfVWur5C6ac9nQtbWuAS2Hoi/4pB1puswK4as1DqSkMg3y3cyLSaWFbFHCPD14abKTmtKk3Jh+GrAtqvOrNk1fckOdhw8QZ5AP26pUZTOURFUD8+Xba9bcUdKiuX/vl3NhAXb6F6vOK/cUlWTOjIIBTC5ags2x/HCVyvZvD+eNtWK8FK7yhTKG0hScgq7Dp9ky9mB7/+sl7XjUAKJyf8OWReOx8ruISu11u4+yrToWL5atpOD8acpnDfQGU8UFUGpUK3j5i0XrtllLTQsG0LnqAhaVClMoL/G7Yl7rLW89dN6Rv2xmY61ivH2bdW1CHQGoAAm1+RUUjJj/vibYbM2EeDrQ8E8OS4aspxxWJ6QFfLPTMOwPApZaeF0Ugq/r9t7dm2xFAt1ShagU1QEbaoVIZfWFktzZ9bsmhazg6+X/bNm1+21w7mtVjgRwVqzSzIOay0jZm1i8C8bqBGej8iI/GdnfZcKyUV4gZwKZV6mACZpYuuBeN77zVnA9cyCpApZ7th79CRfLnG2rfl7fzxBAb60qVaEznUiiCpRQP8W1+jMml3TomNZv/cYOfx8aFW1MJ2jIqhXWmt2ScY2eeF2Ji/axtYDCRw/9c/etH4+hvACOc/2QpQ6Z0meYvkVztKDAphIFmWtZcn2Q0xdHMt3K3YRfzqZUqG5zrbQFM4X6HaJmUaSZ82uqRes2dUpKpx2NYpqzS7JdKy1xMWfPjsW1xkq8s86ifGnk88e6+9riCgQ9K8lfEqF5qJo/pwad3qVFMBEsoGE00n8sHIP06J3sHDLQXwMNCkfRueoCG6sVFBri/2HTfuOMS06li/OrtkV4OzfqTW7JAuz1rL/+CknkB2Id3YLiYtni+f2icQLwllw0D+9HqH/bMmmcHZpCmAi2czWA/FMj4nliyWx7PasLdYhshidosKpUlRrix07mch3K5w1u5Zud9bsur5iQTprzS4RrLXsO3bq7Az2M6Fsa5zz52RiytljA/x8KB58prUs6Jz1GXNRJG9gtu+uVwATyaaSUyzzNh1gWvQOflm9l9PJKVQpmpfOURG0jyxK/qDss7bYmTW7pkfH8oNnza5yBf9Zsyssj9bsErmclJR/wtmZrswz6zpujYvnVNI/4SyHnw8lQs4JZZ6uzVKhuSiUJ3uEMwUwEeFwwmm+XraLaTE7WLXzKAG+PtxcpRCdoyJoVDY0y3Yj/GvNrhx+tIt01uyqoTW7RNJMSoplz9GT53RpJpzdPm7bwQROnxPOAv19KBHsGWt2tkvTCWqF8madSV0KYCJyntW7jjAtOpavl+3kUEIiRfIFclutcG6vHU7JLLC22H+t2dWptrNmV1rt7CAiqZOSYtl15MR5oWxrnNN6tuPgCU4n/xPOcvr7UiIkyLNA9z/LG5UKzZXpZtwrgInIRZ1KSmbm2n1Mi97BHxv2k2KhbqlgOkdF0LpaYYICMs/aYhdbs6tY/px0itKaXSIZWXKKZdfhE2e7NM/uohIXz46Dl1lzMoMv7H3NAcwY0xIYAvgCY621Ay94vAAwDigDnATus9auMsYEAnOAHIAfMN1a+/IFz30KGASEWWsPXKoOBTCR9LPnyEm+WBLL9JhYthyIJ1eAL22rF6VTVDi1M/DaYnHHTzFDa3aJZElndl05MwHgn9azf29tlzuH3zlb2gWdt9ZZSK4AV76HXVMAM8b4AhuAm4FYYDHQ1Vq75pxjBgHHrbX/Z4ypCIyw1t5onE+by1p73BjjD8wDHrXW/uV5XgQwFqgI1FYAE3GftZbobYeYFr2D71bsJuF0MqVDc9EpKoKOtYpRKK/7a4udWbNrWswOZq511uyqEZGfzlHhtK1elHw5tWaXSFZ3Zt/hc0PZmVa0HYdOkHxOOMuTw48S53RllgzJRf0yIRTNnzNda7zWAFYf+J+1toXn9rMA1to3zznme+BNa+08z+3NQANr7d5zjgnCCWAPWGsXeu6bDrwKfA1EKYCJZCzxp5L4fuVupkfHsmirs7ZYswoF6VQ7nBsrFSLAz7vLNZxZs+vLpTvZf8xZs+vWms6+mOW1ZpeIeCQmpxB76MR5exVv8XRtxh5KIMXCyDtr0bpakXSt41IBLDUDPIoBO865HQtcd8Exy4GOwDxjTF2gBBAO7PW0oMUAZXFaxs6Er1uAndba5Rm1a0Mku8uVw4/OURF0jopgy4F4psfsYHpMLL+v20dwroCza4tVKpI33Wo4s2bXtOgdLNl+GF8fw/UVCtI5KpzrKxbUml0i8i/+vj6UCnVauy50OimF2EMJhLq89ExqAtjF0tGFzWYDgSHGmGXASmApkARgrU0GIo0x+YEZxpiqwN/A80Dzy765Mb2B3gDFixdPRbkikh5Kheaif4uKPHFzBeZs3M/06Fgm/rWVcfO3UK1YPjpFhdO+RjHyBV17919KimXhloNMi95x3ppdz7WuSIeaxSiYx/1uUBHJnAL8fCgdltvtMtKmC/KC4w2wBahurT16wWMvA/HAz8BMIMHzUDiwC6hrrd3zX7WoC1IkYzkYf5qvl+1kanQsa3cfJcDPhxZVCtOpdjgNr2JtsZ2HT/BFTCzTYs5fs6tT7XAiI/Jn2IkAIiIXc61jwPxwBuHfCOzEGYTfzVq7+pxj8gMJ1trTxpj7gcbW2ruMMWFAorX2sDEmJ/AL8Ja19rsL3mMrGgMmkqmt2nmE6TGxzFi6kyMnEimaL5Dba4dze+0Iiof89xIQZ9bsmh4Ty7xNzppdDcqE0DlKa3aJSOaWFstQtAbex1mGYpy19nVjTF8Aa+0oTyvZJ0AysAboaa09ZIypDkzwPM8HmGqtfeUir78VBTCRLOFkYjK/rd3LtOhY5mzcj7VQr3QwnWpH0Mqztpi1lpU7jzA1egffLNvFUc+aXU5g05pdIpI1aCFWEXHF7iMn+HLJTqZG72BbXAK5c/hxc+VCrN19lHV7nDW7WnrW7KqvNbtEJItRABMRV1lrWbz1EFOjd/Djyt2ULZSHTrXDaVdDa3aJSNalACYiIiLiZZcKYFpAR0RERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvCxVAcwY09IYs94Ys8kYM+AijxcwxswwxqwwxiwyxlT13B/oub3cGLPaGPN/5zxnkDFmnec5M4wx+dPsU4mIiIhkYJcNYMYYX2AE0AqoDHQ1xlS+4LDngGXW2urAXcAQz/2ngBustTWASKClMaae57Ffgaqe52wAnr3GzyIiIiKSKaSmBawusMla+7e19jTwOdD+gmMqAzMBrLXrgJLGmELWcdxzjL/nj/Uc94u1Nsnz2F9A+LV9FBEREZHMITUBrBiw45zbsZ77zrUc6AhgjKkLlMATqIwxvsaYZcA+4Fdr7cKLvMd9wI9XVLmIiIhIJpWaAGYucp+94PZAoIAnaD0MLAWSAKy1ydbaSJxAVvfM+LCzL27M855jP73omxvT2xgTbYyJ3r9/fyrKFREREcnY/FJxTCwQcc7tcGDXuQdYa48C9wIYYwywxfPn3GMOG2NmAy2BVZ5j7wbaAjdaay8MdWeeNwYYAxAVFXXRY0REREQyk9S0gC0GyhljShljAoA7gG/OPcAYk9/zGEAvYI619qgxJuzM7EZjTE7gJmCd53ZL4BngFmttQpp8GhEREZFM4LItYNbaJGNMP+BnwBcYZ61dbYzp63l8FFAJ+MQYkwysAXp6nl4EmOCZSekDTLXWfud5bDiQA/jVaTTjL2tt37T7aCIiIiIZk/mPnr8MKSoqykZHR7tdhoiIiMhlGWNirLVRF3tMK+GLiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIiIeJkCmIiIiIiXKYCJiIhI9pJ0yu0KFMBEREQkGzmwEYbWgo2/uVqGApiIiIhkD0d3wcRbIfkUBJdytRQ/V99dRERExBsSDsLEjnDiMNzzHYSUcbUctYCJiPdYC5t+g8M73K5ERLKT0wnw2R1wcDN0nQxFI92uSC1gIuIlyUnwY3+IHgc+flC9CzR8DMLKu12ZiGRlyYkw7R7YsQg6T4BSTdyuCFAAExFvOHkUpt/rtH7VexBsCsRMgGWTofIt0OiJDPEbqYhkMdbCN4/Axp+h7XtQub3bFZ2lACYi6etILHzaGfavg3ZDoPY9zv2Nn4KFH8CiD2HN11D2Jue+EvVdLVdEspBfX4Llk6HZcxB1n9vVnEdjwEQk/exaCh/eCEd2QPfp/4QvgNxhcONL8Pgq5+9dy2B8SxjXypkebq1bVYtIVjB/KPw5FOrcD02fdruaf1EAE5H0se57GN8afP3hvp+hzA0XPy4wHzR+Eh5bCS3fgsPb4NPbYHQTWP0VpCR7tWwRyQKWfQa/vghVboVWb4Exblf0LwpgIpK2rIUFI+HzOyGsIvSaCYUqX/55AUFQry88sgxuGQ6n42Ha3TDiOlj6qTOQVkTkcjb8DF8/BKWbwa2jwcfX7YouSgFMRNJOchL88BT8/CxUbAP3fA95Cl3Za/gFQK0e0G8x3D4e/ALh6wdhaE1YOAYST6RP7SKS+W1fCFPvhiLVocsk8MvhdkX/SQFMRNLGqWPOOjuLx0KDh6HzRKdV62r5+ELVjtB3LnSbBnmLOstYvF8N5r3nzKwUETlj7xqY3Mn5XnHndMiRx+2KLsnYTDTQNSoqykZHR7tdhohc6MhOmNwZ9q2FNoPTZ7aRtbDtT5j7DmyeCTnywXW94boHIFdI2r+fiGQeh7fDR82d7xM9f4ECJdyuCABjTIy1Nupij2kZChG5NruWweQuzpitO6c6y0mkB2OgZEPnz84lMO9dmDMIFoxwZlfW7wf5iqXPe4tIxhV/wNnfMTEB7v0xw4Svy1EXpIhcvfU/wvhWzsr2PX9Ov/B1oWK1nPEdDy50FlZcOBqG1IBvHoa4zd6pQUTcd+o4fNrJWW+w6xQoVMXtilJNAUxErs5fo+CzrhBWAe6f6c43voIV4dZR8MhSqH03LJ8Cw6Ngek/Ys8r79YiI9ySdhindYfdy6PRxplvEWQFMRK5MchL80B9+euacmY6F3a2pQAlo846zllj9frDhJxjVECbfATsWu1ubiKS9lBT4qi/8PQtuGQoVWrld0RVTABOR1Dt1DD7vBovGOEGn8ycQkMvtqv6RpxA0f9UJYs2egx1/wUc3wcdtYfMsra4vkhVYCz8NgFVfwE3/BzW7u13RVVEAE5HUObLT2SZo029Oa1OL1zPsAocEBUOzZ+CxVdD8dTiwESZ2gA9vgLXfOb89i0jmNHcwLBrt/BLY8FG3q7lqCmAicnm7l8PYG+HQVug2Fer0crui1MmRGxr0g8dWQNv34cRBmHInfFDfGS+WnOR2hSJyJaLHw++vQfU74OZXM+QWQ6mlACYil7b+J6fly/jAfT9BOS/NdExLfjkg6l7oFwMdPwQMzOgNw2pB9DhIPOl2hSJyOWu+ge+fgHLNof1w8MncESZzVy8i6WvhaPi8K4SWdfZ0LFzV7Yquja8fVO8MD/wJd0yGXKHw3ePOEhZ/DnOmtItIxrNlLnzRC4rVdmY8+vq7XdE1UwATkX9LSYYfn4Efn4byrZzFDfMWcbuqtOPj48zg7DUT7voawsrDLy/A+1Vh9kBIOOh2hSJyxu4VzuSf4FLOEIiMNPHnGiiAicj5Th13vtktHAX1HoIuE7PMN7x/MQZKN4O7v4Wev0Hx+jD7TWe/yV9egGN73K5QJHs7+DdMug1y5IXuXzoTbLKIVAUwY0xLY8x6Y8wmY8yAizxewBgzwxizwhizyBhT1XN/oOf2cmPMamPM/53znGBjzK/GmI2evwuk3ccSkatydJezsv3GX6D1YGj5Rsad6ZjWIupA18+c7skKrZwtjt6vDt894Uw+EBHvOrbX2WIoJQl6zMhyW41dNoAZY3yBEUAroDLQ1RhT+YLDngOWWWurA3cBQzz3nwJusNbWACKBlsaYep7HBgAzrbXlgJme2yLilj0r4cMbnd84u06Buve7XZE7ClWB28ZCv2iocQcs+QSG1oIv+8C+dW5XJ5I9nDwCn94Gx/fBndOcYQJZTGpawOoCm6y1f1trTwOfA+0vOKYyTojCWrsOKGmMKWQdZ0a1+nv+nFkJsT0wwfP1BKDDVX8KEbk2G36BcS2dr+/7Cco3d7eejCCkjLPC9qPL4bq+sPYbGHkdfH4n7IxxuzqRrCvxpPP/bN9aZwhEeJTbFaWL1ASwYsCOc27Heu4713KgI4Axpi5QAgj33PY1xiwD9gG/WmsXep5TyFq7G8Dzd8Gr/AziTcf2QuIJt6uQtLToQ/isCwSXdvZ0LFzN7YoylnzFnK7Yx1ZBk6dh61xnQddPOjgzs7S6vkjaSUmGL3s5/886jIKymXDZm1RKTQC72CpnF37HGQgU8ASth4GlQBKAtTbZWhuJE8jqnhkfllrGmN7GmGhjTPT+/fuv5KmSlnbGOL+RvFMehkTC4rHORqiSeaUkw0/Pwg9PQbkWnpmORd2uKuPKFQI3PO8EsZv+B3tXwYS2MK4FbPhZQUzkWlnrrPO19ltoORCqd3K7onSVmgAWC0Scczsc2HXuAdbao9baez1B6y4gDNhywTGHgdmAp5+DvcaYIgCev/dd7M2ttWOstVHW2qiwsLBUlCtpxlrYMgc+ae/8xr91rrPtQ4GS8P2TMKKOs5p4SrLblcqVOnUcpnSHv0bCdQ/AHZ86q8bL5QXmhUaPO/tNth7sTFyY3BlGNXL2ptP/B5GrM+t1iPkYGj8J9R5wu5p0l5oAthgoZ4wpZYwJAO4Avjn3AGNMfs9jAL2AOdbao8aYMGNMfs8xOYGbgDOjWL8B7vZ8fTfw9TV9Ekk71jqrn3/UHCa0g71r4OZX4PHVzt/3/QTdpkGOPM5q4qMawbrv1QKQWRzdDR+3hg0/QatB0Gpg9pnpmJb8czoTFR5ZCh0+gKRTMP0+GB7lDNxXC7FI6i0cDXMGQa274IYX3a7GK4xNxQ9NY0xr4H3AFxhnrX3dGNMXwFo7yhhTH/gESAbWAD2ttYeMMdVxBtj74oS9qdbaVzyvGQJMBYoD24FO1tpLrn4YFRVlo6Ojr+qDSiqkJMPqGTDvPad7JV9xaPQoRHYH/8CLHJ8Ca75yfmuJ2+SsUHzjS866SpIx7VnltNacOAydxkP5Fm5XlHWkJMO672DOYNizAvIWgwYPOz9Qsuo6aiJpYeV0Z5X7im2g0wRnx4oswhgTY6296CyCVAWwjEIBLJ0knYLln8P8950lCEIrQOMnoOptqdvuITkJlk+G2W/B0Vgo1dQJYll05kqmtfFXmHaP03LZbSoUqe52RVmTtbBpJsx9B7b/CUEhTndKnfshZ363qxPJWDb/Dp92hoi60P0Lp2U5C1EAk4s7HQ8xE5w98I7tgiKRTt97xbZXt8lp4kmIGe+0ACQcgApt4IYXoNCFy8aJ1y0eCz/0d9a46jZVg+29ZdufMPdd2PSrs5J3nV5Q70HIrfGsIuyMgY/bOVsM3fN9lvwFRQFMznfisLP0wF8j4cRBKNHIafEqc4OzNcu1OnUM/hoFfw51vq7eGZoNcJY5EO9KSYZfX4IFw52ZjreP02B7N+xe7gSxNV+DXw6odbfTPZk/4vLPFcmKDmx0xhnnyAM9f4E8hd2uKF0ogInj+D4ndC0aC6ePOT+QGz8Bxetd/rlXI+Gg0625cAykJDpjYZo8nbU2dc7ITsfDF/fD+u+hbh9o+aYG27vtwEaY9z6s+Ny5Xf0OaPQYhJZzsyoR7zq6ywlfSSfhvp+dRY+zKAWw7O7wdqebccknznivKrc60+i9NQbo6G6YO9iZXuzjB3V7O++fhTZVzXCO7YHJXZzB4C3ehHp93a5IznV4h+f/5ATn/2Tl9s4vQ0VquF2ZSPpKOAjjW8ORWLjnOyga6XZF6UoBLLvav8FpgVoxBTDOvnYNH4PQsu7Uc3ALzB7o1JMjj9MFU+8B52tJO3tXO4NaTxxyuhwrtLz8c8QdF7ZKl70ZmjyVfq3SIm46nQATO8CupXDndCjd1O2K0p0CWHazaxnMexfWfAN+gVDbM94kX7jblTn2rYXfX3Om7AeFOAP/o3pefKkLuTIbf/PMdMwN3aaoRSWzOHEYFn8If30ACXFOEOv8CQQEuV2ZSNpITnR2U9n4C3T6GKp0cLsir1AAyy62/elMfd/0mzPjqu79zirnGXXGVWwM/P4K/D3bWTOp6TMQeWeWWgPGqxZ/5Mx0LFjZCV/5LtyyVTK80/HOBJnf/gflmjs7FKRmKRiRjMxa+OpBZ7miNu9CnZ5uV+Q1CmBZ2dk1hwbD9gUQFAr1H3Smuwfmc7u61Pn7D5j5CuyMhuAycP1zUKXj1S2FkR2lpMCvL3pmOjb3zHRUt26mtvgjZ0+8Gl2h/Uj9X5DM7ZcXnVnxzZ6DZs+4XY1XXSqAqakhs0pJdjYsnfvOP6tut3obavbIfN0WpZtCqd9g/Y/w+6vwRU9nptiNLzqBIi2WxsiqTifAl/c73bl17nc2sFULYuZXpyfEH4DZb0CuUGj+mtsViVyd+UOd8FXnfmj6tNvVZCj6Tp3ZJCfCiqnOdkFxG50Wo1uGQ/Uu4Bdw+ednVMZAxdbO1jirvnS2N5rcGSKuc1bVL9nI7QoznmN74bMuzpi/lgPhur4Kq1lJ06chfr8zWzJXGDR81O2KRK7Mss+c1vkqt0Krt/T96QIKYJlF4glYMtH5TeLIDihUDW4f70xfz0prO/n4QvVOzgDNpRPhj7fh4zbOIrE3vgRFa7pdYcawd40TUBPi4I7JTniVrMUY54dWwgFnMd1cYRDZze2qRFJnw8/w9UPO1nS3js5aP6fSiAJYRnfyqLONzF8jnd+GI65zBjGWuzlr/zbh6w9R9zljYBZ96MzqHNMMKt3ibG8UVsHtCt2zaaYz09E/CO79QaE0K/PxdX54JRyEr/tBzmAtKyIZ3/aFMPVuKFzNmUjil8PtijIkDcLPqOLjYOEHziryp45AmRud5RpKNMjaweu/nDwKC0Y4A80TE5wVxJsNgAIl3K7Mu6LHw/dPQsFKnpmOGWRpEUlfp47Bx21h/zq462utEyYZ1941ML6lMyGs5y/OGMZsTLMgM5MjO52QEfOxEzQqtYNGT0CxWm5XljHEH3DGvy36EGwKRN0LjZ+CPIXcrix9paTAby87XdBlb4ZO4zXTMbuJP+Bs35JwAO79SZvcS8ZzeLtzjVrrhK/s9gvyRSiAZQZxm51V65d95gSL6p2dVesLVnS7sozpyE6Y87YzLs4vhzMAveEjkLOA25WlvdMJMKO3M+s1qqcz21UzHbOnQ9ucH3DGOD/g8hd3uyIRR3wcjGsB8fvg3h+hUBW3K8oQFMAysj2rnPFNq2eAjz/U6gENHtFvDqkVtxlmvwkrpzuLzzZ8xAljOXK7XVnaOLYXPrvD2bqjxetQ78Hs2QUt/9i7Gsa3cgbl3/dztu/ikQzg1HGY0A72rYEeX0GJ+m5XlGEogGVEOxY7a3ht+BECcjvr/tR7KOt3paWXPauc7Y02/Oj8YGrSH2rfk7kHf+5b6+zpmHAAbhsLFdu4XZFkFNsWOHvqFawEd3+r7mhxT9JpZ0b2ljnOgPsKrdyuKENRAMsorHW23Zn7Dmyd63SX1XvQ2TIoK3aduWHHImdV/a1zIV+EM1C/+h2Zr8tu8+/OLCL/nND1c40BlH9b/6Ozt16pxtBtWuZeB1Ayp5QU+LIXrPoC2o+Amt3drijDuVQA0/4W3pCSAmu/gw9vcH5rPbARmr8Oj61yFltU+Eo7EXWdFoEeM5yuma8fgg/qw+qvnH+HzCDmY5h0uxMge81U+JKLq9AKbhnq/FI3o0/mub4la7AWfhrghK+b/qfwdRUyWbNAJpOc5Fyc8951po8XKAlt33cWU8zMXWMZnTHOwq2lr3e26Pn9NZh2NxSp4SzmWubGjDmOKiUFZv6fMxmjzI3Q6WMIzOt2VZKR1ezuzI787WXnF45Wb2fMa1uynrmDYdFoqN/PmTAmV0wBLD0knnR2fZ/3PhzeBmGVoONYZzuGzNYVlpkZ4yzjUaG1s33T7Ddg0m1QoqETxDLSWkqJJ5xWjDVfQ+17ofVgXSuSOg0fdRZpXjAcchWEpv3drkiyupiPnV9sq3eBm19V6L9K+g6flk4dh5jx8OdwOL4HitV29ugr3xJ81NvrGh9fiOwKVW+DJROc7Y3GtXA2+r7hRShS3d36ju+Dz7rCzhhn0+X6/fQNTVLPGOeHYPwBmPUa5ApxdpEQSQ9rvoHvHnfWI2w/Qj/broECWFpIOAiLxsBfH8DJw1CqCXQc7eyBpR+kGYdfgDPhIbKb8+81730Y3RiqdITrn4fQst6vad86mNwJju+HLhOdFjuRK+XjA+2Hw4mDzk4JQaFQ+Ra3q5KsZstc+KKX07jQeYKzZZxcNc2CvBbH9jjN/ovHQWK809XV6AmIqON2ZZIaJw7Dn8Oc4Jx0EmreCU2f8d72Pn/Phil3OeMBu33ufFMTuRan4+GT9rB7OXT/wvllUCQt7F4BH7eBPEXgvp8gKNjtijIFLUOR1g5thflDYOmnkJLodG01elwr/2ZWx/fB3Hch+iPndp1eTpDOHZZ+77nkE6cZP7S8s6ejVjSXtJJw0Fmo9chOuPd7Z/KJyLU4+Dd81AJ8A6Dnz9qD9googKWVfWudfQhXTveMK+rmDIANLu1eTZJ2Dm+HP96CZZPBLyfUfxAaPAyB+dLuPVJS4PdXnOuozA2emY5p+Poi4ISvj5pD8ilntfyQMm5XJJnVsb3OmNmTh51rKayC2xVlKgpg12pnjNNCsu478A9yZqk16Ad5i3q/Fkl/+zfArNdhzVcQmN9p3azbGwKCru11E0/AjL7O69a+xzPTUWMoJJ3s3+D84AzMC/f9ol025MqdPOJ0O8ZtdtZXDL9ojpBLUAC7GtbC1nnOqvV/z3JaKer2cfYZzBXinRrEXbuWOVOtN/0KuQs52xvVuvvqVhw/vh8+7wqx0XDzK07LmiZoSHqLjXb26Asu43RHqrVVUivxJHx6O2xf4AyTKHuT2xVlSgpgV8Ja2PCzE7xiFznr6tR/yJnWrUUxs6dtfzrbG21fAPlLwPXPQbVOTjd0auxfD592csaadRyj2WniXZt+g8ldIKKeMzDfP9DtiiSjS0l2Fq9e+y10/BCqd3a7okxLWxGl1qaZMKoRfNbFmeHYejA8tgIaPabwlZ2VaAD3/gh3Tneugxl94IOGzvZSl/sF5u8/YOzNkJgA93yv8CXeV/Ym6DAKts2DL3o6P1xF/ou18P0TTvhqOVDhKx0pgJ3r1FFIPu18s3pkibNmlH9Ot6uSjMAYKHcz9J4Dt493Zr9OudPZ33PzrIs/Z+kkmNQR8hZx9nQM1zIT4pLqnZwfpuu+c2bfZqKeD/GyWa87K903egLqPeB2NVmauiDPdWYzW63sK5eTnATLP4PZA+ForLPe0g0vOWvApaQ4K5LPfcfZj7LzBI29kYxh5ivOddn4KbjxRberkYxm4Wj48Wmo2QNuGaZxqmngUl2QWgn/XApeklq+flCrh9M8Hz0e5gyCj25yFuP19Xf2dKx1F7R5VzMdJeO44UVn38i5gyF3Qbiuj9sVSUaxcjr8+AxUbAtt31f48gIFMJFr4ZcD6vWFmt1h4QcwfxicOgI3/Z+zRpy+iUlGYgy0ec9ZrPXHpyEoBKrd7nZV4rbNvztL5BSvD7eNdX7BlHSnLkiRtJRwEI7ugsJV3a5E5L8lnnTGJ+5Y5Fli4Ea3KxK37IyBj9tBcClnolDO/G5XlKVoFqSItwQFK3xJxucfCHdMdlY1n9IDYmPcrkjccGAjTLodcoU6S5QofHmVApiISHaUM7/zQzdXqLPg5v4Nblck3nR0F0y81VnPsMcMyFPY7YqyHQUwEZHsKk9h54evj6/TJXlkp9sViTckHISJHeHEYWd9Q+0V6goFMBGR7CykjPND+MRhmHSb88NZsq7TCfDZHXBwM9zxKRSNdLuibEsBTEQkuysaCV0nOz+UP7vD+SEtWU9yIky7x5l80fFDKN3U7YqyNQUwERFxFhO+bazzw3naPc4Pa8k6rIVvHoGNP0Obd6BKB7cryvYUwERExFG5PbR91/kh/c3D/+wOIpnfry/B8snQ7Dmo09PtagQtxCoiIueKug+O74fZbzgzJJu/5nZFcq3mD4U/h0Kd+6Hp025XIx4KYCIicr6mTztbFv05DHKFObs6SOa07DP49UWo3AFavaXdOTIQBTARETmfMc4P64QDTtdVrjCI7OZ2VXKlNvwMXz8EpZpCxzHOciOSYSiAiYjIv/n4wq2j4cQh+Lof5AyGCi3drkpSa/tCmHo3FK7mLDfhl8PtiuQCGoQvIiIX55cDukyCItVh2t2w/S+3K5LU2L0cJneGvEWdNd5y5HG7IrkIBTAREflvOfI4P8TzhTs/1Peucbsi+S+HtsFXD8KYZuAX6OxykDvM7arkPyiAiYjIpeUKhe5fgn+Qs2XR4e1uVyTnOrYXfngahtWGldPhugfggflQoITblcklaAyYiIhcXoESzubd41s5mzjf97MTzMQ9Jw45S0wsHAVJp6Bmd2cGa75wtyuTVFALmIiIpE6hKtB1ChyJhU9vh1PH3K4oezodD3PfgSE1YN67UKEVPLQIbhmq8JWJKICJiEjqlagPnT6G3StgSndIOu12RdlH0ilYOAaGRMLMVyCiHvSZC7ePg9CyblcnV0gBTERErkyFVnDLMPh7Nszooy2L0ltKMiz9FIZFwY/9IbSc0wV851RnhqpkShoDJiIiV67mnc5q+b+97IwFa/W2VllPa9bC2m/g99fhwHooEgnt3ocyN+hcZwEKYCIicnUaPuqEsAXDIVdBaNrf7YqyBmth8+9ON+PuZRBaHjp/ApVuUfDKQhTARETk6hgDN78K8Qdg1mtOS1jUvW5XlbltX+gEr23zIF9xaD8SqncBX/24zmr0LyoiIlfPxwfaD4cTB+H7JyAoBCrf4nZVmc+elfD7a7DhJ2fvzVZvQ+17tIVQFqYAJiIi18bX35kZ+UkH+KIn5PwCSjVxu6rMIW4zzHoDVk2HwHxw40twXV8IyOV2ZZLONAtSRESuXUAu6DYFgkvDZ92c/Qjlvx3ZCd88AsPrwPofoNET8OhyaPykwlc2oRYwERFJG0HBzpZFHzWHSbdDz5+dQCb/iD8A896DRR+CTYE6PaHxU5CnkNuViZepBUxERNJOvmLOJtApSc6WRcf2ul1RxnDyqNPVOKQG/DUSqt0OD8dA60EKX9mUApiIiKStsPJw5zQ4vg8m3QYnj7hdkXsSTzj7NQ6pAX+85azh9eBf0GGkNsvO5hTAREQk7YVHQZeJsH+tMyYs8aTbFXlXciJEj4OhNeHXF6FoTbh/lnNOwiq4XZ1kAApgIiKSPsreBB1GOWtafdHT2VInq0tJgRXTnMH13z0O+SLgnu+hx5dQrJbb1UkGokH4IiKSfqp3goQ4+OkZJ5C0G5I1V3O3Ftb/6KzltW81FKoKXadA+RZZ8/PKNVMAExGR9FWvL8Tvg7nvOIuM3vii2xWlrS1znNXrYxc7sz5v+wiqdHQWqRX5DwpgIiKS/m540dk3cu5gyF0QruvjdkXXbmcMzHwV/p4FeYo6rXuRdzoL04pchgKYiIikP2OgzXuQcBB+fNrZsqja7W5XdXX2rYPfX4V13zmfo8UbENUT/APdrkwyEQUwERHxDl8/p3tuUkeY0RdyFoCyN7pdVeod2gqzB8LyzyEgNzR7Duo9AIF53a5MMiEFMBER8R7/QOj6GYxvA1N6wN3fQnhtt6u6tGN7YM4giJkAPr7QoJ+zdVBQsNuVSSamACYiIt4VmA+6T3e2LPr0drjvZ2fx1owm4SDMHwILR0NKItTsAU2fhrxF3a5MsgBN0RAREe/LU9jZssjH1+mSPLLT7Yr+ceq40+I1JNIJYJXawUOLoN37Cl+SZhTARETEHSFloPsXcOKws2VRwkF360k6BX+NgqGRznpeJRtC33lw24dOrSJpSAFMRETcU6QGdJ0MBzfDZ3fA6QTv15CcBEsmwrDazoKxYRWh52/OWLXCVb1fj2QLCmAiIuKuUk3gtrGwYxFMu8fZR9EbUlJg9QwYWQ++6ecsEtvjK2diQEQd79Qg2ZYCmIiIuK9ye2j7Lmz8Gb552AlH6cVa2PgrjGnqBD4fX+jyKdz/O5S5XlsHiVdoFqSIiGQMUfdB/AGY9TrkCoXmr6X9e2xb4GwbtP1PyF8Cbh0N1To5IUzEixTAREQk42jSH47vgz+HOV2CDR9Nm9fdvdwZWL/xF8hdCFoPhlp3g19A2ry+yBVSABMRkYzDGGj1NiTEwa8vOSEsstvVv96BTTDrNWesV2B+uOl/ULcPBASlVcUiV0UBTEREMhYfH7h1FJw4CF/3g5zBUKHllb3G4R3wx1uwbDL4BTota/X7Qc786VKyyJVSABMRkYzHLwd0mQQT2sG0u+Gur6F4vcs/7/h+mPcuLB7r3K7bGxo/AbkLpm+9IldIAUxERDKmHHngzukwrgVM7gz3/gSFKl/82JNHnHFjC0ZC0gmn27LpAMgf4d2aRVIpVctQGGNaGmPWG2M2GWMGXOTxAsaYGcaYFcaYRcaYqp77I4wxs4wxa40xq40xj57znEhjzF/GmGXGmGhjTN20+1giIpIl5AqF7l+Cf5CzZdHh7ec/fjoB5r0P71d3tg8q39zZNqj9CIUvydAuG8CMMb7ACKAVUBnoaoy58FeQ54Bl1trqwF3AEM/9ScCT1tpKQD3goXOe+zbwf9baSOAlz20REZHzFSjhbFmUmAATb3WWqkg67XQzDq0Jv70M4XWgzxzo9DGElnO7YpHLSk0XZF1gk7X2bwBjzOdAe2DNOcdUBt4EsNauM8aUNMYUstbuBnZ77j9mjFkLFPM81wJ5Pc/PB+xKg88jIiJZUaEq0HUKTOwAE26BxHg4tBWK14dO46FEA7crFLkiqQlgxYAd59yOBa674JjlQEdgnqcrsQQQDuw9c4AxpiRQE1jouesx4GdjzGCcljj97xERkf9Wor7TwjWlOxSs5IwPK3uTVq6XTCk1AexiV7a94PZAYIgxZhmwEliK0/3ovIAxuYEvgMestUc9dz8APG6t/cIY0xn4CLjpX29uTG+gN0Dx4sVTUa6IiGRZFVrBUxudNb18tJueZF6puXpjgXNHMoZzQXehtfaotfZez3iuu4AwYAuAMcYfJ3x9aq398pyn3Q2cuT0Np6vzX6y1Y6y1UdbaqLCwsFSUKyIiWVpQsMKXZHqpuYIXA+WMMaWMMQHAHcA35x5gjMnveQygFzDHWnvUGGNwWrbWWmvfveB1dwFNPV/fAGy82g8hIiIikplctgvSWptkjOkH/Az4AuOstauNMX09j48CKgGfGGOScQbY9/Q8vSHQA1jp6Z4EeM5a+wNwP063pR9wEk83o4iIiEhWZ6y9cDhXxhUVFWWjo6PdLkNERETksowxMdbaqIs9pk50ERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRERHxMgUwERERES9TABMRkUxj46GNvPznyyzZu8TtUkSuiZ/bBYiIiFzOoZOHGLFsBNM2TCPFpvDt5m95teGrtCndxu3SRK6KWsBERCTDSkxOZOKaibSZ0YbpG6bTuXxnvrv1O2qE1WDA3AF8sPwDrLVulylyxdQCJiIiGY61lrk75zJo8SC2Ht1Kg6IN6B/Vn7IFygIw5uYx/G/B/xi5bCQ7ju7gfw3+R4BvgMtVi6SeApiIiGQomw9v5u3Fb/Pnrj8pmbckI24cQeNijTHGnD3G39ef1xq+RkSeCEYsG8Hu+N28f/375MuRz8XKRVJPAUxERDKEwycPM3L5SKaun0qQXxD9o/rTtWJX/H39L3q8MYa+NfoSkSeCF+e/SPcfujPyxpFE5I3wcuUiV04BTEREXJWYksjU9VMZuWwkxxOP06l8Jx6KfIgCgQVS9fw2pdtQJFcRHp31KHf+cCdDbhhCzYI107lqkWujQfgiIuKaubFzue2b2xi4aCCVQyozvd10Xqj3QqrD1xm1CtViUutJ5M2Rl14/9+KHv39Ip4pF0oZawERExOv+Pvw3g6IHMW/nPIrnKc7Q64fSLKLZeeO8rlSJvCWY1GoSj856lGfmPkPs8Vjur3b/Nb2mSHpRABMREa85cuoIHyz/gM/XfU5Ov5w8FfUU3Sp2+89xXlcqf2B+Pmz+IS/9+RLDlg5j+9HtvFz/5TR7fZG0ogAmIiLpLiklyRnntXwkx04f47Zyt/FQ5EOE5AxJ8/cK8A3gzUZvUiJPCUYuH8nu+N282+xdzZCUDEUBTERE0tX8nfMZtHgQm49spm7hujxd52kqBFdI1/c0xvBA5AOE5wnnpT9fosePPRhx4wgi8miGpGQMGoQvIiLpYsuRLTw08yH6/taX0ymnef/69xnbfGy6h69ztSvTjg9v/pCDJw9y5/d3smzfMq+9t8ilKICJiEiaOnLqCG8teouOX3ckZm8MT9R+gq/af8WNxW90ZUB8VOEoJrWaRJ6APPT8uSc/bfnJ6zWIXEhdkCIikiaSUpKYvmE6I5aN4MipI3Qs15F+NfsRmjPU7dIoma8kk1o7MyT7z+lP7PFYelbtqRmS4hoFMBERuWYLdi3g7cVvs+nwJqIKRfFM3WeoGFzR7bLOUyCwAB82/5AX57/IkCVD2H50Oy/WfxF/H82QFO9TABMRkau27eg2Bi8ezOzY2RTLXYz3mr3nWldjauTwzcFbjd+ieJ7ijF4xml3Hd/Hu9e+SNyCv26VJNqMAJiIiV+zo6aOMXj6ayesmE+ATwGO1HqN75e7k8M3hdmmXZYyhX81+FM9bnJf/fJkePzgzJMPzhLtdmmQjCmAiIpJqySnJfLHxC4YvHc7hU4e5tdytPFzz4QwxzutK3VLmlvP2kBx6w1BqhNVwuyzJJjQLUkREUmXh7oV0+q4Tr/71KqXyleLztp/zfw3+L1OGrzPqFK7DpNaTCPILoufPPfll6y9ulyTZhAKYiIhc0vaj23n090fp9UsvEhITeKfpO3zc8mMqh1R2u7Q0UTpfaT5t8ykVgyvy5B9PMm7VOKy1bpclWZy6IEVE5KKOnT7Ghys+ZOLaifj7+PNorUfpUblHphjndaWCA4P5qMVHvDDvBd6LeY/tR7fzfL3nNUNS0o0CmIiInCc5JZkZm2YwbOkwDp08RPuy7Xmk5iOEBYW5XVq6yuGbg7eavEVEngg+XPkhO4/v5N1m75InII/bpUkWpAAmIiJnLd6zmLcWvcX6Q+upWbAmI28aSZWQKm6X5TU+xodHaj1CRJ4IXlnwijND8qYRFMtdzO3SJIvRGDAREWHHsR08Putx7vv5Po6ePsqgpoOY0HJCtgpf57q13K2MunkU+xL2cef3d7Jy/0q3S5IsRgFMRCQbO376OO/FvEf7r9ozf9d8+kX245sO39CyZMsMu5iqt1xX5DomtZ5EoF8g9/18H79t+83tkiQLUQATEcmGklOS+XLjl7Sd0ZZxq8bRqlQrvrv1O/rU6EOgX6Db5WUYpfOX5tPWn1I+uDxPzH6Cj1d9rBmSkiY0BkxEJJuJ3hPN24vfZu3BtUSGRTL8xuFUDa3qdlkZVkjOED5q/hHPz3ued2LeYduxbTx33XOaISnXRAFMRCSbiD0Wy7sx7/Lrtl8pnKswbzd5W12NqRToF8igpoMovrQ4Y1eOZdfxXQxuOlgzJOWqKYCJiGRx8YnxjF05lk9Wf4Kvjy8PRT7E3VXuJqdfTrdLy1R8jA+P1nqUiDwRvLrgVe768S5G3jiSIrmLuF2aZEIKYCIiWVSKTeGbzd8wZMkQDpw4QLvS7Xik1iMUzlXY7dIytY7lOlIkVxGemP0E3X7oxvAbhlMlNHvOFpWrp0H4IiJZ0JK9S+j6fVdenP8iRXMX5dPWn/JG4zcUvtJI/aL1mdhqIgE+Adzz0z3M3D7T7ZIkk1EAExHJQnYd38VTfzzF3T/dTdyJOAY2HsikVpOoHlbd7dKynLIFyvJpm08pV6Acj896nAmrJ2iGpKSauiBFRLKAhMQEPlr1ERNWT8BgeKDGA9xT5R6C/IPcLi1LC80ZykctnBmSg6MHs+PYDgbUHYCfj368yqXpChERycRSbArf/f0dQ2KGsO/EPlqXas3jtR9XV6MX5fTLyeCmg3l/yfuMXzWe2OOxDG4ymNwBud0uTTIwBTARkUxq2b5lvLXoLVbFraJaaDXeafYOkQUj3S4rW/IxPjxR+wki8kTw+l+vc/dPdzPixhEKwvKfNAZMRCST2X18N0/PeZoeP/ZgX8I+3mj0BpNaT1L4ygA6le/EyBtHsvP4Trp9343VcavdLkkyKAUwEZFMIiExgRHLRnDLV7fw+/bf6VO9D9/e+i3tyrTDx+jbeUbRoFgDJraaiJ+PH/f+dC+zts9yuyTJgPQ/VkQkg0uxKXy7+VvafdWOUctHcX3E9Xzb4Vv61eynQfYZVLkC5ZjcZjKl85Xm0VmPMmnNJM2QlPNoDJiISAa2fP9y3l70NisOrKBKSBUGNx1MzYI13S5LUiE0ZyjjW47n2bnP8tbit9h+bDtP13laMyQFUAATEcmQ9sTv4f0l7/P9398TljOM1xq+pq7GTCinX07eafoO78W8x4Q1E4g9FsugpoPI5Z/L7dLEZQpgIiIZyMGTB5mybgrjVo0jxaZwf7X76VWtl7oaMzFfH1+eqvMUxfMW542Fb3D3j3cz/MbhmiGZzSmAiYi4wFpL7PFY1h1c98+fuHXsO7EPgBYlW/BE7Scomruoy5VKWulcoTNFcxflqT+e4s7v72T4jcOpFFLJ7bLEJSYzDQqMioqy0dHRbpchInJFElMS+fvw32eD1tqDa1l/cD3HE48D4Gt8KZWvFBWDK1IxuCJRhaOoEqLNnbOq9QfX0+/3fhw5dYRBTQbRNKKp2yVJOjHGxFhroy76mAKYiEjaSUhMYP2h9ayNW3v2702HN5GYkgg4Y4LKFShHpeBKZwNX2fxlCfQLdLly8ab9Cfvp93s/1h1cx9N1nubOSne6XZKkg0sFMHVBiohcpQMnDrD+4HrWHlx7tnVr+9HtWJxfbAvkKEDF4Ip0r9TdCVshFSmRpwS+Pr4uVy5uCwsKY3yL8QyYO4CBiway49gO+kf117WRjSiAiYhcRopNYeexnecFrXUH17H/xP6zxxTLXYyKwRVpW7otlYIrUSG4AoWCCmGMcbFyyciC/IN4r9l7vBPzDhPXTCT2WCxvN3lbEy6yCQUwEZFzJCYnsvnI5vO6EDcc2nDeeK3S+UtTv2h9KhSoQKUQJ2zlDcjrcuWSGfn6+PJ0naeJyBPBwEUDueenexh2wzAK5SrkdmmSzjQGTESyreOnj7Ph0IazLVvrD65n4+GNJKUkAc54rQoFKlAhuIIzZivEGa+VwzeHy5VLVjQndg79/+hP7oDcjLxxJBWCK7hdklwjDcIXkWzvwIkDrI07vwtx+7HtZx8PDgw+Oyj+zJ/ieYprTI541fqD63lw5oMcP32cQU0H0SS8idslyTVQABORbCPFprDj2I7zlnxYF7eOuJNxZ48Jzx1+NmRVCnFmI4blDNN4LckQ9sbv5eHfH2b9ofU8W/dZ7qh4h9slyVXSLEgRyZISkxPZdHjTeWtrrT+0nvjEeAD8jB+l85emYbGGZwfGVwyuSJ6APC5XLvLfCuUqxMctP+bpOU/z+sLX2XZ0G09FPaXW2CxGAUxEMoXjp48747Q8A+PXHVzH5iObz47XCvILokJwBdqVbne2Vats/rIE+Aa4XLnIlQvyD2LI9UMYFD2ISWsnEXs8lrcav6UZklmIApiIZCjWWvaf2H/+Fj0H17Hj2I6zxwQHBlMpuBKNijWiYkhFKhaoSPG8xbVRtWQpvj6+DKg7gIg8Eby9+G3u+ekeht84nIJBBd0uTdKAxoCJiGtSbArbj24/rwtx7cG1HDx58OwxEXkizhsYXym4EmFBYS5WLeJ9f+z4g/5z+pMvRz6G3zBcMyQzCQ3CF5EMITElkV+3/srSfUvPdieeSDoBOOO1yuQvc97A+PIFymu8lojH2ri19JvZj/ikeAY3HUyjYo3cLkkuQwFMRFyVYlP4actPjFg2gu3HthPkF/SvJR/K5C+j8Voil7Enfg/9ZvZj0+FNPFv3WbpU7OJ2SXIJmgUpIq6w1vJH7B8MWzqMDYc2UL5AeYbdMIwm4U00XkvkKhTOVZgJrSbQ/4/+vLbwNbYf284TtZ/QDMlMSAHsHNZajp4+Sr4c+dwuRSTTW7xnMUOWDGH5/uUUz1Oct5u8TYuSLRS8RK5RLv9cDL1hKG8teotP1nxC7LFY3mz8pmZIZjL6TniO6Run0+HrDizes9jtUkQyrdUHVtP7l97c9/N97I7fzcv1X+arDl/RqlQrhS+RNOLn48dz1z3HM3WeYdaOWdz3830cOHHA7bLkCui74TkiwyLJ7Z+bXr/0YuzKsaTYFLdLEsk0Nh/ezOOzHueO7+9g3cF19I/qzw8df+D28rfj7+PvdnkiWY4xhu6VuzPk+iH8feRvun3fjY2HNrpdlqRSqgKYMaalMWa9MWaTMWbARR4vYIyZYYxZYYxZZIyp6rk/whgzyxiz1hiz2hjz6AXPe9jzuquNMW+nzUe6euUKlOPztp/TvERzhiwZwsO/P8yRU0fcLkskQ9t5fCfPz3uejt90ZMHuBTwY+SA/dPyBu6rcpU2rRbzg+uLXM77leJJSkujxYw/m75zvdkmSCpedBWmM8QU2ADcDscBioKu1ds05xwwCjltr/88YUxEYYa290RhTBChirV1ijMkDxAAdrLVrjDHXA88Dbay1p4wxBa21+y5Vi7dmQVpr+Xz957y9+G0K5izIO83eoWpo1XR/X5HM5MCJA4xePprpG6fja3zpWrEr91W9jwKBBdwuTSRb2hO/hwdnPsjfh/+mW6Vu5A3Ie82vaUmjlRLScMGFtKqpeYnmlC1QNk1e679c6yzIusAma+3fnhf7HGgPrDnnmMrAmwDW2nXGmJLGmELW2t3Abs/9x4wxa4Finuc+AAy01p7yPH7J8OVNxhi6VuxKtdBqPDn7SXr82IP+Uf3pWrGrNuuVbO/IqSOMXzWeT9d+SlJKEreWu5U+1ftQKFcht0sTydYK5yrMJy0/4dm5zzJxzUS3y8nwSucvne4B7FJSE8CKATvOuR0LXHfBMcuBjsA8Y0xdoAQQDuw9c4AxpiRQE1jouas80NgY8zpwEnjKWpuhRr9XDa3K1HZTeW7ec7y56E2W7lvK/xr8j1z+udwuTcTrEhIT+HTtp4xfNZ7jicdpXbo1D9Z4kOJ5i7tdmoh45A7IzbAbh5GYkojh6hoMrvZ5wDU1Ulx1vZm0YSQ1Aexin+zC9r+BwBBjzDJgJbAUSDr7AsbkBr4AHrPWHj3nvQsA9YA6wFRjTGl7QZ+oMaY30BugeHHvf6PPlyMfw24YxrhV4xi2dBjrDq7j3WbvUq5AOa/XIuKG08mnmbZhGmNWjOHgyYM0i2hGv8h+2gpFJAPTxJeMLzUBLBaIOOd2OLDr3AM8oepeAONE0S2ePxhj/HHC16fW2i8veN0vPYFrkTEmBQgF9l/w2mOAMeCMAUv1J0tDPsaHXtV6USOsBk/PeZpu33fjhXov0L5sezfKEfGKpJQkvt38LR8s/4Dd8bupW7guj9R6hBphNdwuTUQk00vNLMjFQDljTCljTABwB/DNuQcYY/J7HgPoBcyx1h71hLGPgLXW2ncveN2vgBs8zy8PBAAZehGTOoXrMK3dNKqFVeOF+S/wvz//x8mkk26XJZKmUmwKP2/9mVu/vpWX/nyJkMAQxtw8hrHNxyp8iYikkcu2gFlrk4wx/YCfAV9gnLV2tTGmr+fxUUAl4BNjTDLOAPuenqc3BHoAKz3dkwDPWWt/AMYB44wxq4DTwN0Xdj9mRKE5Qxlz8xhGLhvJhys/ZHXcat5p+o7GwUimZ61l/q75DF0ylLUH11ImXxnev/59boi4IdOOsRARyai0Gfc1mBM7h2fnPkuKTeHVhq9yU4mb3C5J5Kos2buEIUuGsGTfEorlLsZDkQ/RulRr7S8nInINLrUMhQLYNdp1fBdP/fEUKw+spEflHjxe+3ENfpRMY23cWoYtHcbcnXMJzRlKn+p9uK3cbfj76hoWEblW17oOmFxC0dxFmdByAoOjBzNxzURW7l/JoKaDKJyrsNulifynrUe2MnzZcH7e+jN5A/LyeO3H6VqxKzn9crpdmohItqAWsDT009afeHn+y+TwzcHAxgNpUKyB2yWJnGf38d2MWjGKrzd9TYBvAD0q9+DuKnenyYrZIiJyPrWAeUnLki2pUKACT8x+gr6/9aVPjT70rd5X42jEdXEn4hi7cixT1k8BoGvFrvSq1ouQnCEuVyYikj0pgKWxUvlKMbnNZF776zVGLR/Fsn3LGNh4oH7QiSuOnT7Gx6s/ZuKaiZxKPkWHsh3oW70vRXIXcbs0EZFsTV2Q6cRay4xNM3hj4RvkC8jHoKaDqFWolttlSTZxIukEn637jI9WfsTR00dpUbIFD0U+RKl8pdwuTUQk21AXpAuMMXQs15HKIZV5YvYT3PfzfTxe+3HuqnyX1lSSdJOYnMiXG79k9IrR7D+xn0bFGvFIzUeoFFLJ7dJEROQcCmDprGJwRaa0ncJL819icPRgluxdwquNXtWgZ0lTySnJ/LDlB0YuG0ns8VhqFazFoKaDqF2ottuliYjIRagL0kustUxaO4l3o9+lcK7CvNPsHSqHVHa7LMnkrLX8vuN3hi8dzqbDm6gUXImHaz5Mo2KN1NIqIuKyS3VBpmYvSEkDxhh6VO7B+JbjSUxJpMcPPZi2YRqZKQBLxvLX7r+484c7eWzWYySlJDG46WA+b/s5jcMbK3yJiGRwagFzwaGTh3h27rPM3zWftqXb8mK9FwnyD3K7LMkklu9fzrAlw1i4ZyGFcxXmwRoP0q5MO/x8NKJARCQj0SD8DKZAYAFG3jSSMSucTb3Xxq3l3WbvUjp/abdLkwxsw6ENDF86nFk7ZhEcGMyAugPoVL4TAb4BbpcmIiJXSC1gLluwawED5g7gRNIJ/lf/f7Qu3drtkiSD2XF0ByOWj+CHv38gt39u7ql6D90rdVerqYhIBqfNuDO4vfF7eXrO0yzZt4QuFbrwdJ2n1aoh7I3fy5gVY/hy45f4+fjRrVI37qt6H/ly5HO7NBERSQV1QWZwhXIVYmyLsQxdMpSPV3/MqgOrGNx0MOF5wt0uTVxw+ORhPlr1EZ+t+4xkm8zt5W+nd/XehAWFuV2aiIikEbWAZTC/b/+dF+a9AAbeaPQGzSKauV2SeEl8YjyfrPmECasnkJCYQLsy7XigxgMK4iIimZS6IDOZHcd28OTsJ1l7cC33Vr2XR2o+ohluWdip5FNMWTeFsSvHcujUIW4qfhMPRT5E2QJl3S5NRESugbogM5mIPBFMbD2Rtxa9xfhV41m+bzmDmg6iYFBBt0uTNJSYksjXm75m1PJR7E3YS/0i9Xmk1iNUDa3qdmkiIpLO1AKWwX3393e8suAVcvrl5O0mb3NdkevcLkmuUYpN4eetPzNi2Qi2Hd1G9bDqPFrzUeoWqet2aSIikoa0En4m1rZ0Wz5r8xn5c+Sn96+9Gb18NCk2xe2y5CpYa/ljxx90+rYTT89xZroOu2EYk1pNUvgSEclm1AWZCZTJX4bP2nzG/y34P4YvG86y/ct4s9Gb5A/M73ZpkkqL9yxm6JKhLNu/jIg8EQxsPJBWpVrhY/Q7kIhIdqQuyEzEWsu0DdMYuGggITlDGNx0MDXCarhdllzC6rjVDF0ylD93/UnBnAXpG9mXDmU74O/j73ZpIiKSzjQLMotZHbeaJ2c/yd6EvTwV9RTdKnbT5ssZzN+H/2b4suH8uu1X8ufIT69qvehSoQuBfoFulyYiIl6iAJYFHTl1hBfmv8DsHbNpXqI5/9fg/8gdkNvtsrK1FJvC6gOrmbJ+Ct/+/S2BvoHcXeVu7qp8l/5tRESyIS1DkQXly5GPodc7K+cPWTKE9YfW807Td6gQXMHt0rKVQycP8eeuP5m3cx7zd87n0KlDBPgE0KNSD3pW60mBwAJulygiIhmQWsCygJi9MfT/oz9HTx/l+eue59Zyt7pdUpaVYlNYE7eGuTvnMi92HisPrMRiKZCjAA2KNaBRsUY0LNpQwUtERNQFmR0cOHGAAXMGsHDPQjqU7cBz1z1HTr+cbpeVJRw+efifVq5d8zl48iAGQ9XQqjQu1phGxRpROaQyvj6+bpcqIiIZiLogs4HQnKGMvnk0Hyz/gNErRrMmbg3vNnuXEnlLuF1appNiU1gbt5Y5O+cwb+c8Vh1YRYpNIX+O/DQs1pBGxRrRoGgDggOD3S5VREQyKbWAZUHzds7j2bnPkpiSyCsNXqF5yeZul5ThXaqVq1GxRjQq1ogqIVXUyiUiIqmmLshsaE/8Hp7840lW7F9B90rdeaL2E/j7au2pM860cs3dOZd5O52xXGdauRoU9YzlKtZQrVwiInLV1AWZDRXOVZiPW3zMuzHvMmntJFYcWMHgJoMpkruI26W55sipI2dbuebtnHe2latKSBV6V+9No2KNqBpSVa1cIiKS7tQClg38svUXXvrzJfx8/Hiz0Zs0Dm/sdklekWJTWHtwLfNi5zF359yzrVz5cuSjQdEGNC7WmAZFGxCSM8TtUkVEJAtSF6Sw9chWnvzjSTYc2kDv6r15sMaDWbKl52KtXABVQqrQOLyxWrlERMRr1AUplMxXkk9bf8obC99gzIoxLN+3nIFNBhKaM9Tt0q7Jua1c83bOY8WBFWrlEhGRDE8tYNnQjI0zeH3h6+QNyMvbTd4mqvBFw3mGdeTUERbsWsDcnXOZv3M+cSfjAKeV68yMxWqh1dTKJSIirlILmJzn1nK3UjmkMk/+8SS9funFI7Ue4Z4q9+BjfNwu7aJSbArrDq5j3s55zI2de7aVK29AXhoWbUijcGddrszemiciItmHWsCyseOnj/Pyny/zy7ZfaBbejNcavUa+HPncLgvwtHLtXnC2a/FMK1flkMo0KtaIxsUaq5VLREQyNA3Cl/9krWXyuskMjh5MoaBCvNPsHaqEVHGljrOtXDvnsmL/CpJtMnkD8p63LpdauUREJLNQAJPLWrF/BU/+8SRxJ+J4ps4zdK7QGWNMur7nua1c83fN58CJAwBUCq5E4/DGNC7WmKqhVfHzUU+5iIhkPhoDJpdVPaw609pO49l5z/LawteI2RfD/+r/jyD/oDR7j3NbuebtnMfy/ctJtsnkCcjjjOVSK5eIiGQTagGT86TYFMauHMuIZSMombck7zZ7lzL5y1z16x09fZQFuxY4eyzunM/+E/sBp5WrUbFGNA53xnKplUtERLIadUHKFVu0exH95/TnRNIJXqz3Iu3KtEvV86y1rD+0/uyMxXNbuc6M5WpUrJFauUREJMtTAJOrsi9hH0/PeZqYvTHcXv52BtQdQA7fHP867ujpo/y166+z63KplUtERERjwOQqFQwqyNjmYxm2dBjjVo1j9YHVvNPsHcJzh1+8lcs/D/WL1qdxeGMaFm1IWFCY2x9BREQkQ1ILmKTK7B2zeW7ec2Ahp19O9p3YB0DF4Io0LubssVg9rLpauURERDzUAibXrFlEM6a2ncpbi98iwCfg7FgutXKJiIhcOQUwSbXwPOEMu2GY22WIiIhkehlz8z8RERGRLEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvEwBTERERMTLFMBEREREvMxYa92uIdWMMfuBben8NqHAgXR+j8xO5+jSdH4uT+fo0nR+Lk/n6NJ0fi7PG+eohLU27GIPZKoA5g3GmGhrbZTbdWRkOkeXpvNzeTpHl6bzc3k6R5em83N5bp8jdUGKiIiIeJkCmIiIiIiXKYD92xi3C8gEdI4uTefn8nSOLk3n5/J0ji5N5+fyXD1HGgMmIiIi4mVqARMRERHxskwbwIwxLY0x640xm4wxAy7yuDHGDPU8vsIYU+tyzzXGBBtjfjXGbPT8XcBz/83GmBhjzErP3zdc5P2+McasOud2DmPMFM97LDTGlEzzk3AJmeD83GOM2W+MWeb50yvtz8KlZZRzZIyZ7XmtM+eioOd+XUNc8vzoGvrnOQHGmDHGmA3GmHXGmNs89+sa4pLnR9eQc3+ec87BMmPMAWPM+57Hsv01dJnzc/XXkLU20/0BfIHNQGkgAFgOVL7gmNbAj4AB6gELL/dc4G1ggOfrAcBbnq9rAkU9X1cFdl7wXh2BycCqc+57EBjl+foOYIrOz3nn5x5guK4hCzAbiLpIjbqGLn1+dA398z7/B7zm+doHCNU1lKrzo2vo4nXFAE10DaXq/Fz1NeTKRZcG/yj1gZ/Puf0s8OwFx4wGup5zez1Q5FLPPXOM5+siwPqLvLcB4oAcntu5gXlAZc4PGD8D9T1f++Es9mZ0fs4ed9UXbRY8R7O5eMDQNXTp86Nr6J9ztAPIpWvois+PriHPOTrn/nKe83VmjLiuoUufn6u+hjJrF2QxnBNwRqznvtQcc6nnFrLW7gbw/F3wIu99G7DUWnvKc/tV4B0g4b/e31qbBBwBQi73wdJIZjg/ALd5moynG2MiLvup0lZGOkcA4z3N1y8aY8yF75/NryG4+PkBXUOnjDH5Pfe9aoxZYoyZZowpdOH7Z9dr6DLnB3QNnbrg/q44rVz2wvfPrtfQBfdfeH7gKq+hzBrAzEXus6k8JjXPvfibGlMFeAvo47kdCZS11s64yhrTS2Y4P98CJa211YHfgAmpeY80lCHOkced1tpqQGPPnx5XUGN6yQznR9eQww8IB+Zba2sBC4DBV1BjeskM50fX0L/dAXx2hTWml8xwfq76GsqsASwWODdlhgO7UnnMpZ671xhTBMDz974zBxljwoEZwF3W2s2eu+sDtY0xW3G62cobY2Zf+P7GGD8gH3DwCj/n1crw58daG3fObxYfArWv+FNem4xyjrDW7vT8fQxnrFzdC98/G19D/3l+dA2dPUdxOC3MZ37RmQacGYisa+gS50fX0D//zzyP1QD8rLUxF3v/bHwNnXnsX+fnmq6h9O7DTY8/OL/R/A2U4p/BdVUuOKYN5w/MW3S55wKDOH9g3tuer/N7jrvtEjWV5PwxTg9x/sDFqTo/552fIud8fSvwV3a8hjyvdWZAsD8wHeirayhV50fX0D/v8zlwg+fre4BpuoZSdX50DZ3/XgOB/7vgPl1Dlz4/V30Nee1CS4d/mNbABpxZDs977uvLP9+cDTDC8/hKzhnEe7Hneu4PAWYCGz1/B3vufwGIB5ad86fgBfWU5PyAEYjzm9YmYBFQWufnvPPzJrDac7HPAipmx2sIyIUzo2aF53wMAXx1DaXq/Oga8vw/A0oAczznaSZQXNdQqs6PrqFzvlfjBJaKF9Sna+jS5+eqryGthC8iIiLiZZl1DJiIiIhIpqUAJiIiIuJlCmAiIiIiXqYAJiIiIuJlCmAiIiIiXqYAJiIiIuJlCmAiIiIiXqYAJiIiIuJl/w8cgCsaxfgX0gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(0.9274082603996057, 0.9322021010753989)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Est:50 ccp_alpha:0.000010 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9995651701271877 \n",
    "#Est:100 ccp_alpha:0.000030 min_samples_split:3 accuracy test max:0.9426832361146336 accuracy train max:0.999891292531797 \n",
    "#Est:200 ccp_alpha:0.000070 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9989129253179694 \n",
    "def feature_check(alpha, n_est=50,ccp_alpha=0.00005,min_samples_split=2, C=None, kernel=None, solver=None,penalty=None):\n",
    "    \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=0,test_size=0.3,stratify=target)\n",
    "    \n",
    "    results = defaultdict(lambda: [])\n",
    "    \n",
    "    for n in alpha:\n",
    "        kf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "        all_features = None\n",
    "\n",
    "        #print(\"Alpha:{:.6f}\".format(number_of_features))\n",
    "\n",
    "        #make 5 fold cross valdation with use of features selected by train split\n",
    "\n",
    "        for train_index, test_index in kf.split(df,target):\n",
    "            X_train = df.iloc[train_index]\n",
    "            X_train, X_test, y_train, y_test = df.iloc[train_index], df.iloc[test_index], target[train_index], target[test_index]\n",
    "\n",
    "            #select K features with use of random forest feature selection\n",
    "            #X_train_f, ft = rforest_features(X_train, y_train, n)\n",
    "            X_train_f, ft = lasso(X_train, y_train, alpha=n)\n",
    "            if all_features == None:\n",
    "                all_features = set(ft)\n",
    "            else:\n",
    "                all_features = all_features.intersection(set(ft))\n",
    "            #for f in ft:\n",
    "            #    all_features.append(f)\n",
    "\n",
    "            #keep only selected features in train and test splits\n",
    "            X_test_f = X_test[ft]\n",
    "\n",
    "    \n",
    "            lr_model = LogisticRegression(max_iter=10000,penalty=\"l2\",\n",
    "                                                            solver=\"saga\",C=77.9)\n",
    "            lr_model.fit(X_train_f,y_train)\n",
    "\n",
    "            results[\"k_lr_test\"].append(accuracy_score(y_test, lr_model.predict(X_test_f)))\n",
    "            results[\"k_lr_train\"].append(accuracy_score(y_train, lr_model.predict(X_train_f)))\n",
    "            \n",
    "            \n",
    "            #Kernel:rbf C:19 accuracy test max:0.9264951920766673 accuracy train max:0.9580352710950134 \n",
    "            #Kernel:rbf C:20 accuracy test max:0.9258100000405267 accuracy train max:0.9572173665580053 \n",
    "            #Kernel:rbf C:21 accuracy test max:0.9262675763191611 accuracy train max:0.9585108039737607 \n",
    "            svc_model = SVC(kernel=\"rbf\", C=19)\n",
    "            svc_model.fit(X_train_f,y_train)\n",
    "            results[\"k_svc_test\"].append(accuracy_score(y_test, svc_model.predict(X_test_f)))\n",
    "            results[\"k_svc_train\"].append(accuracy_score(y_train, svc_model.predict(X_train_f)))\n",
    "\n",
    "            #Est:50 ccp_alpha:0.000010 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9995651701271877 \n",
    "            #Est:100 ccp_alpha:0.000030 min_samples_split:3 accuracy test max:0.9426832361146336 accuracy train max:0.999891292531797 \n",
    "            #Est:200 ccp_alpha:0.000070 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9989129253179694 \n",
    "\n",
    "            rfor_model = RandomForestClassifier(n_estimators=100,#bootstrap=False,  #best 150 - 100\n",
    "                                                                criterion='gini',\n",
    "                                                                ccp_alpha=0.000030, #best 0.0001 - 0.00005\n",
    "                                                                min_samples_split=3) #best 2 \n",
    "            rfor_model.fit(X_train_f,y_train)\n",
    "            results[\"k_rfor_test\"].append(accuracy_score(y_test, rfor_model.predict(X_test_f)))\n",
    "            results[\"k_rfor_train\"].append(accuracy_score(y_train, rfor_model.predict(X_train_f)))\n",
    "            results[\"k_rfor_unseen\"].append(accuracy_score(target_test, rfor_model.predict(df_test[ft])))\n",
    "        results[\"lr_test\"].append( sum(results[\"k_lr_test\"]) / len(results[\"k_lr_test\"]))\n",
    "        results[\"lr_train\"].append(sum(results[\"k_lr_train\"])/ len(results[\"k_lr_train\"]) )\n",
    "        results[\"lr_unseen\"].append(sum(results[\"k_lr_unseen\"])/ len(results[\"k_lr_unseen\"]) )\n",
    "        results[\"k_lr_train\"] = []\n",
    "        results[\"k_lr_test\"] = []\n",
    "        \n",
    "        results[\"svc_test\"].append( sum(results[\"k_svc_test\"]) / len(results[\"k_svc_test\"]))\n",
    "        results[\"svc_train\"].append(sum(results[\"k_svc_train\"])/ len(results[\"k_svc_train\"]) )\n",
    "        results[\"k_svc_train\"] = []\n",
    "        results[\"k_svc_test\"] = []\n",
    "        \n",
    "        results[\"rfor_test\"].append( sum(results[\"k_rfor_test\"]) / len(results[\"k_rfor_test\"]))\n",
    "        results[\"rfor_train\"].append(sum(results[\"k_rfor_train\"])/ len(results[\"k_rfor_train\"]) )\n",
    "        results[\"k_rfor_train\"] = []\n",
    "        results[\"k_rfor_test\"] = []\n",
    "    \n",
    "    plt.figure(figsize=(10.0,10.0))\n",
    "    plt.plot(alpha, results[\"rfor_test\"],label=\"rfor_test\")\n",
    "    plt.plot(alpha, results[\"svc_test\"],label=\"svc_test\")\n",
    "    plt.plot(alpha, results[\"lr_test\"],label=\"lr_test\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10.0,10.0))\n",
    "    plt.plot(alpha, results[\"rfor_train\"],label=\"rfor_train\")\n",
    "    plt.plot(alpha, results[\"svc_train\"],label=\"svc_train\")\n",
    "    plt.plot(alpha, results[\"lr_train\"],label=\"lr_train\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    #print(results[\"rfor_test\"])\n",
    "    #print(results[\"rfor_train\"])\n",
    "    return max(results[\"lr_test\"]), results[\"lr_train\"][results[\"lr_test\"].index(max(results[\"lr_test\"]))]\n",
    "#-------------------------------------\n",
    "#random forest parameters fine tuning\n",
    "#-------------------------------------\n",
    "#for n_est in [50,100,150,200]:\n",
    "#    for ccp_alpha in [0.00001, 0.00002,0.00003, 0.00004,0.00005, 0.00006,0.00007]:\n",
    "#        for min_samples_split in [2,3,4]:\n",
    "#            res_test, res_train = feature_check(range(30,90,5),n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split =min_samples_split )\n",
    "#            print(\"Est:{} ccp_alpha:{:.6f} min_samples_split:{} accuracy test max:{} accuracy train max:{} \".format(n_est,ccp_alpha,min_samples_split,res_test, res_train))\n",
    "\n",
    "#---------------------------\n",
    "#SVM parameters fine tuning\n",
    "#---------------------------\n",
    "#for kernel in [\"rbf\"]:\n",
    "#    for c in [18,19,20,21,22]:\n",
    "#        res_test, res_train = feature_check(range(30,90,5),C=c, kernel= kernel)#,n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split =min_samples_split )\n",
    "#        print(\"Kernel:{} C:{} accuracy test max:{} accuracy train max:{} \".format(kernel, c,res_test, res_train))\n",
    "\"\"\"\n",
    "for penalty in ['elasticnet','none']:\n",
    "    for C in np.logspace(-4,4,20):\n",
    "        for solver in ['lbfgs','newton-cg','liblinear','sag','saga']:\n",
    "            if penalty in ['l1','elasticnet'] and solver in ['newton-cg', 'lbfgs', 'sag', 'saga']:\n",
    "                continue\n",
    "            if (penalty == 'elasticnet' and solver != \"saga\") or (penalty=='none' and solver =='liblinear'):\n",
    "                continue\n",
    "            #if solver in ['newton-cg', 'lbfgs', 'sag', 'saga'] and penalty in ['l2', 'none']\n",
    "            res_test, res_train = feature_check(range(30,50,5),C=C, solver= solver, penalty=penalty)# kernel= kernel)#,n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split \n",
    "            print(\"Penalty:{} C:{} solver:{} accuracy test max:{} accuracy train max:{} \".format(penalty, C, solver,res_test, res_train))\n",
    "\"\"\"\n",
    "#for penalty in [\"l2\"]:\n",
    "#    for solver in [\"saga\"]:#\"saga\"]:#,\"sag\",\"liblinear\"]:\n",
    "#        if penalty == \"l1\" and solver != \"liblinear\":\n",
    "#            continue\n",
    "#        for C in np.arange(77.0,79.0,0.1):\n",
    "#            res_test, res_train = feature_check(range(30,50,5),C=C, solver= solver, penalty=penalty)# kernel= kernel)#,n_est=n_est,ccp_alpha=ccp_alpha,min_samples_split \n",
    "#            print(\"Penalty:{} C:{} solver:{} accuracy test max:{} accuracy train max:{} \".format(penalty, C, solver,res_test, res_train))\n",
    "\n",
    "#print(\"Test:\",res_test)\n",
    "#print(\"Train:\",res_train)\n",
    "\n",
    "feature_check(list(np.arange(0.00024,0.00028,0.000005)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost fine tune and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.0594\n",
      "Time spend:0.34943735202153525 min. left comp: 5 left time:1.7471867601076763\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3708c4aac463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mselection_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;31m# eval model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0mselect_X_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect_X_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpredictions_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselect_X_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_base.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m         X = check_array(X, dtype=None, accept_sparse='csr',\n\u001b[1;32m     76\u001b[0m                         force_all_finite=not tags.get('allow_nan', True))\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             warn(\"No features were selected: either the data is\"\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_base.py\u001b[0m in \u001b[0;36mget_support\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[0mare\u001b[0m \u001b[0mindices\u001b[0m \u001b[0minto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \"\"\"\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_support_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_from_model.py\u001b[0m in \u001b[0;36m_get_support_mask\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m                              \u001b[0;34m' \"prefit=True\" while passing the fitted'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                              ' estimator to the constructor.')\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_feature_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m         \u001b[0mthreshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_calculate_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_from_model.py\u001b[0m in \u001b[0;36m_get_feature_importances\u001b[0;34m(estimator, norm_order)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_get_feature_importances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Retrieve or aggregate feature importances from estimator\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mimportances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"feature_importances_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coef_\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfeature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    740\u001b[0m                 .format(self.booster))\n\u001b[1;32m    741\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_booster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportance_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0mall_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mget_score\u001b[0;34m(self, fmap, importance_type)\u001b[0m\n\u001b[1;32m   1911\u001b[0m             \u001b[0maverage_over_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1913\u001b[0;31m         \u001b[0mtrees\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mimportance_type\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'='\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mget_dump\u001b[0;34m(self, fmap, with_stats, dump_format)\u001b[0m\n\u001b[1;32m   1802\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1803\u001b[0m                 \u001b[0mftype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrom_pystr_to_cstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1804\u001b[0;31m             _check_call(_LIB.XGBoosterDumpModelExWithFeatures(\n\u001b[0m\u001b[1;32m   1805\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1806\u001b[0m                 \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------------------#\n",
    "#Very expensive in computations with all parameters\n",
    "#now while i know the best parameters we need to run only 2 models comparison\n",
    "#need to implement ROC plot for each model\n",
    "#----------------------------------------------------------------#\n",
    "\n",
    "objective=[\"multi:softprob\"]#,\"multi:softmax\"]\n",
    "learning_rate =[0.078,0.078,0.078, 0.079, 0.079,0.079,0.079,0.079,0.079,0.079,0.079,0.079]\n",
    "n_estimators=[430, 440, 490, 400, 400, 410, 440, 450, 450, 480, 490, 480]\n",
    "max_depth=[4,3,4,3,3,4,3,4,4,4,4,3]\n",
    "colsample_bytree = [0.35, 0.35,0.5, 0.3, 0.45, 0.3, 0.45, 0.35, 0.45, 0.45, 0.45, 0.35]\n",
    "#-----------------------------\n",
    "objective=[\"multi:softprob\"]\n",
    "learning_rate =[0.078, 0.079]\n",
    "n_estimators=[490, 400]\n",
    "max_depth=[4, 4]\n",
    "colsample_bytree = [0.5, 0.45]\n",
    "eval_metric=[\"aucpr\"]\n",
    "\n",
    "model_param = []\n",
    "model_scores = []\n",
    "for i in range(len(colsample_bytree)):\n",
    "    model_scores.append(defaultdict(lambda: {\"test\": [], \"train\": []}))\n",
    "\n",
    "c = 0\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "number_of_iter = 1\n",
    "for fold in range(number_of_iter):\n",
    "    #split entire dataset into 2 parts: \n",
    "    #                    1st with all data used in training/evaluation(80%) \n",
    "    #                    2nd with data used in final testing (20%) \n",
    "    df, target, df_test, target_test = read_data(filename, verbose=True)\n",
    "    \n",
    "    #at this step we use only 1st splited part with 80% of our data that is used on train/eval split (70/30)\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(df, target, test_size=0.3)\n",
    "\n",
    "    # scale data separatly\n",
    "    X_train, X_eval, df_test = scale_data(X_train, X_eval, df_test)\n",
    "    \n",
    "    # oversamplong clean on training set\n",
    "    X_train, y_train = oversample(X_train, y_train)\n",
    "    #oversampling clean on evaluation set\n",
    "    X_eval, y_eval = oversample(X_eval, y_eval)\n",
    "\n",
    "\n",
    "    models = []\n",
    "    predictions = []\n",
    "    accuracy = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #all_comp = len(objective) * len(learning_rate) * len(n_estimators) * len(max_depth) * len(colsample_bytree)\n",
    "    all_comp = len(learning_rate) * number_of_iter\n",
    "\n",
    "    \n",
    "    for i in range(len(colsample_bytree)):\n",
    "        obj = objective[0]\n",
    "        lr = learning_rate[i]\n",
    "        n_est = n_estimators[i]\n",
    "        mx_d = max_depth[i]\n",
    "        col_b = colsample_bytree[i]\n",
    "        c += 1\n",
    "\n",
    "\n",
    "        model_param.append(\"Obj:{} LR:{} N_EST:{} MAX_DEPTH:{} ByTREE:{} EvalTree:{}\".format(\n",
    "                            obj, lr, n_est, mx_d, col_b, eval_metric))\n",
    "        scores = defaultdict(lambda: {\"test\": [], \"train\": [], \"unseen\": []})\n",
    "        model = XGBClassifier(objective=obj,\n",
    "                                       num_class = 2,\n",
    "                                       learning_rate =lr,\n",
    "                                       n_estimators=n_est,\n",
    "                                       max_depth=mx_d,\n",
    "                                       colsample_bytree = col_b,\n",
    "                                       eval_metric=eval_metric,#\"rmse\",\n",
    "                                       use_label_encoder=False)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # make predictions for test data and evaluate\n",
    "        predictions = model.predict(X_eval)\n",
    "\n",
    "        accuracy = accuracy_score(y_eval, predictions)\n",
    "        print(\"Accuracy: {:.4f}\".format(accuracy * 100.0))\n",
    "        spend_time = (time.time() - start_time) / 60\n",
    "\n",
    "        print(\"Time spend:{} min. left comp: {} left time:{}\". format(\n",
    "               spend_time, all_comp - c, (spend_time / c)* (all_comp - c)) )\n",
    "        # Fit model using each importance as a threshold\n",
    "        thresholds = list(np.sort(model.feature_importances_))\n",
    "        thresholds.sort(reverse=True)\n",
    "        #thresholds = thresholds[220:221]\n",
    "\n",
    "        for t in thresholds:\n",
    "            # select features using threshold\n",
    "            selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "            select_X_train = selection.transform(X_train)\n",
    "            # train model\n",
    "            selection_model = XGBClassifier(objective=obj,\n",
    "                                       num_class = 2,\n",
    "                                       learning_rate =lr,\n",
    "                                       n_estimators=n_est,\n",
    "                                       max_depth=mx_d,\n",
    "                                       colsample_bytree = col_b,\n",
    "                                       eval_metric=eval_metric,#\"rmse\",\n",
    "                                       use_label_encoder=False)\n",
    "            selection_model.fit(select_X_train, y_train)\n",
    "            # eval model\n",
    "            select_X_test = selection.transform(X_eval)\n",
    "            predictions = selection_model.predict(select_X_test)\n",
    "            predictions_train = selection_model.predict(select_X_train)\n",
    "            accuracy = accuracy_score(y_eval, predictions)\n",
    "            accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "            \"\"\"\n",
    "            select_df_test = selection.transform(df_test)\n",
    "            predictions_df_test = selection_model.predict(select_df_test)\n",
    "            accuracy_df_test = accuracy_score(target_test, predictions_df_test)\n",
    "            \"\"\"\n",
    "            model_scores[i][select_X_train.shape[1]][\"test\"].append(accuracy*100.0)\n",
    "            model_scores[i][select_X_train.shape[1]][\"train\"].append(accuracy_train*100.0)\n",
    "        \n",
    "\n",
    "for i in range(len(model_scores)):\n",
    "    print(\"Model: {} param:{}\".format(i, model_param[i]))\n",
    "    n = list(model_scores[i].keys())\n",
    "    n.sort(reverse=True)\n",
    "    for sc in n:\n",
    "        print(\"n:{} Accuracy evaluation : {:.3f} Accuracy train : {:.3f}\"\n",
    "                        #+\"Accuracy df test : {:.3f}\"\n",
    "                        .format(sc,\n",
    "                             sum(model_scores[i][sc][\"test\"])/len(model_scores[i][sc][\"test\"]),\n",
    "                             sum(model_scores[i][sc][\"train\"])/len(model_scores[i][sc][\"train\"]),\n",
    "                             #sum(model_scores[sc][\"unseen\"])/len(model_scores[sc][\"unseen\"])\n",
    "                             ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest fine tune and feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spend:0.34943735202153525 min. left comp: 107 left time:37.38979666630427\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mst_fr_hs_rt_word_1_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.033829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.887534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.001563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.246672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8093</th>\n",
       "      <td>0.887534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8094</th>\n",
       "      <td>0.033829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8095</th>\n",
       "      <td>1.535125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8096</th>\n",
       "      <td>0.887534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8097</th>\n",
       "      <td>0.033829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8098 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mst_fr_hs_rt_word_1_2\n",
       "0                  0.033829\n",
       "1                  0.887534\n",
       "2                 -1.001563\n",
       "3                  0.887534\n",
       "4                 -1.246672\n",
       "...                     ...\n",
       "8093               0.887534\n",
       "8094               0.033829\n",
       "8095               1.535125\n",
       "8096               0.887534\n",
       "8097               0.033829\n",
       "\n",
       "[8098 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       1\n",
       "4       0\n",
       "       ..\n",
       "8093    1\n",
       "8094    1\n",
       "8095    1\n",
       "8096    1\n",
       "8097    1\n",
       "Name: target, Length: 8098, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-55c218b00d4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                     \u001b[0;31m# select features using threshold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mX_train_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrforest_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0mX_eval_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_eval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dd238dfa3f68>\u001b[0m in \u001b[0;36mrforest_features\u001b[0;34m(data, labels, n_features)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0membeded_rf_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0membeded_rf_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0membeded_rf_support\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeded_rf_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_from_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \"Since 'prefit=True', call transform directly\")\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    387\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \"\"\"\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Est:50 ccp_alpha:0.000010 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9995651701271877 \n",
    "#Est:100 ccp_alpha:0.000030 min_samples_split:3 accuracy test max:0.9426832361146336 accuracy train max:0.999891292531797 \n",
    "#Est:200 ccp_alpha:0.000070 min_samples_split:3 accuracy test max:0.9414151661171697 accuracy train max:0.9989129253179694 \n",
    "\n",
    "n_estimators=[50,100,150,200]\n",
    "ccp_alpha =list(np.arange(0.00001,0.0001,0.00001))\n",
    "min_samples_split=[2,3,4]\n",
    "#-----------------------------\n",
    "\n",
    "\n",
    "model_param = []\n",
    "model_scores = []\n",
    "for n_est in n_estimators:\n",
    "    for c_alpha in ccp_alpha:\n",
    "        for min_sam in min_samples_split:\n",
    "            model_scores.append(defaultdict(lambda: {\"test\": [], \"train\": []}))\n",
    "            model_param.append(\"N_estim:{} CC_alpha:{} Min_sample_split:{}\".format(\n",
    "                                    n_est, c_alpha, min_sam))\n",
    "#for i in range(len(min_samples_split)):\n",
    "#    model_scores.append(defaultdict(lambda: {\"test\": [], \"train\": []}))\n",
    "\n",
    "c = 0\n",
    "\n",
    "filename = \"../data/features_large_with_words.csv\"\n",
    "number_of_iter = 1\n",
    "for fold in range(number_of_iter):\n",
    "    #split entire dataset into 2 parts: \n",
    "    #                    1st with all data used in training/evaluation(80%) \n",
    "    #                    2nd with data used in final testing (20%) \n",
    "    df, target, df_test, target_test = read_data(filename, verbose=True)\n",
    "    \n",
    "    #at this step we use only 1st splited part with 80% of our data that is used on train/eval split (70/30)\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(df, target, test_size=0.3)\n",
    "\n",
    "    # scale data separatly\n",
    "    X_train, X_eval, df_test = scale_data(X_train, X_eval, df_test)\n",
    "    \n",
    "    # oversamplong clean on training set\n",
    "    X_train, y_train = oversample(X_train, y_train)\n",
    "    #oversampling clean on evaluation set\n",
    "    X_eval, y_eval = oversample(X_eval, y_eval)\n",
    "\n",
    "\n",
    "    models = []\n",
    "    predictions = []\n",
    "    accuracy = []\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #all_comp = len(objective) * len(learning_rate) * len(n_estimators) * len(max_depth) * len(colsample_bytree)\n",
    "    all_comp = len(model_scores) * number_of_iter\n",
    "    i=0\n",
    "    for n_est in n_estimators:\n",
    "        for c_alpha in ccp_alpha:\n",
    "            for min_sam in min_samples_split:\n",
    "                c += 1\n",
    "                i += 1\n",
    "\n",
    "                scores = defaultdict(lambda: {\"test\": [], \"train\": [], \"unseen\": []})\n",
    "\n",
    "                print(\"Time spend:{} min. left comp: {} left time:{}\". format(\n",
    "                       spend_time, all_comp - c, (spend_time / c)* (all_comp - c)) )\n",
    "\n",
    "                for n in range(1,X_train.shape[1]):\n",
    "                    # select features using threshold\n",
    "                    X_train_f, ft = rforest_features(X_train, y_train, n)\n",
    "                    X_eval_f = X_eval[ft]\n",
    "\n",
    "                    # train model\n",
    "                    selection_model = RandomForestClassifier(n_estimators=n_est,\n",
    "                                                criterion='gini',\n",
    "                                                ccp_alpha=c_alpha, #best 0.0001 - 0.00005\n",
    "                                                min_samples_split=min_sam) #best \n",
    "                    \n",
    "                    selection_model.fit(X_train_f, y_train)\n",
    "                    # eval model\n",
    "\n",
    "                    predictions = selection_model.predict(X_eval_f)\n",
    "                    accuracy = accuracy_score(y_eval, predictions)\n",
    "\n",
    "                    predictions_train = selection_model.predict(X_train_f)\n",
    "                    accuracy_train = accuracy_score(y_train, predictions_train)\n",
    "                    \"\"\"\n",
    "                    select_df_test = selection.transform(df_test)\n",
    "                    predictions_df_test = selection_model.predict(select_df_test)\n",
    "                    accuracy_df_test = accuracy_score(target_test, predictions_df_test)\n",
    "                    \"\"\"\n",
    "                    model_scores[i][n][\"test\"].append(accuracy*100.0)\n",
    "                    model_scores[i][n][\"train\"].append(accuracy_train*100.0)\n",
    "        \n",
    "\n",
    "for i in range(len(model_scores)):\n",
    "    print(\"Model: {} param:{}\".format(i, model_param[i]))\n",
    "    n = list(model_scores[i].keys())\n",
    "    n.sort(reverse=True)\n",
    "    for sc in n:\n",
    "        print(\"n:{} Accuracy evaluation : {:.3f} Accuracy train : {:.3f}\"\n",
    "                        #+\"Accuracy df test : {:.3f}\"\n",
    "                        .format(sc,\n",
    "                             sum(model_scores[i][sc][\"test\"])/len(model_scores[i][sc][\"test\"]),\n",
    "                             sum(model_scores[i][sc][\"train\"])/len(model_scores[i][sc][\"train\"]),\n",
    "                             #sum(model_scores[sc][\"unseen\"])/len(model_scores[sc][\"unseen\"])\n",
    "                             ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation Version 2 (manual implementation of K-fold)\n",
    " - Select features at each fold based by train split , each iteration select new features\n",
    " - at the end of execution make stratified 70/30 split with union of all selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#read csv file data with oversampling\n",
    "#filename = \"../data/features_large_lexical.csv\"\n",
    "#df, target, df_test,target_test = read_data(filename)\n",
    "\n",
    "\n",
    "\n",
    "def k_fold_val_2(number_of_features, \n",
    "                 objective=\"multi:softprob\",\n",
    "                 num_class = 2,\n",
    "                 learning_rate =0.2,\n",
    "                 n_estimators=100,\n",
    "                 max_depth=5,\n",
    "                 colsample_bytree = 0.7,\n",
    "                 eval_metric=\"aucpr\"#\"rmse\"\n",
    "                                           ):\n",
    "    accur_scores = []\n",
    "    accur_scores_train = []\n",
    "    accur_scores_unseen = []\n",
    "    #create Strafied 5 fold class\n",
    "    kf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "    all_features = None\n",
    "    for folds in range(5):\n",
    "        filename = \"../data/features_large_lexical.csv\"\n",
    "        df, target, df_test,target_test = read_data(filename)\n",
    "\n",
    "        #print(\"Alpha:{:.6f}\".format(number_of_features))\n",
    "\n",
    "        #make 5 fold cross valdation with use of features selected by train split\n",
    "\n",
    "        for train_index, test_index in kf.split(df,target):\n",
    "            X_train = df.iloc[train_index]\n",
    "            X_train, X_test, y_train, y_test = df.iloc[train_index], df.iloc[test_index], target[train_index], target[test_index]\n",
    "\n",
    "            #select K features with use of random forest feature selection\n",
    "            X_train_f, ft = rforest_features(X_train, y_train, number_of_features)\n",
    "            #X_train_f, ft = lasso(X_train, y_train, alpha=number_of_features)\n",
    "            if all_features == None:\n",
    "                all_features = set(ft)\n",
    "            else:\n",
    "                all_features = all_features.intersection(set(ft))\n",
    "            #for f in ft:\n",
    "            #    all_features.append(f)\n",
    "\n",
    "            #keep only selected features in train and test splits\n",
    "            X_test_f = X_test[ft]\n",
    "            \"\"\"\n",
    "            #another classifiers for cross validation, that have pour performance till now\n",
    "            reg_scores = cross_val_score(LogisticRegression(max_iter=10000,penalty='l1',\n",
    "                                                            solver='liblinear',C=1.0), X, target,cv=5)\n",
    "\n",
    "            svc_scores = cross_val_score(SVC(kernel='rbf', C=0.9), X, target,cv=5)\n",
    "\n",
    "            for_scores = cross_val_score(RandomForestClassifier(n_estimators=35,\n",
    "                                                                criterion='gini',\n",
    "                                                                ccp_alpha=0.0003,\n",
    "                                                                min_samples_split=4), X, target,cv=5)\n",
    "\n",
    "            \"\"\"\n",
    "            xGb_cl = XGBClassifier(objective=objective,\n",
    "                                   num_class = num_class,\n",
    "                                   learning_rate =learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   colsample_bytree = colsample_bytree,\n",
    "                                   eval_metric=eval_metric,#\"rmse\",\n",
    "                                   use_label_encoder=False)\n",
    "            #fit model with train dataset\n",
    "            xGb_cl.fit(X_train_f, y_train)\n",
    "\n",
    "            #predict test dataset and keep prediction labels\n",
    "            pred = xGb_cl.predict(X_test_f)\n",
    "            pred_train = xGb_cl.predict(X_train_f)\n",
    "            pred_unseen = xGb_cl.predict(df_test[ft])\n",
    "            #print(\"Fold accuracy score: \",accuracy_score(y_test, pred))\n",
    "            accur_scores.append(accuracy_score(y_test, pred))\n",
    "            accur_scores_train.append(accuracy_score(y_train, pred_train))\n",
    "            accur_scores_unseen.append(accuracy_score(target_test, pred_unseen))\n",
    "    #print(all_features)\n",
    "    accur_scores = sum(accur_scores) / len(accur_scores)\n",
    "    accur_scores_train = sum(accur_scores_train) / len(accur_scores_train)\n",
    "    accur_scores_unseen = sum(accur_scores_unseen) / len(accur_scores_unseen)\n",
    "    #print(\"Avg k-fold:{} for :{} features\".format(accur_scores, number_of_features))\n",
    "    #plt.figure(num=None, figsize=(60, 60))\n",
    "    #list_features = list(set(all_features))\n",
    "    #print(len(list_features))\n",
    "    #print(len(ft))\n",
    "    \n",
    "    #count_features = [all_features.count(f) for f in list_features]\n",
    "    #print(count_features)\n",
    "    #plt.bar(range(len(list_features)), count_features)\n",
    "    #plt.xticks(range(len(list_features)), list_features, rotation=45)\n",
    "    #plt.savefig(\"features.png\",dpi=600)\n",
    "\n",
    "    #after k-folds keep all selected features and make 70/30 stratified split and train and predict\n",
    "    all_features = list(set(all_features))\n",
    "    #print(\"70/30\")\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[all_features], \n",
    "                                                        target,\n",
    "                                                        test_size=0.3,\n",
    "                                                        stratify=target)\n",
    "\n",
    "    xGb_cl = XGBClassifier(objective=\"multi:softprob\",\n",
    "                           num_class = 2,\n",
    "                           learning_rate =0.2,\n",
    "                           n_estimators=100,\n",
    "                           max_depth=5,\n",
    "                           colsample_bytree = 0.7,\n",
    "                           eval_metric=\"aucpr\",#\"rmse\",\n",
    "                           use_label_encoder=False)\n",
    "\n",
    "    xGb_cl.fit(X_train, y_train)\n",
    "    pred = xGb_cl.predict(X_test)\n",
    "    print(\"Final accuracy score:\",accuracy_score(y_test, pred),\"\\n--------------\\n\")\n",
    "    return accur_scores, accuracy_score(y_test, pred)\n",
    "    \"\"\"\n",
    "    return accur_scores, accur_scores_train, accur_scores_unseen\n",
    "\n",
    "#k_fold_val_2(number_of_features=80,n_rep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9308 avg_train:0.9502238095238096 avg unseen:0.9294763513513514\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9310031746031745 avg_train:0.9505333333333333 avg unseen:0.9294031531531534\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9310349206349207 avg_train:0.9504587301587302 avg unseen:0.9290033783783782\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9305079365079366 avg_train:0.9505904761904763 avg unseen:0.9289414414414413\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9314857142857145 avg_train:0.9507761904761903 avg unseen:0.929222972972973\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.931015873015873 avg_train:0.9513126984126985 avg unseen:0.9293693693693695\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9323365079365079 avg_train:0.961347619047619 avg unseen:0.9306249999999999\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9324952380952382 avg_train:0.9616238095238097 avg unseen:0.930501126126126\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9323238095238094 avg_train:0.9620460317460319 avg unseen:0.9300619369369368\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9329079365079366 avg_train:0.9620555555555557 avg unseen:0.930213963963964\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9328317460317459 avg_train:0.9623523809523808 avg unseen:0.9302477477477478\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9323492063492064 avg_train:0.9619761904761907 avg unseen:0.9297072072072073\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.933568253968254 avg_train:0.9718841269841269 avg unseen:0.9307319819819818\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9336952380952382 avg_train:0.9724111111111111 avg unseen:0.9312331081081083\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9344571428571428 avg_train:0.9726619047619046 avg unseen:0.9308952702702702\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9338666666666668 avg_train:0.972904761904762 avg unseen:0.930641891891892\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9334730158730157 avg_train:0.9730015873015874 avg unseen:0.930563063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9338158730158731 avg_train:0.972452380952381 avg unseen:0.9301520270270269\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9320698412698413 avg_train:0.9556539682539681 avg unseen:0.9303322072072072\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9320380952380951 avg_train:0.9555095238095238 avg unseen:0.9300506756756756\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9322984126984127 avg_train:0.9563857142857143 avg unseen:0.9304391891891891\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9323555555555556 avg_train:0.9557650793650795 avg unseen:0.9302533783783783\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9327428571428572 avg_train:0.9565285714285716 avg unseen:0.9303997747747745\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9326349206349206 avg_train:0.9562904761904761 avg unseen:0.929774774774775\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9334031746031747 avg_train:0.9669285714285714 avg unseen:0.9309121621621621\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9336444444444445 avg_train:0.9671841269841269 avg unseen:0.930884009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9337777777777778 avg_train:0.9677666666666666 avg unseen:0.93106981981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.933384126984127 avg_train:0.9680857142857144 avg unseen:0.9313288288288287\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9335619047619047 avg_train:0.9681333333333333 avg unseen:0.9308445945945945\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.933384126984127 avg_train:0.9682015873015873 avg unseen:0.9304842342342342\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9343555555555555 avg_train:0.9778158730158731 avg unseen:0.9311430180180181\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346793650793651 avg_train:0.9781444444444443 avg unseen:0.9311148648648647\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342349206349206 avg_train:0.9784603174603176 avg unseen:0.930822072072072\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9345968253968254 avg_train:0.9787301587301589 avg unseen:0.930518018018018\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9347809523809523 avg_train:0.9789380952380954 avg unseen:0.9303096846846848\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349015873015873 avg_train:0.9791095238095237 avg unseen:0.9304504504504506\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9330920634920635 avg_train:0.963736507936508 avg unseen:0.9310810810810811\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.933638095238095 avg_train:0.9638666666666665 avg unseen:0.9309009009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9338857142857142 avg_train:0.9640857142857141 avg unseen:0.9306644144144144\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9337777777777778 avg_train:0.9641650793650792 avg unseen:0.9306024774774774\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9336317460317461 avg_train:0.9644412698412698 avg unseen:0.9308389639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9339936507936507 avg_train:0.9645396825396825 avg unseen:0.9300337837837839\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9346539682539681 avg_train:0.975831746031746 avg unseen:0.9314695945945947\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345333333333334 avg_train:0.9761174603174604 avg unseen:0.9306081081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350158730158729 avg_train:0.9764777777777778 avg unseen:0.9309346846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9347619047619049 avg_train:0.9769920634920634 avg unseen:0.9309515765765766\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9347809523809523 avg_train:0.9768873015873014 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.977268253968254 avg unseen:0.9309853603603603\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349460317460317 avg_train:0.9877650793650793 avg unseen:0.9309459459459459\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.934768253968254 avg_train:0.9881444444444445 avg unseen:0.9314583333333332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350603174603175 avg_train:0.9882730158730157 avg unseen:0.9306925675675676\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348634920634922 avg_train:0.9882904761904762 avg unseen:0.9306756756756757\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934615873015873 avg_train:0.9884174603174603 avg unseen:0.930259009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348825396825396 avg_train:0.9887396825396827 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9341523809523808 avg_train:0.9668412698412698 avg unseen:0.9311373873873876\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.933815873015873 avg_train:0.9673873015873015 avg unseen:0.9310641891891892\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9336761904761904 avg_train:0.9672920634920635 avg unseen:0.9306475225225225\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9342603174603176 avg_train:0.9676333333333332 avg unseen:0.9305855855855857\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9335936507936508 avg_train:0.967331746031746 avg unseen:0.929954954954955\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343365079365079 avg_train:0.9676190476190477 avg unseen:0.9302083333333333\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9346476190476191 avg_train:0.9800730158730158 avg unseen:0.9314977477477476\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348952380952381 avg_train:0.9801761904761904 avg unseen:0.9309346846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9346730158730159 avg_train:0.9804301587301586 avg unseen:0.9306081081081081\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.934736507936508 avg_train:0.9805587301587301 avg unseen:0.9307432432432431\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9352825396825396 avg_train:0.9806809523809524 avg unseen:0.930884009009009\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347492063492063 avg_train:0.9808285714285714 avg unseen:0.9303378378378379\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348507936507938 avg_train:0.9917952380952381 avg unseen:0.9310022522522523\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348380952380951 avg_train:0.991795238095238 avg unseen:0.9313738738738738\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9351809523809523 avg_train:0.9921000000000001 avg unseen:0.9306362612612612\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9347809523809524 avg_train:0.9922476190476189 avg unseen:0.9307545045045046\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348952380952381 avg_train:0.9921539682539683 avg unseen:0.9305518018018016\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934736507936508 avg_train:0.9920777777777778 avg unseen:0.9299211711711711\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9339428571428572 avg_train:0.9695428571428572 avg unseen:0.9307939189189188\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9342857142857142 avg_train:0.9704317460317461 avg unseen:0.931463963963964\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9343746031746033 avg_train:0.9703111111111111 avg unseen:0.9306587837837837\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9342666666666668 avg_train:0.9709603174603175 avg unseen:0.9310585585585587\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934679365079365 avg_train:0.9707825396825395 avg unseen:0.9303772522522521\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9346031746031745 avg_train:0.9708619047619047 avg unseen:0.9306081081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9344444444444444 avg_train:0.9831920634920636 avg unseen:0.9308727477477476\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9352190476190478 avg_train:0.9840380952380953 avg unseen:0.9310585585585587\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349714285714286 avg_train:0.9841238095238095 avg unseen:0.9309459459459459\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9353587301587302 avg_train:0.9844317460317461 avg unseen:0.9312556306306309\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353968253968253 avg_train:0.9846015873015874 avg unseen:0.9308277027027027\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9342349206349206 avg_train:0.984690476190476 avg unseen:0.9302083333333332\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9353714285714286 avg_train:0.9943857142857143 avg unseen:0.9308389639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9352571428571429 avg_train:0.994773015873016 avg unseen:0.9308051801801803\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9348698412698413 avg_train:0.994379365079365 avg unseen:0.9300844594594594\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9355428571428573 avg_train:0.9947095238095237 avg unseen:0.930563063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9351301587301588 avg_train:0.9947523809523809 avg unseen:0.9302702702702702\n",
      "Obj:multi:softprob\n",
      "LR: 0.07\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349206349206348 avg_train:0.9947174603174602 avg unseen:0.9300506756756755\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9314158730158731 avg_train:0.9528222222222221 avg unseen:0.9302927927927929\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9312571428571427 avg_train:0.9528825396825397 avg unseen:0.9300225225225226\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9312952380952382 avg_train:0.9530936507936509 avg unseen:0.9295326576576577\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9315873015873016 avg_train:0.9533253968253966 avg unseen:0.9297522522522522\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9313904761904762 avg_train:0.9534492063492062 avg unseen:0.9294481981981981\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.931511111111111 avg_train:0.953388888888889 avg unseen:0.929543918918919\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9325714285714284 avg_train:0.9640920634920636 avg unseen:0.93080518018018\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9327111111111109 avg_train:0.9645936507936508 avg unseen:0.9307713963963963\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9331111111111111 avg_train:0.9646936507936509 avg unseen:0.930793918918919\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9327174603174603 avg_train:0.9650714285714285 avg unseen:0.9305743243243242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9332952380952381 avg_train:0.9651238095238094 avg unseen:0.9302252252252251\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9332444444444445 avg_train:0.9651460317460318 avg unseen:0.9305461711711711\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9339428571428571 avg_train:0.9748714285714285 avg unseen:0.9312331081081081\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9337968253968253 avg_train:0.9750698412698412 avg unseen:0.9308277027027027\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9340000000000002 avg_train:0.9752238095238095 avg unseen:0.9312162162162164\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9333079365079365 avg_train:0.9751698412698412 avg unseen:0.9305855855855856\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9339365079365077 avg_train:0.975590476190476 avg unseen:0.9311148648648649\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9341079365079364 avg_train:0.9759111111111111 avg unseen:0.9306306306306306\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9326793650793651 avg_train:0.9583984126984126 avg unseen:0.9308502252252252\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.932368253968254 avg_train:0.9584380952380952 avg unseen:0.9307601351351352\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9329650793650793 avg_train:0.9584777777777777 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9334603174603174 avg_train:0.9587587301587303 avg unseen:0.9306137387387386\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9328825396825399 avg_train:0.9586317460317462 avg unseen:0.9303096846846848\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.932730158730159 avg_train:0.9588761904761905 avg unseen:0.9304222972972974\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9344380952380952 avg_train:0.9697015873015874 avg unseen:0.9310810810810811\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9339174603174603 avg_train:0.9704682539682542 avg unseen:0.9311373873873876\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9345714285714286 avg_train:0.9704380952380953 avg unseen:0.9311204954954954\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9340380952380952 avg_train:0.9707063492063494 avg unseen:0.9308445945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9337841269841269 avg_train:0.9704761904761906 avg unseen:0.930275900900901\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934279365079365 avg_train:0.9710349206349207 avg unseen:0.930867117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9344190476190476 avg_train:0.9809365079365079 avg unseen:0.9312049549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348000000000001 avg_train:0.981347619047619 avg unseen:0.9316216216216215\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350285714285714 avg_train:0.9818936507936509 avg unseen:0.9311486486486489\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9350412698412699 avg_train:0.9819539682539681 avg unseen:0.9307995495495497\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9349333333333334 avg_train:0.9820888888888889 avg unseen:0.9306644144144145\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.935231746031746 avg_train:0.9826 avg unseen:0.9305349099099098\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9339174603174603 avg_train:0.9663603174603175 avg unseen:0.9308558558558557\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.9670634920634922 avg unseen:0.9313682432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9340761904761905 avg_train:0.9670968253968254 avg unseen:0.9305799549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9337587301587301 avg_train:0.9670539682539682 avg unseen:0.9305799549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9342222222222222 avg_train:0.9671841269841269 avg unseen:0.9303716216216218\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9341650793650794 avg_train:0.9671539682539683 avg unseen:0.9303322072072072\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349269841269842 avg_train:0.9794746031746033 avg unseen:0.9310867117117115\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345142857142857 avg_train:0.9798253968253969 avg unseen:0.9311711711711711\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934488888888889 avg_train:0.9799285714285716 avg unseen:0.9307488738738741\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9349714285714286 avg_train:0.9801412698412698 avg unseen:0.9305349099099098\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9352190476190475 avg_train:0.9807920634920634 avg unseen:0.931126126126126\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347301587301585 avg_train:0.9805904761904761 avg unseen:0.9302477477477477\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9351428571428572 avg_train:0.9913841269841269 avg unseen:0.9311261261261262\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9355619047619047 avg_train:0.9911714285714287 avg unseen:0.9310698198198198\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349396825396825 avg_train:0.991331746031746 avg unseen:0.9306981981981983\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9354920634920635 avg_train:0.9913809523809523 avg unseen:0.9302984234234234\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348063492063492 avg_train:0.9919190476190476 avg unseen:0.9307038288288286\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9352 avg_train:0.9921714285714286 avg unseen:0.9302533783783784\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9343174603174603 avg_train:0.9697396825396827 avg unseen:0.9305518018018017\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349333333333334 avg_train:0.970257142857143 avg unseen:0.9311711711711713\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9347365079365079 avg_train:0.9706317460317461 avg unseen:0.9310472972972973\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.933968253968254 avg_train:0.9708396825396827 avg unseen:0.930625\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9346539682539682 avg_train:0.9711000000000002 avg unseen:0.9305743243243243\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9340317460317461 avg_train:0.9709253968253967 avg unseen:0.930748873873874\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349142857142857 avg_train:0.9834349206349207 avg unseen:0.9305405405405405\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9351492063492062 avg_train:0.9836904761904762 avg unseen:0.9307319819819818\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9350920634920635 avg_train:0.984115873015873 avg unseen:0.9302984234234234\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9350412698412699 avg_train:0.9842206349206349 avg unseen:0.9303772522522523\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353142857142857 avg_train:0.9849190476190478 avg unseen:0.9306531531531532\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934152380952381 avg_train:0.984952380952381 avg unseen:0.9303153153153154\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349841269841269 avg_train:0.9944349206349208 avg unseen:0.930625\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345904761904761 avg_train:0.9946809523809522 avg unseen:0.9303603603603604\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349142857142857 avg_train:0.9944666666666665 avg unseen:0.9307882882882882\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.935288888888889 avg_train:0.9948873015873017 avg unseen:0.9304617117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353206349206351 avg_train:0.9947539682539682 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9351365079365079 avg_train:0.9949555555555555 avg unseen:0.9305123873873874\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9347555555555555 avg_train:0.9730873015873017 avg unseen:0.9310585585585587\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9339809523809524 avg_train:0.9732444444444445 avg unseen:0.930242117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9345206349206349 avg_train:0.9737984126984127 avg unseen:0.9307432432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348126984126984 avg_train:0.9738301587301589 avg unseen:0.9302139639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348444444444445 avg_train:0.9744126984126984 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934368253968254 avg_train:0.9745460317460317 avg unseen:0.9307432432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9347873015873015 avg_train:0.9868809523809522 avg unseen:0.9305123873873874\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.934920634920635 avg_train:0.9874682539682539 avg unseen:0.9304954954954954\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9347936507936508 avg_train:0.9877333333333332 avg unseen:0.9306587837837837\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9353650793650793 avg_train:0.9881523809523808 avg unseen:0.9305292792792793\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348380952380952 avg_train:0.9882555555555556 avg unseen:0.9305349099099102\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.934679365079365 avg_train:0.988422222222222 avg unseen:0.930394144144144\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9352444444444443 avg_train:0.99674126984127 avg unseen:0.9306813063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349968253968254 avg_train:0.9967761904761904 avg unseen:0.9304729729729729\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9353650793650795 avg_train:0.9967460317460318 avg unseen:0.9309290540540541\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9351746031746032 avg_train:0.9968380952380954 avg unseen:0.9303997747747749\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9353904761904762 avg_train:0.9969873015873015 avg unseen:0.930365990990991\n",
      "Obj:multi:softprob\n",
      "LR: 0.08\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9352888888888887 avg_train:0.9971190476190476 avg unseen:0.9297184684684684\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9315619047619047 avg_train:0.9546777777777778 avg unseen:0.9299493243243244\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9316888888888889 avg_train:0.9550698412698412 avg unseen:0.9301520270270269\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9322285714285713 avg_train:0.955347619047619 avg unseen:0.9299380630630631\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9316253968253968 avg_train:0.9551047619047619 avg unseen:0.9295213963963965\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9324634920634921 avg_train:0.9559873015873014 avg unseen:0.9302364864864865\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.932152380952381 avg_train:0.9560555555555555 avg unseen:0.9303265765765767\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9334285714285713 avg_train:0.9667412698412697 avg unseen:0.9310135135135135\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.933606349206349 avg_train:0.966968253968254 avg unseen:0.9308614864864865\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9336253968253967 avg_train:0.9669619047619046 avg unseen:0.930867117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9336698412698411 avg_train:0.9669206349206348 avg unseen:0.9305236486486487\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9334222222222223 avg_train:0.9673603174603175 avg unseen:0.9307770270270271\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9336126984126984 avg_train:0.9676634920634921 avg unseen:0.93053490990991\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9337396825396825 avg_train:0.9773714285714288 avg unseen:0.9309403153153153\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9340444444444445 avg_train:0.9773 avg unseen:0.9309346846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934095238095238 avg_train:0.9778190476190476 avg unseen:0.9307826576576576\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9340190476190476 avg_train:0.9777857142857144 avg unseen:0.9308445945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344444444444444 avg_train:0.9781698412698412 avg unseen:0.9306475225225225\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343365079365079 avg_train:0.9783301587301588 avg unseen:0.930242117117117\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9331873015873015 avg_train:0.9607492063492064 avg unseen:0.9310191441441441\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9331428571428573 avg_train:0.9609777777777778 avg unseen:0.9305461711711712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9330666666666665 avg_train:0.9608222222222222 avg unseen:0.9305799549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9332444444444444 avg_train:0.9611936507936508 avg unseen:0.9305855855855856\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9334539682539682 avg_train:0.9612603174603175 avg unseen:0.9306193693693693\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9333650793650795 avg_train:0.9614126984126985 avg unseen:0.9303828828828827\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9337460317460318 avg_train:0.9723317460317461 avg unseen:0.9309515765765766\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9337142857142856 avg_train:0.9731285714285715 avg unseen:0.9307713963963965\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342158730158729 avg_train:0.9731015873015876 avg unseen:0.9308783783783784\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.9734365079365079 avg unseen:0.9304391891891893\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9342857142857142 avg_train:0.9733380952380952 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9339238095238094 avg_train:0.9734587301587303 avg unseen:0.9304448198198196\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342984126984124 avg_train:0.9842730158730157 avg unseen:0.9311430180180181\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9341269841269842 avg_train:0.9843746031746033 avg unseen:0.9309628378378378\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.93455873015873 avg_train:0.9846428571428572 avg unseen:0.9313738738738738\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9353396825396826 avg_train:0.985 avg unseen:0.9307488738738737\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9346539682539682 avg_train:0.9848238095238094 avg unseen:0.9306193693693695\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347301587301587 avg_train:0.9855428571428569 avg unseen:0.9303603603603604\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9341206349206349 avg_train:0.9691682539682539 avg unseen:0.931131756756757\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9344317460317461 avg_train:0.9695714285714286 avg unseen:0.9308389639639639\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9341523809523808 avg_train:0.9697920634920636 avg unseen:0.930974099099099\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9339873015873016 avg_train:0.9701126984126985 avg unseen:0.9304786036036035\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9339111111111111 avg_train:0.9700555555555556 avg unseen:0.9302927927927929\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9350539682539681 avg_train:0.9706111111111113 avg unseen:0.9307826576576576\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9345269841269842 avg_train:0.9827492063492063 avg unseen:0.9306024774774775\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9348380952380951 avg_train:0.9827904761904763 avg unseen:0.930670045045045\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934825396825397 avg_train:0.9834809523809525 avg unseen:0.9306813063063064\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9350920634920636 avg_train:0.9836238095238096 avg unseen:0.9304279279279278\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9346095238095239 avg_train:0.9834984126984126 avg unseen:0.9305461711711712\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343365079365081 avg_train:0.9840079365079365 avg unseen:0.9304786036036035\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9351492063492065 avg_train:0.993725396825397 avg unseen:0.9307770270270271\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345523809523811 avg_train:0.9938666666666668 avg unseen:0.9305630630630631\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9351492063492063 avg_train:0.9941285714285715 avg unseen:0.9306249999999999\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9354222222222223 avg_train:0.9941253968253968 avg unseen:0.9303997747747746\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344952380952379 avg_train:0.9941238095238095 avg unseen:0.9301858108108109\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347809523809523 avg_train:0.9943507936507938 avg unseen:0.9299268018018019\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342539682539682 avg_train:0.9727857142857145 avg unseen:0.9304842342342344\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9345396825396826 avg_train:0.9732253968253968 avg unseen:0.9310022522522522\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9345904761904761 avg_train:0.9735380952380952 avg unseen:0.9307094594594593\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9343111111111111 avg_train:0.9737936507936508 avg unseen:0.9303716216216215\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9349015873015875 avg_train:0.9739873015873016 avg unseen:0.9302533783783783\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348190476190478 avg_train:0.974236507936508 avg unseen:0.9304842342342342\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9351555555555556 avg_train:0.9868825396825396 avg unseen:0.9307376126126125\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349269841269842 avg_train:0.9872285714285715 avg unseen:0.9309403153153154\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9346095238095238 avg_train:0.9872190476190476 avg unseen:0.9299662162162162\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9354285714285714 avg_train:0.9881349206349205 avg unseen:0.930962837837838\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.935377777777778 avg_train:0.9878301587301589 avg unseen:0.9302195945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9343492063492064 avg_train:0.9878809523809524 avg unseen:0.9300844594594594\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349650793650793 avg_train:0.9964873015873016 avg unseen:0.9306193693693694\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9353460317460317 avg_train:0.9966777777777778 avg unseen:0.930731981981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9353206349206348 avg_train:0.996742857142857 avg unseen:0.9307432432432433\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.935657142857143 avg_train:0.9967746031746031 avg unseen:0.9307376126126128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934647619047619 avg_train:0.9967158730158728 avg unseen:0.9301351351351351\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348063492063491 avg_train:0.9971412698412698 avg unseen:0.9300619369369368\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9340634920634922 avg_train:0.9761650793650795 avg unseen:0.9307770270270268\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346412698412698 avg_train:0.9764365079365079 avg unseen:0.9310472972972974\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9344634920634921 avg_train:0.9770571428571428 avg unseen:0.9307150900900901\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9343809523809524 avg_train:0.9771444444444445 avg unseen:0.9306531531531532\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344317460317461 avg_train:0.9774111111111111 avg unseen:0.9301069819819819\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9347174603174603 avg_train:0.9775444444444447 avg unseen:0.9302759009009008\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.934247619047619 avg_train:0.9906190476190476 avg unseen:0.9308952702702703\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9356253968253968 avg_train:0.9909142857142857 avg unseen:0.9309177927927929\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342857142857142 avg_train:0.9911222222222221 avg unseen:0.9303153153153151\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9346730158730159 avg_train:0.9911825396825394 avg unseen:0.9300788288288286\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9348444444444445 avg_train:0.991309523809524 avg unseen:0.9302646396396396\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9348126984126984 avg_train:0.9916396825396826 avg unseen:0.9301745495495496\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9354666666666667 avg_train:0.9983031746031746 avg unseen:0.9309346846846845\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9350730158730158 avg_train:0.9982206349206347 avg unseen:0.930579954954955\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349079365079365 avg_train:0.9986222222222222 avg unseen:0.930382882882883\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9352317460317461 avg_train:0.9985682539682541 avg unseen:0.9305405405405406\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9350603174603173 avg_train:0.9984333333333333 avg unseen:0.9300168918918917\n",
      "Obj:multi:softprob\n",
      "LR: 0.09\n",
      "nEst: 400\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9359555555555555 avg_train:0.9985968253968255 avg unseen:0.9301126126126128\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9321968253968252 avg_train:0.9571476190476192 avg unseen:0.930563063063063\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9321015873015872 avg_train:0.9568412698412699 avg unseen:0.9303603603603604\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9325904761904762 avg_train:0.9574095238095238 avg unseen:0.9300731981981981\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9328952380952383 avg_train:0.9577301587301587 avg unseen:0.9301632882882882\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9332444444444444 avg_train:0.9580555555555553 avg unseen:0.93044481981982\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9329650793650792 avg_train:0.957595238095238 avg unseen:0.9303772522522523\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9331111111111111 avg_train:0.9688507936507936 avg unseen:0.9313907657657657\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9336190476190476 avg_train:0.9690079365079365 avg unseen:0.9311148648648649\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9343555555555557 avg_train:0.9691142857142857 avg unseen:0.9311092342342343\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9331047619047618 avg_train:0.9690539682539684 avg unseen:0.9308502252252252\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9338285714285713 avg_train:0.9697873015873016 avg unseen:0.930670045045045\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9338603174603174 avg_train:0.9697047619047622 avg unseen:0.9303096846846847\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342349206349208 avg_train:0.9798349206349206 avg unseen:0.9307545045045046\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346920634920636 avg_train:0.9806079365079364 avg unseen:0.9311148648648647\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9347492063492066 avg_train:0.9803476190476189 avg unseen:0.9308445945945946\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9344253968253969 avg_train:0.9806507936507934 avg unseen:0.9307094594594595\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9343746031746032 avg_train:0.9807301587301587 avg unseen:0.9309459459459458\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 150\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9341015873015873 avg_train:0.9805047619047618 avg unseen:0.9301914414414415\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9330666666666666 avg_train:0.9626746031746033 avg unseen:0.930748873873874\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9335809523809524 avg_train:0.9632587301587301 avg unseen:0.9309572072072071\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9335809523809523 avg_train:0.9630793650793651 avg unseen:0.9304391891891891\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9340444444444445 avg_train:0.9635984126984128 avg unseen:0.9303885135135136\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.934279365079365 avg_train:0.9636238095238094 avg unseen:0.9306418918918918\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9332507936507937 avg_train:0.9633650793650791 avg unseen:0.93\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9343174603174603 avg_train:0.9748412698412698 avg unseen:0.9306081081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346031746031745 avg_train:0.9754698412698413 avg unseen:0.9308952702702703\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9348126984126984 avg_train:0.975915873015873 avg unseen:0.9309009009009008\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9342730158730159 avg_train:0.9759015873015873 avg unseen:0.9305461711711712\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9345142857142856 avg_train:0.9762809523809522 avg unseen:0.9303603603603602\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9344063492063492 avg_train:0.9758968253968252 avg unseen:0.9304842342342342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9354031746031746 avg_train:0.9870555555555556 avg unseen:0.9313288288288288\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9343619047619048 avg_train:0.9871555555555555 avg unseen:0.9310529279279279\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9348253968253969 avg_train:0.9873920634920634 avg unseen:0.9310416666666664\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9346920634920634 avg_train:0.9877507936507937 avg unseen:0.9308277027027027\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9347365079365079 avg_train:0.9874666666666666 avg unseen:0.9299099099099098\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 200\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349841269841269 avg_train:0.988088888888889 avg unseen:0.9302927927927928\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348507936507936 avg_train:0.9717285714285714 avg unseen:0.9312049549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9342349206349206 avg_train:0.972031746031746 avg unseen:0.9303322072072073\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342158730158729 avg_train:0.9721603174603176 avg unseen:0.9308333333333333\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9344380952380953 avg_train:0.9724714285714285 avg unseen:0.9302533783783783\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.93455873015873 avg_train:0.9729650793650794 avg unseen:0.9306137387387391\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9339301587301586 avg_train:0.9731031746031745 avg unseen:0.9303603603603605\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9342666666666667 avg_train:0.9853206349206348 avg unseen:0.9300506756756756\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9346730158730159 avg_train:0.9860492063492065 avg unseen:0.9312049549549549\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9342285714285713 avg_train:0.9859206349206348 avg unseen:0.9302195945945945\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9346539682539684 avg_train:0.9860968253968254 avg unseen:0.9303490990990991\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9345015873015875 avg_train:0.9867222222222223 avg unseen:0.9304222972972974\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9349142857142859 avg_train:0.9869015873015874 avg unseen:0.9299831081081082\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348317460317461 avg_train:0.9955650793650794 avg unseen:0.9305855855855857\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.935263492063492 avg_train:0.9959412698412698 avg unseen:0.9307601351351352\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9349206349206348 avg_train:0.9959539682539682 avg unseen:0.9307263513513512\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348126984126983 avg_train:0.9959698412698413 avg unseen:0.9299436936936937\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.935047619047619 avg_train:0.9962984126984127 avg unseen:0.930304054054054\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 300\n",
      "m_depth: 6\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.935015873015873 avg_train:0.9963333333333333 avg unseen:0.9299493243243243\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9347238095238096 avg_train:0.975752380952381 avg unseen:0.9310135135135132\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9344063492063492 avg_train:0.9757761904761905 avg unseen:0.9304842342342341\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.934584126984127 avg_train:0.976020634920635 avg unseen:0.9303603603603602\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9338857142857143 avg_train:0.9761809523809525 avg unseen:0.9302702702702702\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9349015873015873 avg_train:0.9763793650793651 avg unseen:0.9303716216216218\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 4\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9350412698412697 avg_train:0.977115873015873 avg unseen:0.9303209459459459\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9349396825396826 avg_train:0.9897999999999999 avg unseen:0.9302308558558559\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9349269841269842 avg_train:0.9903460317460316 avg unseen:0.931097972972973\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9352698412698411 avg_train:0.990311111111111 avg unseen:0.9306362612612613\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9349841269841269 avg_train:0.9903126984126983 avg unseen:0.9307545045045044\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.65\n",
      "ev_m: aucpr score avg_test:0.9344888888888888 avg_train:0.9909301587301588 avg unseen:0.9299887387387388\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 5\n",
      "bytree: 0.7\n",
      "ev_m: aucpr score avg_test:0.9344571428571428 avg_train:0.9908904761904763 avg unseen:0.9299718468468469\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.45\n",
      "ev_m: aucpr score avg_test:0.9348825396825396 avg_train:0.9980079365079365 avg unseen:0.930304054054054\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.5\n",
      "ev_m: aucpr score avg_test:0.9350539682539681 avg_train:0.9982507936507939 avg unseen:0.9306531531531531\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.55\n",
      "ev_m: aucpr score avg_test:0.9352507936507936 avg_train:0.9981444444444444 avg unseen:0.9304729729729729\n",
      "Obj:multi:softprob\n",
      "LR: 0.1\n",
      "nEst: 350\n",
      "m_depth: 6\n",
      "bytree: 0.6\n",
      "ev_m: aucpr score avg_test:0.9348761904761905 avg_train:0.9983253968253966 avg unseen:0.9306081081081081\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-0382528d26c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m                         \u001b[0mf_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                             k_accuracy, k_accuracy_train, k_accuracy_unseen = k_fold_val_2(number_of_features=45, \n\u001b[0m\u001b[1;32m     58\u001b[0m                                                                         \u001b[0mobjective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                                                                        \u001b[0mnum_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-3b1508eecbd1>\u001b[0m in \u001b[0;36mk_fold_val_2\u001b[0;34m(number_of_features, objective, num_class, learning_rate, n_estimators, max_depth, colsample_bytree, eval_metric)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m#select K features with use of random forest feature selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mX_train_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrforest_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0;31m#X_train_f, ft = lasso(X_train, y_train, alpha=number_of_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mall_features\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-dd238dfa3f68>\u001b[0m in \u001b[0;36mrforest_features\u001b[0;34m(data, labels, n_features)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0membeded_rf_selector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSelectFromModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0membeded_rf_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0membeded_rf_support\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeded_rf_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_selection/_from_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \"Since 'prefit=True', call transform directly\")\n\u001b[1;32m    223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    387\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \"\"\"\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG -> 1: that can be executed at any machine with final csv file and \n",
    "#--------------------------------------------------------------------------\n",
    "#good one \n",
    "#------------------------------------------\n",
    "#LR: 0.1\n",
    "#nEst: 150\n",
    "#m_depth: 7\n",
    "#bytree: 0.7\n",
    "#ev_m: aucpr score avg:0.9394461205162186\n",
    "#-----------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "learning_rate =[0.1,0.2,0.3,0.4]\n",
    "n_estimators=[50,100, 150,200,250,300,350]\n",
    "max_depth=[1,2,3,4,5,6,7,8,9,10,15,20]\n",
    "colsample_bytree = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "eval_metric=[\"rmse\",\"auc\"]\n",
    "\"\"\"\n",
    "\n",
    "#print(df.shape)\n",
    "values = list(np.arange(0.00080, 0.002, 0.00001))\n",
    "values=list(range(40,50))\n",
    "#X,ft = lasso(df, target, alpha=0.02)\n",
    "#print(len(ft))\n",
    "\n",
    "#plt.figure(figsize=(8.0,8.0))\n",
    "k_scores = defaultdict(lambda: 0)\n",
    "\n",
    "objective=[\"multi:softprob\",\"multi:softprob\",\"reg:squarederror\",\"reg:squaredlogerror\", \n",
    "           \"binary:logistic\",\"binary:logitraw\",\"binary:hinge\"]\n",
    "learning_rate =[0.08, 0.1, 0.13]\n",
    "n_estimators=[150, 200, 250,300]\n",
    "max_depth=[5,6,7,8]\n",
    "colsample_bytree = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75]\n",
    "\n",
    "\n",
    "objective=[\"multi:softprob\"]#,\"multi:softmax\"]\n",
    "learning_rate =[0.07,0.08,0.09]\n",
    "n_estimators=[150,200,300,350]\n",
    "max_depth=[4,5,6]\n",
    "colsample_bytree = [0.45,0.5,0.55,0.6,0.65]\n",
    "\n",
    "eval_metric=[\"aucpr\"]\n",
    "for obj in objective:\n",
    "    for lr in learning_rate:\n",
    "        for n_est in n_estimators:\n",
    "            for m_depth in max_depth:\n",
    "                for bytree in colsample_bytree:\n",
    "                    \n",
    "                    for ev_m in eval_metric:\n",
    "                        k_scores = []\n",
    "                        k_scores_train = []\n",
    "                        k_scores_unseen = []\n",
    "                        f_scores = []\n",
    "                        for rnd in range(3):\n",
    "                            k_accuracy, k_accuracy_train, k_accuracy_unseen = k_fold_val_2(number_of_features=45, \n",
    "                                                                        objective=obj,\n",
    "                                                                       num_class = 2,\n",
    "                                                                       learning_rate =lr,\n",
    "                                                                       n_estimators=n_est,\n",
    "                                                                       max_depth=m_depth,\n",
    "                                                                       colsample_bytree = bytree,\n",
    "                                                                       eval_metric=ev_m)\n",
    "                            #k_accuracy, full_accuracy = k_fold_val_2(number_of_features=alpha)\n",
    "                            k_scores.append(k_accuracy)\n",
    "                            k_scores_train.append(k_accuracy_train)\n",
    "                            k_scores_unseen.append(k_accuracy_unseen)\n",
    "                        \n",
    "                        #f_scores.append(full_accuracy)\n",
    "                        f_out = open(\"param_ftune.txt\",\"a+\")\n",
    "                        f_out.write(\"Obj:{}\\nLR: {}\\nnEst: {}\\nm_depth: {}\\nbytree: {}\\nev_m: {} score avg:{} score train:{} avg unseen:{}\\n------------\\n\".format(\n",
    "                                            obj,\n",
    "                                            lr,\n",
    "                                            n_est, \n",
    "                                            m_depth,\n",
    "                                            bytree, \n",
    "                                            ev_m, \n",
    "                                            sum(k_scores)/len(k_scores),\n",
    "                                            sum(k_scores_train)/len(k_scores_train),\n",
    "                                            sum(k_scores_unseen)/len(k_scores_unseen)) )\n",
    "                        f_out.close()\n",
    "\n",
    "                        #if sum(k_scores)/len(k_scores) >= 0.938:\n",
    "                        print(\"Obj:{}\\nLR: {}\\nnEst: {}\\nm_depth: {}\\nbytree: {}\\nev_m: {} score avg_test:{} avg_train:{} avg unseen:{}\".format(\n",
    "                                            obj,\n",
    "                                            lr,\n",
    "                                            n_est, \n",
    "                                            m_depth,\n",
    "                                            bytree, \n",
    "                                            ev_m, \n",
    "                                            sum(k_scores)/len(k_scores),\n",
    "                                            sum(k_scores_train)/len(k_scores_train),\n",
    "                                            sum(k_scores_unseen)/len(k_scores_unseen)) )\n",
    "\n",
    "#res = []\n",
    "#for alpha in values:\n",
    "#    res.append(k_scores[alpha] / 10)\n",
    "#plt.plot(values, res, label=\"K-Fold\")\n",
    "#plt.plot(values,f_scores, label=\"Full Split 70/30\")\n",
    "#plt.legend()\n",
    "#plt.xticks(rotation=45)\n",
    "#plt.savefig(\"Lasso_feature.png\",dpi=600)\n",
    "\n",
    "\n",
    "#features = list(range(2,167))\n",
    "#accuracy_fold = []\n",
    "#accuracy_split = []\n",
    "#for f_number in features:\n",
    "#    acc_s, acc_s_s = k_fold_val_2(number_of_features=f_number,n_rep=10)\n",
    "#    accuracy_fold.append(acc_s)\n",
    "#    accuracy_split.append(acc_s_s)\n",
    "#plt.plot(features, accuracy_fold, label=\"K-Fold avg accuracy\")\n",
    "#plt.plot(features, accuracy_split, label=\"70/30 split accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../data/features_large_lexical.csv\"\n",
    "df, target, df_test,target_test = read_data(filename)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=0,test_size=0.2,stratify=target)\n",
    "\n",
    "\n",
    "objective=\"multi:softprob\"\n",
    "learning_rate =0.07\n",
    "n_estimators=300\n",
    "max_depth=5\n",
    "colsample_bytree = 0.45\n",
    "eval_metric=\"aucpr\"\n",
    "model = XGBClassifier(objective=objective,\n",
    "                                   num_class = 2,\n",
    "                                   learning_rate =learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   colsample_bytree = colsample_bytree,\n",
    "                                   eval_metric=eval_metric,#\"rmse\",\n",
    "                                   use_label_encoder=False)\n",
    "model.fit(X_train, y_train)\n",
    "# make predictions for test data and evaluate\n",
    "predictions = model.predict(X_test)\n",
    "predictions_unseen = model.predict(df_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "accuracy_unseen = accuracy_score(target_test, predictions_unseen)\n",
    "print(\"Accuracy test: {} Accuracy unseen: {}\".format(accuracy, accuracy_unseen))\n",
    "# Fit model using each importance as a threshold\n",
    "thresholds = sort(model.feature_importances_)\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True)\n",
    "    select_X_train = selection.transform(X_train)\n",
    "    # train model\n",
    "    selection_model = XGBClassifier(objective=objective,\n",
    "                                   num_class = num_class,\n",
    "                                   learning_rate =learning_rate,\n",
    "                                   n_estimators=n_estimators,\n",
    "                                   max_depth=max_depth,\n",
    "                                   colsample_bytree = colsample_bytree,\n",
    "                                   eval_metric=eval_metric,#\"rmse\",\n",
    "                                   use_label_encoder=False)\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    # eval model\n",
    "    select_X_test = selection.transform(X_test)\n",
    "    predictions = selection_model.predict(select_X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    select_X_unseen = selection.transform(df_test)\n",
    "    predictions_unseen = selection_model.predict(select_X_unseen)\n",
    "    accuracy_unseen = accuracy_score(target_test, predictions_unseen)\n",
    "    print(\"Thresh={}, n={}, Accuracy test: {} Accuracy unseen: {}\".format(thresh, select_X_train.shape[1], accuracy, accuracy_unseen))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy 0.9269684290604793\n",
      "Train accuracy 0.9945781413488063\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df, target,random_state=0,test_size=0.2)\n",
    "X_train_f, ft = rforest_features(X_train, y_train, 95)\n",
    "#X_train_f, ft = lasso(X_train, y_train, alpha=number_of_features)\n",
    "\n",
    "#for f in ft:\n",
    "#    all_features.append(f)\n",
    "\n",
    "#keep only selected features in train and test splits\n",
    "X_test_f = X_test[ft]\n",
    "\"\"\"\n",
    "#another classifiers for cross validation, that have pour performance till now\n",
    "reg_scores = cross_val_score(LogisticRegression(max_iter=10000,penalty='l1',\n",
    "                                                solver='liblinear',C=1.0), X, target,cv=5)\n",
    "\n",
    "svc_scores = cross_val_score(SVC(kernel='rbf', C=0.9), X, target,cv=5)\n",
    "\n",
    "for_scores = cross_val_score(RandomForestClassifier(n_estimators=35,\n",
    "                                                    criterion='gini',\n",
    "                                                    ccp_alpha=0.0003,\n",
    "                                                    min_samples_split=4), X, target,cv=5)\n",
    "\n",
    "\"\"\"\n",
    "xGb_cl = XGBClassifier(objective=\"multi:softprob\",\n",
    "                               random_state = 10,\n",
    "                               num_class = 2,\n",
    "                               learning_rate =0.02,\n",
    "                               n_estimators=700,\n",
    "                               max_depth=8,\n",
    "                               min_child_weight=1,\n",
    "                               reg_alpha = 0.4,\n",
    "                               #reg_lambda = 1,\n",
    "                               #colsample_bytree = 0.7,\n",
    "                               eval_metric=\"aucpr\",#\"rmse\",\n",
    "                       use_label_encoder=False)\n",
    "\n",
    "xGb_cl.fit(X_train_f, y_train)\n",
    "\n",
    "#predict test dataset and keep prediction labels\n",
    "pred = xGb_cl.predict(X_test_f)\n",
    "pred_train = xGb_cl.predict(X_train_f)\n",
    "#print(\"Fold accuracy score: \",accuracy_score(y_test, pred))\n",
    "print(\"Test accuracy\",accuracy_score(y_test, pred))\n",
    "print(\"Train accuracy\",accuracy_score(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimentionality reduction t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#X, _ = lasso_features(df, target, 4)\n",
    "#X,_ = lasso(df, target, alpha=0.00002)\n",
    "X = rforest_features(df, target, 56)\n",
    "tsne_2d = make_tsne(X, target, \"full dataset\",learning_rate=100, perplexity=20)\n",
    "#tsne_2d = make_tsne(X, target, \"full dataset\")\n",
    "#tsne_2d = make_tsne(X, target, \"full dataset\")\n",
    "#print(\"Plot t-SNE for train dataset with features selected from train portion\")\n",
    "#make_tsne(X_train_scaled, y_train, \"train dataset\")\n",
    "#print(\"Plot t-SNE for test dataset with features selected from train portion\")\n",
    "#make_tsne(X_test_scaled, y_test, \"test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimentionality reduction UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#X, _ = lasso_features(df, target, 4)\n",
    "umap_2d = perofrm_umap_clustering(X, target, \"all dataset\")\n",
    "#embedding = perofrm_umap_clustering(X, target, \"all dataset\")\n",
    "#embedding = perofrm_umap_clustering(X, target, \"all dataset\")\n",
    "#perofrm_umap_clustering(X_test_scaled, y_test,  \"test dataset\")\n",
    "#perofrm_umap_clustering(X_train_scaled, y_train, \"train dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#dbscan_clustering(tsne_2d, 0.00557, 25)\n",
    "#dbscan_clustering(tsne_2d, 0.0045, 25)\n",
    "#dbscan_clustering(tsne_2d, 0.0057, 30)\n",
    "#dbscan_clustering(umap_2d,0.007, 30)\n",
    "#dbscan_clustering(umap_2d,0.0025, 20)\n",
    "dbscan_clustering(umap_2d,0.0025, 20)\n",
    "#dbscan_clustering(X, 0.00557, 25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean-Shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "mean_shift(tsne_2d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "res = cosine_similarity(X,Y= None, dense_output=True)\n",
    "pairs = set()\n",
    "f_out = open(\"cosine_graph.dot\",\"w+\")\n",
    "f_out.write(\"digraph {\\n\")\n",
    "for i in range(0, res.shape[0]):\n",
    "    f_out.write(\"\\t\\\"{}\\\"     [label=\\\"{}\\\"];\\n\".format(i,i))\n",
    "    #    \"1\" -- \"2\" [weight=100];\")\n",
    "for i in range(0,res.shape[0]):\n",
    "    max_sim = res[i,list(set(range(0,39123)) - set([i]))]\n",
    "    max_sim = [x for x in max_sim if x >= 0.9]\n",
    "    if len(max_sim) == 0:\n",
    "        continue\n",
    "    max_sim.sort(reverse=True)\n",
    "    if len(max_sim) > 5:\n",
    "        max_sim = max_sim[:5]\n",
    "    \n",
    "    \n",
    "    #print(max_sim)\n",
    "    #print(np.where(res[1,:] == max_sim))\n",
    "    for m_value in max_sim:\n",
    "        line = \"\\t\\\"{}\\\" -- \\\"{}\\\" [weight={}];\\n\".format(i, np.where(res[i,:] == m_value)[0][0], int(m_value*100.0))\n",
    "        line_rev = \"\\t\\\"{}\\\" -- \\\"{}\\\" [weight={}];\\n\".format(np.where(res[i,:] == m_value)[0][0], i, int(m_value*100.0))\n",
    "        if line in pairs or line_rev in pairs:\n",
    "            continue\n",
    "        pairs.add(line)\n",
    "        #print(line)\n",
    "        f_out.write(line)\n",
    "f_out.write(\"}\")\n",
    "f_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "gmm = GaussianMixture(n_components=2,n_init=100,init_params = \"kmeans\",covariance_type = \"full\")\n",
    "#X, _ = lasso_features(df, target, 10)\n",
    "#X = kbest_features(df, target, 3)\n",
    "gmm.fit(X)\n",
    "\n",
    "#predictions from gmm\n",
    "labels = gmm.predict(X)\n",
    "TP = [True if labels[i]==1 and target[i] == 1 else False for i in range(len(target)) ].count(True)\n",
    "TN = [True if labels[i]==0 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FP = [True if labels[i]==1 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FN = [True if labels[i]==0 and target[i] == 1 else False for i in range(len(target)) ].count(True)\n",
    "P = len(target[target == 1])\n",
    "N = len(target[target == 0])\n",
    "print(\"True pos :{} tp rate:{}\".format(TP,TP/P))\n",
    "print(\"True neg :{} tn rate:{}\".format(TN, TN/N))\n",
    "print(\"False pos :{} fp rate:{}\".format(FP,FP/N))\n",
    "print(\"False neg :{} fn rate:{}\".format(FN,FN/P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#X = lasso_features(df, target, 4)\n",
    "#X = rforest_features(df, target, 4)\n",
    "TPR_L = []\n",
    "TNR_L = []\n",
    "FNR_L = []\n",
    "FPR_L = []\n",
    "purity = []\n",
    "#for n_features in range(1,10):\n",
    "for j in range(2,40):\n",
    "    #X = kbest_features(df, target, n_features)\n",
    "    X, _ = lasso_features(df, target, 4)\n",
    "    #X = rforest_features(df, target, 4)\n",
    "   \n",
    "    TPR = []\n",
    "    TNR = []\n",
    "    FPR = []\n",
    "    FNR = []\n",
    "    tmp_pur = []\n",
    "    for k in range(0,10):\n",
    "        gmm = GaussianMixture(n_components=j,init_params = \"kmeans\",covariance_type = \"full\")\n",
    "        gmm.fit(X)\n",
    "\n",
    "        #predictions from gmm\n",
    "        labels = gmm.predict(X)\n",
    "        tmp_pur.append(compute_pur(labels,target))\n",
    "        TP = [True if labels[i]==1 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "        TN = [True if labels[i]==0 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "        FP = [True if labels[i]==1 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "        FN = [True if labels[i]==0 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "        P = len(target[target == 2])\n",
    "        N = len(target[target == 0])\n",
    "        TPR.append(TP/P)\n",
    "        TNR.append(TN/N)\n",
    "        FPR.append(FP/N)\n",
    "        FNR.append(FN/P)\n",
    "    purity.append(sum(tmp_pur) / len(tmp_pur))\n",
    "    TPR = sum(TPR) / len(TPR)\n",
    "    TNR = sum(TNR) / len(TNR)\n",
    "    FPR = sum(FPR) / len(FPR)\n",
    "    FNR = sum(FNR) / len(FNR)\n",
    "    \n",
    "    #print(\"Number of features:{} TPR:{} TNR:{} FNR:{} FPR:{}\".format(n_features, TPR, TNR, FNR, FPR))\n",
    "    TPR_L.append(TPR)\n",
    "    TNR_L.append(TNR)\n",
    "    FNR_L.append(FNR)\n",
    "    FPR_L.append(FPR)\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "plt.plot(range(1,len(TPR_L)+1) ,TPR_L, label=\"True Positive\", color=\"Blue\" )\n",
    "plt.plot(range(1,len(TNR_L)+1) ,TNR_L, label=\"True Negative\", color=\"Green\" )\n",
    "plt.plot(range(1,len(FNR_L)+1) ,FNR_L, label=\"False Negative\", color=\"Red\")\n",
    "plt.plot(range(1,len(FPR_L)+1) ,FPR_L, label=\"False Positive\" , color =\"y\")\n",
    "plt.xticks(range(1,len(FPR_L)+1))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "plt.plot(range(2,40),purity, label=\"Purity avg\")\n",
    "plt.title(\"Avg cluster purity\")\n",
    "plt.show()\n",
    "print(1+TPR_L.index(max(TPR_L)))\n",
    "print(1+TNR_L.index(max(TNR_L)))\n",
    "print(1+FNR_L.index(min(FNR_L)))\n",
    "print(1+FPR_L.index(min(FPR_L)))\n",
    "\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "X, _ = lasso_features(df, target, 4)\n",
    "X_s = X[target==0,:]\n",
    "print(X_s.shape)\n",
    "cov = EllipticEnvelope().fit(X)\n",
    "res = cov.predict(X)\n",
    "print(len(target))\n",
    "labels = [target[i] for i in range(len(res)) if res[i] == -1]\n",
    "\n",
    "print(\"Wrong:\",labels.count(0))\n",
    "print(\"Right:\",labels.count(2))\n",
    "gm = GaussianMixture(n_components=2)\n",
    "gm.fit(X)\n",
    "densities = gm.score_samples(X)\n",
    "print(len(densities))\n",
    "density_threshold = np.percentile(densities, 4)\n",
    "anomalies = np.where(densities < density_threshold)[0]\n",
    "labels = [target[i] for i in anomalies]\n",
    "print(labels.count(0))\n",
    "print(labels.count(2))\n",
    "#print(anomalies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "X, _ = lasso_features(df, target, 4)\n",
    "print(set(target))\n",
    "bgm = BayesianGaussianMixture(n_components=3, n_init=10, random_state=0,covariance_type='tied',init_params='kmeans')\n",
    "bgm.fit(X)\n",
    "labels = bgm.predict(X)\n",
    "\n",
    "labels[target == 2]\n",
    "\n",
    "TP = [True if labels[i]==2 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "TN = [True if labels[i]==0 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FP = [True if labels[i]==2 and target[i] == 0 else False for i in range(len(target)) ].count(True)\n",
    "FN = [True if labels[i]==0 and target[i] == 2 else False for i in range(len(target)) ].count(True)\n",
    "P = len(target[target == 2])\n",
    "N = len(target[target == 0])\n",
    "print(\"True pos :{} tp rate:{}\".format(TP,TP/P))\n",
    "print(\"True neg :{} tn rate:{}\".format(TN, TN/N))\n",
    "print(\"False pos :{} fp rate:{}\".format(FP,FP/N))\n",
    "print(\"False neg :{} fn rate:{}\".format(FN,FN/P))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# calculate interquartile range\n",
    "def remove_outliers(data, labels):\n",
    "    all_outliers = set()\n",
    "    for i in range(data.shape[1]):\n",
    "        q25, q75 = np.percentile(data[:,i], 25), np.percentile(data[:,i], 75)\n",
    "        iqr = q75 - q25\n",
    "        # calculate the outlier cutoff\n",
    "        cut_off = iqr * 1.5\n",
    "        lower, upper = q25 - cut_off, q75 + cut_off\n",
    "        # identify outliers\n",
    "        outliers = set()\n",
    "        for x in data[:,i]:\n",
    "            if x < lower or x > upper:\n",
    "                outliers = set(np.where(data[:,i] == x)[0]).union(outliers)\n",
    "        #outliers = [np.where(X[:,i] == x)[0] for x in X[:,i] if x < lower or x > upper]\n",
    "        #print(len(outliers))\n",
    "        #print(len(set(outliers)))\n",
    "        all_outliers = all_outliers.union(outliers)\n",
    "    #print(\"----\")\n",
    "    #print(len(all_outliers))\n",
    "    l = [labels[i] for i in all_outliers]\n",
    "    #print(l.count(0))\n",
    "    #print(l.count(1))\n",
    "    #print(l.count(2))\n",
    "    ind_clear = list(set(range(X.shape[0])) -all_outliers)\n",
    "    X_clear = data[ind_clear,:]\n",
    "    target_clear = labels[ind_clear]\n",
    "    #print(X_clear.shape)\n",
    "    return X_clear, target_clear\n",
    "#print(len(outliers))\n",
    "#print(target[outliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "#X, _ = lasso_features(df, target, 4)\n",
    "print(set(target))\n",
    "#bgm = BayesianGaussianMixture(n_components=3, n_init=10, random_state=0,covariance_type='tied',init_params='kmeans')\n",
    "bgm = GaussianMixture(n_components=3, n_init=10,max_iter=1000, random_state=0,covariance_type = \"full\",init_params='kmeans')\n",
    "\n",
    "bgm.fit(X_clear)\n",
    "labels = bgm.predict(X_clear)\n",
    "\n",
    "print(labels[target_clear == 2])\n",
    "\n",
    "TP = [True if labels[i]==2 and target_clear[i] == 2 else False for i in range(len(target_clear)) ].count(True)\n",
    "TN = [True if labels[i]==0 and target_clear[i] == 0 else False for i in range(len(target_clear)) ].count(True)\n",
    "FP = [True if labels[i]==2 and target_clear[i] == 0 else False for i in range(len(target_clear)) ].count(True)\n",
    "FN = [True if labels[i]==0 and target_clear[i] == 2 else False for i in range(len(target_clear)) ].count(True)\n",
    "P = len(target_clear[target_clear == 2])\n",
    "N = len(target_clear[target_clear == 0])\n",
    "print(\"True pos :{} tp rate:{}\".format(TP,TP/P))\n",
    "print(\"True neg :{} tn rate:{}\".format(TN, TN/N))\n",
    "print(\"False pos :{} fp rate:{}\".format(FP,FP/N))\n",
    "print(\"False neg :{} fn rate:{}\".format(FN,FN/P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "r = labels[target_clear == 2]\n",
    "print(len(np.where(r == 0)[0]))\n",
    "print(len(np.where(r == 1)[0]))\n",
    "print(len(np.where(r == 2)[0]))\n",
    "print(\"------------\")\n",
    "r = labels[target_clear == 1]\n",
    "print(len(np.where(r == 0)[0]))\n",
    "print(len(np.where(r == 1)[0]))\n",
    "print(len(np.where(r == 2)[0]))\n",
    "print(\"------------\")\n",
    "r = labels[target_clear == 0]\n",
    "print(len(np.where(r == 0)[0]))\n",
    "print(len(np.where(r == 1)[0]))\n",
    "print(len(np.where(r == 2)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "#new_lb = { int(i.split(\" \")[0]) : int(i.split(\" \")[1]) for i in open('labels_sample.txt','r').read().split(\"\\n\") if i != ''}\n",
    "#new_target = np.array([new_lb[user_ids[u]] for u in range(len(user_ids))])\n",
    "#print(new_target.shape)\n",
    "print(target.shape)\n",
    "#X, _ = lasso_features(df, target, 5)\n",
    "#X = rforest_features(df, target, 5)\n",
    "#X = kbest_features(df, new_target, 5)\n",
    "#X_clear, target_clear = remove_outliers(X, target)\n",
    "#clusters = dbscan_clustering(X_clear, 0.03, 25)\n",
    "#print(set(clusters))\n",
    "#cc = defaultdict(lambda: set())\n",
    "#for i in range(len(clusters)):\n",
    "#    cc[clusters[i]].add(i)\n",
    "#g_1 = set(np.where(target_clear == 1)[0])\n",
    "#g_2 = set(np.where(target_clear == 2)[0])\n",
    "#g_0 = set(np.where(target_clear == 0)[0])\n",
    "#for c in cc:\n",
    "#    if c == -1:\n",
    "#        continue\n",
    "#    print(\" Inter g_0: {} g_1: {} g_2: {}\".format(len(g_0.intersection(cc[c])),  len(g_1.intersection(cc[c])), len(g_2.intersection(cc[c])) ))\n",
    "    \n",
    "\n",
    "for f in range(1,10):\n",
    "    bic = []\n",
    "    aic=[]\n",
    "    plt.figure(figsize=(12.0,12.0))\n",
    "    X, _ = lasso_features(df, target, f)\n",
    "    print(\"Features:{}\".format(f))\n",
    "    for n in range(1,50):\n",
    "        bgm = GaussianMixture(n_components=n, n_init=10,max_iter=10000, random_state=0,covariance_type = \"tied\",init_params='kmeans')\n",
    "\n",
    "        bgm.fit(X)\n",
    "        bic.append(bgm.bic(X))\n",
    "        aic.append(bgm.aic(X))\n",
    "    plt.plot(range(len(bic)), bic,label=\"Bic\")\n",
    "    plt.plot(range(len(aic)), aic,label=\"Aic\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"GM_f_{}_tied.png\".format(f))\n",
    "    plt.clf()\n",
    "#accuracy_score(target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "\n",
    "print(target.shape)\n",
    "#X, _ = lasso_features(df, target, 5)\n",
    "#X = rforest_features(df, target, 5)\n",
    "#X = kbest_features(df, new_target, 5)\n",
    "#X_clear, target_clear = remove_outliers(X, target)\n",
    "\n",
    "score = []\n",
    "inhert = []\n",
    "aic=[]\n",
    "purity = []\n",
    "avg_cl_size = []\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "X, _ = lasso_features(df, target, 50)\n",
    "clusters = range(2,30)\n",
    "for k in clusters:\n",
    "    # Declaring Model\n",
    "    model = KMeans(n_clusters=k)\n",
    "\n",
    "    # Fitting Model\n",
    "    model.fit(X)\n",
    "    score.append(silhouette_score(X, model.labels_))\n",
    "    inhert.append(model.inertia_)\n",
    "    purity.append(compute_pur(model.labels_,target))\n",
    "    avg_cl_size.append(compute_size(model.labels_))\n",
    "plt.plot(  clusters, score)\n",
    "plt.plot(  clusters, inhert)\n",
    "plt.xticks(clusters,rotation=45)\n",
    "plt.xlabel(\"K number\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"K-means scores for mutliple number of clusters\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "plt.plot( clusters, purity)\n",
    "plt.title(\"Cluster purity\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(12.0,12.0))\n",
    "\n",
    "\n",
    "plt.plot( clusters, avg_cl_size)\n",
    "plt.title(\"Avg cluster size\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "#print(len(model.labels_))\n",
    "#print(model.labels_[0])\n",
    "#print(type(model.labels_[0]))\n",
    "#print(type(target[1]))\n",
    "#print(target[1])\n",
    "def compute_pur(labels,target):\n",
    "    d = defaultdict(lambda: [])\n",
    "    for i in range(len(labels)):\n",
    "        d[int(labels[i])].append(int(target[i]))\n",
    "    cluster_purity = []\n",
    "    for c in d:\n",
    "        max_tar = max(d[c])\n",
    "        cluster_purity.append(max_tar/len(d[c]))\n",
    "    return sum(cluster_purity)/ len(cluster_purity)\n",
    "\n",
    "def compute_size(labels):\n",
    "    d = defaultdict(lambda: [])\n",
    "    for i in range(len(labels)):\n",
    "        d[int(labels[i])].append(int(target[i]))\n",
    "    clusters = [len(d[x]) for x in d]\n",
    "    \n",
    "    return sum(clusters)/ len(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Reading the DataFrame\n",
    "#seeds_df = pd.read_csv(\n",
    "#    \"https://raw.githubusercontent.com/vihar/unsupervised-learning-with-python/master/seeds-less-rows.csv\")\n",
    "\n",
    "# Remove the grain species from the DataFrame, save for later\n",
    "#varieties = list(seeds_df.pop('grain_variety'))\n",
    "\n",
    "# Extract the measurements as a NumPy array\n",
    "#samples = seeds_df.values\n",
    "\n",
    "\"\"\"\n",
    "Perform hierarchical clustering on samples using the\n",
    "linkage() function with the method='complete' keyword argument.\n",
    "Assign the result to mergings.\n",
    "\"\"\"\n",
    "mergings = linkage(X, method='complete')\n",
    "\n",
    "\"\"\"\n",
    "Plot a dendrogram using the dendrogram() function on mergings,\n",
    "specifying the keyword arguments labels=varieties, leaf_rotation=90,\n",
    "and leaf_font_size=6.\n",
    "\"\"\"\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    "           )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "X, _ = lasso_features(df, target, 4)\n",
    "for i in range(10,250,10):\n",
    "    print(\"i:{}\".format(i))\n",
    "    tsne_2d = make_tsne(X, target, \"full dataset\", learning_rate=350, perplexity=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "X, _ = lasso_features(df, target, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "perofrm_umap_clustering(X, target,  \"Full\", n_neighbors=25, n_components=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------\n",
    "#FLAG  -> 0: that cannot be executed in any machine, excpet Alex.\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "scores = []\n",
    "for j in range(100):\n",
    "    df, target, user_ids = read_data(\"features_large_extra.csv\")\n",
    "    X = rforest_features(df, target, 40)\n",
    "    for i in range(1000):\n",
    "        clf = XGBClassifier(objective=\"multi:softprob\",\n",
    "                            num_class = 2,\n",
    "                            learning_rate =0.2,\n",
    "                            n_estimators=100,\n",
    "                            max_depth=5,\n",
    "                            colsample_bytree = 0.7,\n",
    "                            eval_metric=\"rmse\",\n",
    "                            use_label_encoder=False)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, target, test_size=0.3,)\n",
    "        clf.fit(X_train, y_train)\n",
    "        scores.append(clf.score(X_test,y_test))\n",
    "    \n",
    "print(\"Avg score:{}\".format(sum(scores)/len(scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
