#from utils import *
from xgboost import XGBClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.metrics import f1_score, roc_curve, auc, precision_recall_curve, accuracy_score, average_precision_score
#from imblearn.over_sampling import SMOTE
#from imblearn.combine import SMOTETomek
from sklearn.utils import shuffle
from collections import defaultdict, Counter
from datetime import datetime

from utils import *

# feature selection model parameters
objective = "multi:softprob"
learning_rate = 0.078
xgb_n_estimators = 490
max_depth = 4
colsample_bytree = 0.5
eval_metric = "aucpr"

#filename and path of dataset with extracted features
filename = "../data/us_2020_election_data.csv"


def get_most_freq_features(number_of_iter=20, gpu=False):
    feature_scores = []

    all_comp = number_of_iter * 5
    start_time = datetime.now()
    c = 0

    # Make multiple folds of data
    for fold in range(number_of_iter):
        # split entire dataset into 2 parts:
        #                    1st(80%) with all data used in training/validation
        #                    2nd(20%) with data used in final testing (20%)
        # Each time when we read file, we random shuffle it

        # df , target is 80% of dataset from file
        # df_test, target_test is 20% of dataset used only for evaluation
        df, target, df_test, target_test = read_data(filename, known_features=False)

        ##############################
        # Make 5 Fold cross validation#
        ##############################

        # Make Stratified K-Fold cross validation of K=5 with random shufle
        kf = StratifiedKFold(n_splits=5, shuffle=True)

        for train_index, test_index in kf.split(df, target):
            X_train, X_val = df.iloc[train_index], df.iloc[test_index]
            y_train, y_val = target.iloc[train_index], target.iloc[test_index]

            ####################################################
            # Oversample train and validation portion separately#
            ####################################################

            # oversampling clean on training set
            X_train, y_train = oversample(X_train, y_train)

            # oversampling clean on validation set
            X_val, y_val = oversample(X_val, y_val)

            ############################################
            # Train model based on train portion of data#
            # This model used for feature selection.    #
            ############################################

            model = get_xgboost_model(objective, learning_rate, xgb_n_estimators, max_depth,
                                      colsample_bytree, eval_metric, num_class=2, gpu=gpu)

            model.fit(X_train, y_train)

            ##########################################################
            # Select from trained model the N most important features#
            ##########################################################

            selection = SelectFromModel(model, threshold=-np.inf, max_features=200, prefit=True)

            ######################################################################################
            # Based on those selected features, transform the train,validation and final test sets#
            ######################################################################################

            X_train = X_train.iloc[:, selection.get_support(indices=True)]
            X_val = X_val.iloc[:, selection.get_support(indices=True)]

            ########################################################################
            # Train model based on train portion of data with selected features only#
            ########################################################################

            selected_model = get_xgboost_model(objective, learning_rate, xgb_n_estimators, max_depth,
                                      colsample_bytree, eval_metric, num_class=2, gpu=gpu)

            XGB_fitted_opt = selected_model.fit(X_train, y_train)

            XGB_val_probs = XGB_fitted_opt.predict_proba(X_val)

            # keep probabilities for the positive outcome only

            XGB_val_probs = XGB_val_probs[:, 1]

            ################################################
            # Evaluation of model by prediction of val data#
            ################################################

            # XGB_precision_train, XGB_recall_train, _ = precision_recall_curve(y_train, XGB_train_probs)
            XGB_precision_val, XGB_recall_val, _ = precision_recall_curve(y_val, XGB_val_probs)

            # Precision-Recall AUC

            XGB_auc = auc(XGB_recall_val, XGB_precision_val)


            # predict train labels
            predictions_train = selected_model.predict(X_train)

            # predict validation labels
            predictions_val = selected_model.predict(X_val)

            ##################################
            # Compute accuracy and F1 scores #
            ##################################

            accuracy_val = accuracy_score(y_val, predictions_val)
            XGB_f1 = f1_score(y_val, predictions_val)



            ##########################################################
            # Keep feature names and average of scores for each fold #
            ##########################################################

            feature_scores.append((X_train.columns.tolist(),
                                   (accuracy_val + XGB_auc + XGB_f1) / 3.0
                                   ))


            c += 1
            spend_time = (datetime.now() - start_time).seconds / 60.0
            print("Time spend:{} min. left comp: {} left time:{}".format(spend_time, all_comp - c,
                                                                         (spend_time / c) * (all_comp - c)))
    # after multiple monte_carlo executions we need to identify list of sorted best features
    # sort iterational features by their validation score, where first items have highest val score
    feature_scores.sort(key=lambda t: t[1], reverse=True)
    freq_features = []
    for features, val_score in feature_scores[:10]:
        freq_features += features
    freq_features = [x[0] for x in Counter(freq_features).most_common()]
    del(feature_scores)
    return freq_features


def sort_features_by_score(features, iteration=10, gpu=False):
    feature_score = defaultdict(lambda: 0.0)

    for k in range(0, iteration):
        # read from file
        # --> 80% for train + validation (df and target)
        # --> 20% for testing (df_test and y_test) Used for Final Testing
        df, target, df_test, y_test = read_data(filename, known_features=False)

        kf = StratifiedKFold(n_splits=5, shuffle=True)
        fold_id = 0
        for train_index, val_index in kf.split(df, target):
            X_train, X_val = df.iloc[train_index], df.iloc[val_index]
            y_train, y_val = target.iloc[train_index], target.iloc[val_index]

            # Oversample the Train and Validation data portions
            X_train, y_train = oversample(X_train, y_train)
            X_val, y_val = oversample(X_val, y_val)
            XGB_Y_train = y_train
            XGB_Y_val = y_val

            for feature in features:
                # Create new model which would be used for feature selection
                XGB_model = get_xgboost_model(objective, learning_rate, xgb_n_estimators, max_depth,
                                      colsample_bytree, eval_metric, num_class=2, gpu=gpu)

                XGB_X_train = X_train[feature]
                XGB_X_val = X_val[feature]

                XGB_fitted_opt = XGB_model.fit(np.vstack(XGB_X_train), XGB_Y_train.ravel())

                XGB_val_probs = XGB_fitted_opt.predict_proba(np.vstack(XGB_X_val))

                # keep probabilities for the positive outcome only
                XGB_val_probs = XGB_val_probs[:, 1]

                XGB_precision, XGB_recall, _ = precision_recall_curve(XGB_Y_val, XGB_val_probs)

                # AUC
                XGB_auc = auc(XGB_recall, XGB_precision)

                # F1 Score
                XGB_f1 = f1_score(XGB_Y_val, XGB_model.predict(np.vstack(XGB_X_val)))

                # summarize scores
                # print('XGBoost validation: f1=%.3f auc=%.3f' % (XGB_f1, XGB_auc))
                # print('XGBoost test: f1=%.3f auc=%.3f' % (XGB_f1_test, XGB_auc_test))

                feature_score[feature] += (XGB_f1 + XGB_auc) / 2

            print("Fold:{} of 5  Iteration:{} of {}".format(fold_id, k, iteration))
            fold_id += 1

    feature_score = [(feature, feature_score[feature] / (iteration * 5)) for feature in features]
    feature_score.sort(key=lambda t: t[1], reverse=True)
    result = [x[0] for x in feature_score]
    del(feature_score)
    return result


def monte_carlo_fs(features, nmbr_of_features=list(range(10, 250)), iteration=20, gpu=False):
    nmbr = [0] * len(features)

    f1nes = [0] * len(features)
    auces = [0] * len(features)
    f1_and_auc = [0] * len(features)

    f1nes_val = [0] * len(features)
    auces_val = [0] * len(features)
    f1_and_auc_val = [0] * len(features)

    f1nes_train = [0] * len(features)
    auces_train = [0] * len(features)
    f1_and_auc_train = [0] * len(features)

    for k in range(0, iteration):
        # read from file
        # --> 80% for train + validation (df and target)
        # --> 20% for testing (df_test and y_test) Used for Final Testing
        df, target, df_test, y_test = read_data(filename, known_features=False)

        # Make Stratified K-Fold cross validation of K=5 with random shufle
        kf = StratifiedKFold(n_splits=5, shuffle=True)
        fold_id = 0
        for train_index, val_index in kf.split(df, target):
            X_train, X_val = df.iloc[train_index], df.iloc[val_index]
            y_train, y_val = target.iloc[train_index], target.iloc[val_index]


            X_test = df_test
            # Oversample the Train and Validation data portions
            X_train, y_train = oversample(X_train, y_train)
            X_val, y_val = oversample(X_val, y_val)
            XGB_Y_train = y_train
            XGB_Y_val = y_val
            XGB_Y_test = y_test

            for i in nmbr_of_features:
                # Create new model which would be used for feature selection
                XGB_model = get_xgboost_model(objective, learning_rate, xgb_n_estimators, max_depth,
                                      colsample_bytree, eval_metric, num_class=2, gpu=gpu)

                nmbr[i] = i
                XGB_X_train = X_train[features[:i]]
                XGB_X_val = X_val[features[:i]]
                XGB_X_test = X_test[features[:i]]

                XGB_fitted_opt = XGB_model.fit(XGB_X_train, XGB_Y_train.ravel())

                XGB_train_probs = XGB_fitted_opt.predict_proba(XGB_X_train)
                XGB_val_probs = XGB_fitted_opt.predict_proba(XGB_X_val)
                XGB_test_probs = XGB_fitted_opt.predict_proba(XGB_X_test)

                # keep probabilities for the positive outcome only
                XGB_train_probs = XGB_train_probs[:, 1]
                XGB_val_probs = XGB_val_probs[:, 1]
                XGB_test_probs = XGB_test_probs[:, 1]

                XGB_precision_train, XGB_recall_train, _ = precision_recall_curve(XGB_Y_train, XGB_train_probs)
                XGB_precision, XGB_recall, _ = precision_recall_curve(XGB_Y_val, XGB_val_probs)
                XGB_precision_test, XGB_recall_test, _ = precision_recall_curve(XGB_Y_test, XGB_test_probs)

                # AUC
                XGB_auc_train = auc(XGB_recall_train, XGB_precision_train)
                XGB_auc = auc(XGB_recall, XGB_precision)
                XGB_auc_test = auc(XGB_recall_test, XGB_precision_test)

                # F1 Score
                XGB_f1_train = f1_score(XGB_Y_train, XGB_model.predict(XGB_X_train))
                XGB_f1 = f1_score(XGB_Y_val, XGB_model.predict(XGB_X_val))
                XGB_f1_test = f1_score(XGB_Y_test, XGB_model.predict(XGB_X_test))

                # summarize scores
                # print('XGBoost validation: f1=%.3f auc=%.3f' % (XGB_f1, XGB_auc))
                # print('XGBoost test: f1=%.3f auc=%.3f' % (XGB_f1_test, XGB_auc_test))
                f1nes[i] += XGB_f1_test
                auces[i] += XGB_auc_test
                f1_and_auc[i] += (XGB_f1_test + XGB_auc_test) / 2

                f1nes_val[i] += XGB_f1
                auces_val[i] += XGB_auc
                f1_and_auc_val[i] += (XGB_f1 + XGB_auc) / 2

                f1nes_train[i] += XGB_f1_train
                auces_train[i] += XGB_auc_train
                f1_and_auc_train[i] += (XGB_f1_train + XGB_auc_train) / 2
            print("Fold:{} of 5  Iteration:{} of {}".format(fold_id, k, iteration))
            fold_id += 1

    for i in range(1, len(f1nes)):
        f1nes[i] /= (iteration * 5)
        auces[i] /= (iteration * 5)
        f1_and_auc[i] /= (iteration * 5)

        f1nes_val[i] /= (iteration * 5)
        auces_val[i] /= (iteration * 5)
        f1_and_auc_val[i] /= (iteration * 5)

        f1nes_train[i] /= (iteration * 5)
        auces_train[i] /= (iteration * 5)
        f1_and_auc_train[i] /= (iteration * 5)


    f_out = open("fs_stats.txt", "w+")

    f_out.write("Validation data:\n")

    f_out.write("Max AUC:{} and F1 for {} number of features\n".format(
        auces_val[f1_and_auc_val.index(max(f1_and_auc_val))],
        f1nes_val[f1_and_auc_val.index(max(f1_and_auc_val))],
        f1_and_auc_val.index(max(f1_and_auc_val)) ))


    f_out.write("-------------------------------------\n")
    for fNumber in nmbr_of_features:
        f_out.write("Number of features:{}\n".format(fNumber))
        f_out.write("Train F1:{} AUC:{}\n".format(f1nes_train[fNumber], auces_train[fNumber]))
        f_out.write("Test F1:{} AUC:{}\n".format(f1nes[fNumber], auces[fNumber]))
        f_out.write("Validation F1:{} AUC:{}\n".format(f1nes_val[fNumber], auces_val[fNumber]))
        f_out.write("-------------------------------------\n")
    f_out.close()
    f_out = open("selected_features.txt","w+")
    f_out.write("{}".format(features[: f1_and_auc_val.index(max(f1_and_auc_val)) ]))
    f_out.close()



